{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in ./.venv/lib/python3.9/site-packages (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./.venv/lib/python3.9/site-packages (from tensorflow) (25.1.24)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.venv/lib/python3.9/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./.venv/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in ./.venv/lib/python3.9/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.9/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.9/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./.venv/lib/python3.9/site-packages (from tensorflow) (4.25.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./.venv/lib/python3.9/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in ./.venv/lib/python3.9/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.venv/lib/python3.9/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.venv/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in ./.venv/lib/python3.9/site-packages (from keras>=3.0.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in ./.venv/lib/python3.9/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in ./.venv/lib/python3.9/site-packages (from keras>=3.0.0->tensorflow) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.9/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.9/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.9/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./.venv/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow) (8.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv/lib/python3.9/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.9/site-packages (from rich->keras>=3.0.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.venv/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.venv/lib/python3.9/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp39-cp39-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.1 tzdata-2025.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from joblib import parallel_backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in data_collection: ['FALL_1.csv', 'FALL_2.csv', 'STUMBLE_1.csv', 'WALK_1.csv', 'WALK_2.csv', 'SITTING_1.csv', 'ACTIVITY_1.csv']\n"
     ]
    }
   ],
   "source": [
    "# Dataset Builder\n",
    "\n",
    "# List each file in folder data_collection\n",
    "files = os.listdir('data_collection')\n",
    "\n",
    "print('Files in data_collection:', files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data (first 5 rows):\n",
      "      0     1     2     3     4     5     6     7     8     9  ...   392  \\\n",
      "0  9.34  9.37  9.36  9.38  9.35  9.36  9.38  9.37  9.34  9.35  ...  9.35   \n",
      "1  9.36  9.36  9.35  9.37  9.38  9.37  9.37  9.36  9.37  9.37  ...  9.37   \n",
      "2  9.35  9.38  9.37  9.37  9.37  9.37  9.36  9.36  9.37  9.38  ...  9.36   \n",
      "3  9.36  9.37  9.37  9.36  9.36  9.38  9.36  9.40  9.37  9.36  ...  9.36   \n",
      "4  9.37  9.36  9.36  9.38  9.37  9.36  9.41  9.37  9.37  9.37  ...  9.36   \n",
      "\n",
      "    393   394   395   396   397   398   399  binary_label  sub_label  \n",
      "0  9.36  9.36  9.37  9.38  9.39  9.37  9.36             1     FALL_1  \n",
      "1  9.37  9.36  9.36  9.36  9.34  9.39  9.36             1     FALL_1  \n",
      "2  9.34  9.35  9.37  9.37  9.36  9.36  9.37             1     FALL_1  \n",
      "3  9.38  9.39  9.37  9.37  9.38  9.38  9.39             1     FALL_1  \n",
      "4  9.36  9.36  9.37  9.36  9.35  9.35  9.34             1     FALL_1  \n",
      "\n",
      "[5 rows x 402 columns]\n",
      "Shape of combined data: (436, 402)\n"
     ]
    }
   ],
   "source": [
    "data_folder = 'data_collection'\n",
    "\n",
    "# --- Function to determine binary label based on file name ---\n",
    "def get_binary_label(filename):\n",
    "    # Treat any file with \"FALL\" (case insensitive) as a fall (1); otherwise, non-fall (0)\n",
    "    return 1 if \"FALL\" in filename.upper() else 0\n",
    "\n",
    "# --- Import each file, add labels, and store in a list ---\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(data_folder, file)\n",
    "        # Read CSV file (assumed to have no header and 500 columns)\n",
    "        df = pd.read_csv(file_path, header=None)\n",
    "        \n",
    "        # Encode the labels:\n",
    "        # Binary label: 1 for fall events (FALL_1, FALL_2), 0 for others.\n",
    "        binary_label = get_binary_label(file)\n",
    "        # Sub-label: the file name without the .csv extension (e.g., \"FALL_1\")\n",
    "        sub_label = file.replace('.csv', '')\n",
    "        \n",
    "        # Add new columns to the DataFrame (one row per sequence)\n",
    "        df['binary_label'] = binary_label\n",
    "        df['sub_label'] = sub_label\n",
    "        \n",
    "        dataframes.append(df)\n",
    "\n",
    "# --- Combine all dataframes into a single DataFrame ---\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "print(\"Combined data (first 5 rows):\")\n",
    "print(combined_df.head())\n",
    "print(\"Shape of combined data:\", combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of fall events: 8.26%\n"
     ]
    }
   ],
   "source": [
    "# Print % of fall events with 2 decimal places\n",
    "num_falls = combined_df['binary_label'].sum()\n",
    "total_events = combined_df.shape[0]\n",
    "percent_falls = 100 * num_falls / total_events\n",
    "\n",
    "print(f\"Percentage of fall events: {percent_falls:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of combined data with augmented data: (724, 402)\n"
     ]
    }
   ],
   "source": [
    "# From augmented folder, import the first 5 files with FALL in the name and add to the combined DataFrame\n",
    "augmented_folder = 'augmented'\n",
    "augmented_files = os.listdir(augmented_folder)\n",
    "\n",
    "# --- Import each file, add labels, and store in a list ---\n",
    "dataframes = []\n",
    "\n",
    "for file in augmented_files:\n",
    "    if file.endswith('.csv') and \"FALL\" in file.upper():\n",
    "        file_path = os.path.join(augmented_folder, file)\n",
    "        # Read CSV file (assumed to have no header and 500 columns)\n",
    "        df = pd.read_csv(file_path, header=None)\n",
    "        \n",
    "        # Encode the labels:\n",
    "        # Binary label: 1 for fall events (FALL_1, FALL_2), 0 for others.\n",
    "        binary_label = get_binary_label(file)\n",
    "        # Sub-label: the file name without the .csv extension (e.g., \"FALL_1\")\n",
    "        sub_label = file.replace('.csv', '')\n",
    "        \n",
    "        # Add new columns to the DataFrame (one row per sequence)\n",
    "        df['binary_label'] = binary_label\n",
    "        df['sub_label'] = sub_label\n",
    "        \n",
    "        dataframes.append(df)\n",
    "\n",
    "# --- Combine all dataframes into a single DataFrame ---\n",
    "augmented = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# --- Combine the augmented data with the original data ---\n",
    "combined_df = pd.concat([combined_df, augmented], ignore_index=True)\n",
    "\n",
    "print(\"Shape of combined data with augmented data:\", combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Best cross-validation accuracy: 0.860\n",
      "\n",
      "Test set performance:\n",
      "Accuracy: 0.8482758620689655\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88        89\n",
      "           1       0.79      0.82      0.81        56\n",
      "\n",
      "    accuracy                           0.85       145\n",
      "   macro avg       0.84      0.84      0.84       145\n",
      "weighted avg       0.85      0.85      0.85       145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# --- Assume 'combined_df' is your DataFrame from the previous cell ---\n",
    "# The first 500 columns are your features, and 'binary_label' is the target.\n",
    "\n",
    "# Select features (columns 0 to 399) and the target\n",
    "X = combined_df.iloc[:, :500]\n",
    "y = combined_df['binary_label']\n",
    "\n",
    "# --- Split the data into train (80%) and test (20%) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Define a Random Forest classifier ---\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# --- Set up the parameter grid for optimization ---\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# --- Use GridSearchCV for hyperparameter optimization ---\n",
    "grid_search = GridSearchCV(estimator=rf_clf,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,                # 5-fold cross-validation\n",
    "                           n_jobs=-1,           # use all available cores\n",
    "                           scoring='accuracy')\n",
    "\n",
    "with parallel_backend('threading'):\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "# --- Print the best parameters and best cross-validation accuracy ---\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: {:.3f}\".format(grid_search.best_score_))\n",
    "\n",
    "# --- Evaluate the best estimator on the test set ---\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "print(\"\\nTest set performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "Best cross-validation accuracy: 0.815\n",
      "\n",
      "Test set performance:\n",
      "Accuracy: 0.8275862068965517\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.86        89\n",
      "           1       0.76      0.80      0.78        56\n",
      "\n",
      "    accuracy                           0.83       145\n",
      "   macro avg       0.82      0.82      0.82       145\n",
      "weighted avg       0.83      0.83      0.83       145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# --- Assume 'combined_df' is your DataFrame from the previous cell ---\n",
    "# The first 500 columns are your features and 'binary_label' is the target.\n",
    "\n",
    "# Select features (columns 0 to 399) and the target\n",
    "X = combined_df.iloc[:, :500]\n",
    "y = combined_df['binary_label']\n",
    "\n",
    "# --- Split the data into train (80%) and test (20%) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Scale features (important for SVM) ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Define an SVM classifier ---\n",
    "svm_clf = SVC(random_state=42)\n",
    "\n",
    "# --- Set up a parameter grid for grid search ---\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],          # Regularization parameter\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],  # Kernel coefficient for 'rbf'\n",
    "    'kernel': ['rbf']           # Using RBF kernel; you can also try 'linear'\n",
    "}\n",
    "\n",
    "# --- Set up GridSearchCV for hyperparameter optimization ---\n",
    "grid_search = GridSearchCV(estimator=svm_clf,\n",
    "                           param_grid=param_grid,\n",
    "                           cv=5,              # 5-fold cross-validation\n",
    "                           n_jobs=-1,         # Use all available cores\n",
    "                           scoring='accuracy')\n",
    "\n",
    "\n",
    "with parallel_backend('threading'):\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# --- Print the best parameters and best cross-validation accuracy ---\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: {:.3f}\".format(grid_search.best_score_))\n",
    "\n",
    "# --- Evaluate the best estimator on the test set ---\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred = best_svm.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nTest set performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4681 - loss: 0.9392 - val_accuracy: 0.6638 - val_loss: 0.5919 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6347 - loss: 0.6674 - val_accuracy: 0.6724 - val_loss: 0.5395 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6660 - loss: 0.6065 - val_accuracy: 0.6552 - val_loss: 0.5111 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7425 - loss: 0.5270 - val_accuracy: 0.6810 - val_loss: 0.4859 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7969 - loss: 0.4985 - val_accuracy: 0.7414 - val_loss: 0.4579 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7200 - loss: 0.4972 - val_accuracy: 0.7500 - val_loss: 0.4363 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8185 - loss: 0.3881 - val_accuracy: 0.8276 - val_loss: 0.4077 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8570 - loss: 0.3523 - val_accuracy: 0.8362 - val_loss: 0.3889 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8967 - loss: 0.3020 - val_accuracy: 0.8448 - val_loss: 0.3678 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9095 - loss: 0.2616 - val_accuracy: 0.8276 - val_loss: 0.3620 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9115 - loss: 0.2334 - val_accuracy: 0.7931 - val_loss: 0.3777 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9224 - loss: 0.2141 - val_accuracy: 0.8448 - val_loss: 0.3474 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9173 - loss: 0.2137 - val_accuracy: 0.8103 - val_loss: 0.3793 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9336 - loss: 0.1883 - val_accuracy: 0.7931 - val_loss: 0.4159 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9182 - loss: 0.2221 - val_accuracy: 0.8276 - val_loss: 0.4004 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9508 - loss: 0.1499 - val_accuracy: 0.8276 - val_loss: 0.4474 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9597 - loss: 0.1310 - val_accuracy: 0.8190 - val_loss: 0.4889 - learning_rate: 5.0000e-04\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7972 - loss: 0.3611\n",
      "Test accuracy: 0.7862069010734558\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.82        89\n",
      "           1       0.72      0.73      0.73        56\n",
      "\n",
      "    accuracy                           0.79       145\n",
      "   macro avg       0.77      0.78      0.78       145\n",
      "weighted avg       0.79      0.79      0.79       145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# --- Assume 'combined_df' is your DataFrame from earlier ---\n",
    "# The first 500 columns are features and 'binary_label' is the target.\n",
    "X = combined_df.iloc[:, :500].values\n",
    "y = combined_df['binary_label'].values\n",
    "\n",
    "# --- Split the data into train (80%) and test (20%) ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Scale the features ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Build a more complex Neural Network Model ---\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(500,)),\n",
    "    \n",
    "    # First hidden block\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Second hidden block\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Third hidden block\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Fourth hidden block\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    # Output layer for binary classification\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# --- Define the optimizer with a custom learning rate ---\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# --- Compile the model ---\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# --- Set up callbacks ---\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
    "\n",
    "# --- Train the model ---\n",
    "history = model.fit(X_train_scaled, y_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# --- Evaluate the model on the test set ---\n",
    "loss, accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(\"Test accuracy:\", accuracy)\n",
    "\n",
    "# --- Generate predictions and show classification report ---\n",
    "y_pred_prob = model.predict(X_test_scaled)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
