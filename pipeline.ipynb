{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "\n",
    "# Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning & Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "import optuna\n",
    "\n",
    "# Model Saving & Experiment Tracking\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow ui --port 5000\n",
      "[2025-03-13 19:32:07 +0100] [86116] [INFO] Starting gunicorn 23.0.0\n",
      "[2025-03-13 19:32:07 +0100] [86116] [ERROR] Connection in use: ('0.0.0.0', 5000)\n",
      "[2025-03-13 19:32:07 +0100] [86116] [ERROR] connection to ('0.0.0.0', 5000) failed: [Errno 48] Address already in use\n",
      "[2025-03-13 19:32:08 +0100] [86116] [ERROR] Connection in use: ('0.0.0.0', 5000)\n",
      "[2025-03-13 19:32:08 +0100] [86116] [ERROR] connection to ('0.0.0.0', 5000) failed: [Errno 48] Address already in use\n",
      "[2025-03-13 19:32:09 +0100] [86116] [ERROR] Connection in use: ('0.0.0.0', 5000)\n",
      "[2025-03-13 19:32:09 +0100] [86116] [ERROR] connection to ('0.0.0.0', 5000) failed: [Errno 48] Address already in use\n",
      "[2025-03-13 19:32:10 +0100] [86116] [ERROR] Connection in use: ('0.0.0.0', 5000)\n",
      "[2025-03-13 19:32:10 +0100] [86116] [ERROR] connection to ('0.0.0.0', 5000) failed: [Errno 48] Address already in use\n",
      "[2025-03-13 19:32:11 +0100] [86116] [ERROR] Connection in use: ('0.0.0.0', 5000)\n",
      "[2025-03-13 19:32:11 +0100] [86116] [ERROR] connection to ('0.0.0.0', 5000) failed: [Errno 48] Address already in use\n",
      "[2025-03-13 19:32:12 +0100] [86116] [ERROR] Can't connect to ('0.0.0.0', 5000)\n",
      "Running the mlflow server failed. Please see the logs above for details.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "print(\"mlflow ui --port 5000\")\n",
    "!mlflow ui --port 5000 --host 0.0.0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    '''\n",
    "    Load data from a csv file into a pandas dataframe.\n",
    "    \n",
    "    Args:\n",
    "    file_name: The name of the csv file to load\n",
    "    \n",
    "    Returns:\n",
    "    df: A pandas dataframe containing the data from the csv file\n",
    "    '''\n",
    "    folder = \"datasets\"\n",
    "\n",
    "    df = pd.read_csv(f\"{folder}/{file_name}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratify Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_activity_split(df, target_column=\"fall_binary\", test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits the dataset while ensuring 20% of each 'activity' is in the test set.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The full dataset containing 'activity' and the target variable.\n",
    "        target_column (str): The column representing the target labels.\n",
    "        test_size (float): The fraction of each activity to be in the test set.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        data_train (pd.DataFrame): Training set.\n",
    "        data_test (pd.DataFrame): Test set.\n",
    "    \"\"\"\n",
    "\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "\n",
    "    # ✅ Loop through each activity and apply train-test split\n",
    "    for activity, group in df.groupby(\"activity\"):\n",
    "        train, test = train_test_split(group, test_size=test_size, random_state=random_state, stratify=group[target_column])\n",
    "        train_list.append(train)\n",
    "        test_list.append(test)\n",
    "\n",
    "    # ✅ Concatenate results into train & test datasets\n",
    "    data_train = pd.concat(train_list).reset_index(drop=True)\n",
    "    data_test = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "    return data_train, data_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Per Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_per_activity(model, test_X, test_y, feature_columns=None):\n",
    "    \"\"\"\n",
    "    Evaluates model performance per activity type and logs the results in MLflow.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model (LSTM or Tree-based).\n",
    "        test_X: Test feature data (DataFrame).\n",
    "        test_y: Test target labels (Series).\n",
    "        target_column: The name of the target column.\n",
    "        feature_columns: The feature columns used for training.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing classification results per activity.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ✅ Ensure X_test is a DataFrame\n",
    "    if not isinstance(test_X, pd.DataFrame):\n",
    "        test_X = pd.DataFrame(test_X, columns=feature_columns)\n",
    "\n",
    "    # ✅ Ensure y_test is a Series\n",
    "    if isinstance(test_y, pd.DataFrame):\n",
    "        test_y = test_y.squeeze()  # Convert to Series if needed\n",
    "\n",
    "    # ✅ Ensure 'activity' column exists\n",
    "    if \"activity\" not in test_X.columns:\n",
    "        raise ValueError(\"Dataset does not contain an 'activity' column.\")\n",
    "\n",
    "    is_lstm = isinstance(model, tf.keras.Model)\n",
    "    results = []\n",
    "\n",
    "    # ✅ Loop through each unique activity and evaluate model performance\n",
    "    for activity in test_X[\"activity\"].unique():\n",
    "        # Filter test data for the current activity\n",
    "        X_test_activity = test_X[test_X[\"activity\"] == activity].copy()\n",
    "\n",
    "        # Extract the actual labels\n",
    "        y_test_activity = test_y.loc[X_test_activity.index].values.flatten()  # Ensure correct shape\n",
    "\n",
    "        # ✅ Check for missing values\n",
    "        if np.isnan(y_test_activity).any():\n",
    "            print(f\"Warning: NaN values found in y_test_activity for activity {activity}!\")\n",
    "            y_test_activity = np.nan_to_num(y_test_activity, nan=0)  # Replace NaN with 0 (No Fall)\n",
    "\n",
    "        # ✅ Ensure y_test_activity is integer\n",
    "        try:\n",
    "            y_test_activity = y_test_activity.astype(int)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error converting y_test_activity to integer for activity {activity}: {e}\")\n",
    "            continue  # Skip this activity if conversion fails\n",
    "\n",
    "        # ✅ Select feature columns\n",
    "        if is_lstm:\n",
    "            feature_columns = [col for col in X_test_activity.columns if col.startswith(\"value\")]\n",
    "\n",
    "        X_test_activity = X_test_activity[feature_columns].values  # Extract feature values\n",
    "\n",
    "        print(f\"Activity: {activity}, X shape: {X_test_activity.shape}, y shape: {y_test_activity.shape}\")\n",
    "\n",
    "        # ✅ Reshape X for LSTM input\n",
    "        if is_lstm:\n",
    "            X_test_activity = X_test_activity.reshape(-1, 500, 1)\n",
    "\n",
    "        # ✅ Predict fall_binary values\n",
    "        y_pred = model.predict(X_test_activity).round().astype(int).flatten()\n",
    "\n",
    "        # ✅ Compute correct and incorrect counts\n",
    "        correct = np.sum(y_pred == y_test_activity)\n",
    "        incorrect = len(y_pred) - correct\n",
    "\n",
    "        # ✅ Determine if the first sample of the activity is a fall or not\n",
    "        actual_fall = \"Fall\" if y_test_activity[0] == 1 else \"No Fall\"\n",
    "\n",
    "        print(f\"Activity {activity}: {correct} correct, {incorrect} incorrect\")\n",
    "\n",
    "        # ✅ Store results\n",
    "        results.append({\n",
    "            \"activity\": activity,\n",
    "            \"Actual Fall\": actual_fall,  # ✅ Single column for actual fall status\n",
    "            \"correct\": correct,\n",
    "            \"incorrect\": incorrect,\n",
    "            \"total\": len(y_pred),\n",
    "            \"accuracy\": correct / len(y_pred) if len(y_pred) > 0 else 0\n",
    "        })\n",
    "\n",
    "    # ✅ Convert results into a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # ✅ Mapping dictionary from abbreviations to full names\n",
    "    activity_mapping = {\n",
    "        \"S\": \"Still\",\n",
    "        \"CD\": \"Close Door\",\n",
    "        \"KD\": \"Knock Door\",\n",
    "        \"MA\": \"Minor Ambience (Sitting and Eating)\",\n",
    "        \"FOB\": \"Fall of object (Backpack)\",\n",
    "        \"FOL\": \"Fall of object (FaszienRolle)\",\n",
    "        \"WBS\": \"Walk by Sensor\",\n",
    "        \"RBS\": \"Rush by Sensor\",\n",
    "        \"SC\": \"Sitting down on chair\",\n",
    "        \"LC\": \"Laying down on couch\",\n",
    "        \"STC\": \"Stand up from Chair\",\n",
    "        \"PUF\": \"Picking something up from floor\",\n",
    "        \"K\": \"Kneeling down then standing up\",\n",
    "        \"SLB\": \"Standing Lost Balance\",\n",
    "        \"TF\": \"Trip and Fall - Forwards\",\n",
    "        \"SFB\": \"Slip and Fall - Backwards\",\n",
    "        \"FCS\": \"Chair - Fall to side\",\n",
    "        \"FCF\": \"Chair - Fall to Front\",\n",
    "        \"LAF\": \"Lying - Awake Fall\",\n",
    "        \"LSF\": \"Lying - Asleep Fall\",\n",
    "        \"FR\": \"Fall Recovery\",\n",
    "        \"KID\": \"Kids Running\"\n",
    "    }\n",
    "\n",
    "    # ✅ Function to extract the relevant part before the first '_'\n",
    "    def get_activity_name(code):\n",
    "        key = code.split('_')[0]  # Extract first part of activity code\n",
    "        return activity_mapping.get(key, code)  # Replace with full name if exists\n",
    "\n",
    "    # ✅ Apply the mapping to the results dataframe\n",
    "    results_df[\"activity\"] = results_df[\"activity\"].apply(get_activity_name)\n",
    "\n",
    "    # ✅ Save the results as a CSV file\n",
    "    results_path = \"activity_results.csv\"\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "\n",
    "    # ✅ Log results in MLflow\n",
    "    mlflow.log_artifact(results_path)\n",
    "\n",
    "    print(\"\\n📊 Per-Activity Results:\")\n",
    "    print(results_df)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Per Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_per_distance(model, test_X, test_y, feature_columns=None):\n",
    "    \"\"\"\n",
    "    Evaluates model performance per distance type and logs the results in MLflow.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model (LSTM or Tree-based).\n",
    "        test_X: Test feature data (DataFrame).\n",
    "        test_y: Test target labels (Series).\n",
    "        feature_columns: The feature columns used for training.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing classification results per distance.\n",
    "    \"\"\"\n",
    "\n",
    "    X_test = test_X.copy()\n",
    "    y_test = test_y.copy()\n",
    "\n",
    "    # ✅ Ensure X_test is a DataFrame\n",
    "    if not isinstance(X_test, pd.DataFrame):\n",
    "        X_test = pd.DataFrame(X_test, columns=feature_columns)\n",
    "\n",
    "    # ✅ Ensure y_test is a Series\n",
    "    if isinstance(y_test, pd.DataFrame):\n",
    "        y_test = y_test.squeeze()  # Convert to Series if needed\n",
    "\n",
    "    is_lstm = isinstance(model, tf.keras.Model)\n",
    "\n",
    "    # ✅ Ensure 'distance_m' column exists\n",
    "    if \"distance_m\" not in X_test.columns:\n",
    "        raise ValueError(\"Dataset does not contain a 'distance_m' column.\")\n",
    "\n",
    "    # ✅ Get unique distances and setup subplots dynamically\n",
    "    unique_distances = sorted(X_test[\"distance_m\"].unique())\n",
    "    fig, axes = plt.subplots(len(unique_distances), 1, figsize=(6, 4 * len(unique_distances))) \n",
    "\n",
    "    results = []\n",
    "    confusion_matrices = {}\n",
    "\n",
    "    # ✅ Loop through each unique distance and evaluate model performance\n",
    "    for i, distance in enumerate(unique_distances):\n",
    "        X_test_distance = X_test[X_test[\"distance_m\"] == distance].copy()\n",
    "        y_test_distance = y_test.loc[X_test_distance.index].values.flatten()  # Ensure correct shape\n",
    "\n",
    "        # ✅ Select feature columns\n",
    "        if is_lstm:\n",
    "            feature_columns = [col for col in X_test_distance.columns if col.startswith(\"value\")]\n",
    "\n",
    "        X_test_distance = X_test_distance[feature_columns].values  # Extract feature values\n",
    "\n",
    "        print(f\"Distance: {distance}, X shape: {X_test_distance.shape}, y shape: {y_test_distance.shape}\")\n",
    "\n",
    "        # ✅ Reshape X for LSTM input\n",
    "        if is_lstm:\n",
    "            X_test_distance = X_test_distance.reshape(-1, 500, 1)\n",
    "\n",
    "        # ✅ Predict fall_binary values\n",
    "        y_pred = model.predict(X_test_distance).round().astype(int).flatten()\n",
    "\n",
    "        # ✅ Compute correct and incorrect counts\n",
    "        correct = np.sum(y_pred == y_test_distance)\n",
    "        incorrect = len(y_pred) - correct\n",
    "\n",
    "        # ✅ Compute confusion matrix, ensuring both classes appear\n",
    "        cm = confusion_matrix(y_test_distance, y_pred, labels=[1, 0])\n",
    "\n",
    "        # ✅ Ensure confusion matrix always has shape (2,2)\n",
    "        if cm.shape == (1, 1):  \n",
    "            cm_fixed = np.array([[cm[0, 0], 0], [0, 0]])  \n",
    "        elif cm.shape == (1, 2):  \n",
    "            cm_fixed = np.vstack([cm, [0, 0]])  \n",
    "        elif cm.shape == (2, 1):  \n",
    "            cm_fixed = np.hstack([cm, [[0], [0]]])  \n",
    "        else:\n",
    "            cm_fixed = cm  \n",
    "\n",
    "        # ✅ Correct confusion matrix order:\n",
    "        # TP | FN\n",
    "        # FP | TN\n",
    "        cm_corrected = np.array([\n",
    "            [cm_fixed[0, 0], cm_fixed[1, 0]],  # True Positives, False Negatives\n",
    "            [cm_fixed[0, 1], cm_fixed[1, 1]]   # False Positives, True Negatives\n",
    "        ])\n",
    "\n",
    "        confusion_matrices[distance] = cm_corrected\n",
    "\n",
    "        # ✅ Plot confusion matrix with **correct** labels\n",
    "        sns.heatmap(cm_corrected, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                    xticklabels=[\"Actual Fall\", \"Actual No Fall\"], \n",
    "                    yticklabels=[\"Predicted Fall\", \"Predicted No Fall\"],\n",
    "                    ax=axes[i] if len(unique_distances) > 1 else axes)\n",
    "        axes[i].set_title(f\"Confusion Matrix - Distance {distance}\")\n",
    "        axes[i].set_xlabel(\" \")\n",
    "        axes[i].set_ylabel(\" \")\n",
    "\n",
    "        print(f\"Distance {distance}: {correct} correct, {incorrect} incorrect\")\n",
    "\n",
    "        # ✅ Store results\n",
    "        results.append({\n",
    "            \"distance\": distance,\n",
    "            \"correct\": correct,\n",
    "            \"incorrect\": incorrect,\n",
    "            \"total\": len(y_pred),\n",
    "            \"accuracy\": correct / len(y_pred) if len(y_pred) > 0 else 0\n",
    "        })\n",
    "\n",
    "    # ✅ Save and log confusion matrix plot\n",
    "    confusion_matrix_path = \"confusion_matrices_distance.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_matrix_path)\n",
    "    plt.close()\n",
    "\n",
    "    # ✅ Convert results into a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # ✅ Save results as CSV\n",
    "    results_path = \"distance_results.csv\"\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "\n",
    "    # ✅ Log results in MLflow\n",
    "    mlflow.log_artifact(results_path)\n",
    "    mlflow.log_artifact(confusion_matrix_path)\n",
    "\n",
    "    print(\"\\n📊 Per-Distance Results:\")\n",
    "    print(results_df)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial, model_class, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Defines the Optuna optimization objective for hyperparameter tuning.\n",
    "    \n",
    "    Args:\n",
    "        trial: Optuna trial object.\n",
    "        model_class: The model class (e.g., XGBClassifier, RandomForestClassifier, GradientBoostingClassifier, SVC).\n",
    "        X_train, X_test, y_train, y_test: Training and testing data.\n",
    "\n",
    "    Returns:\n",
    "        The validation F1-score of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Suggest hyperparameters based on model type\n",
    "    if model_class == XGBClassifier:\n",
    "        # Compute default scale_pos_weight (balance classes)\n",
    "        num_no_falls = sum(y_train == 0)\n",
    "        num_falls = sum(y_train == 1)\n",
    "        default_scale_pos_weight = num_no_falls / num_falls\n",
    "\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500, step=50),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0, 10),\n",
    "            \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", default_scale_pos_weight * 0.5, default_scale_pos_weight * 1.5)\n",
    "        }\n",
    "        model = XGBClassifier(**params, objective=\"binary:logistic\", use_label_encoder=False, random_state=42)\n",
    "\n",
    "    elif model_class == RandomForestClassifier:\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500, step=50),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5),\n",
    "        }\n",
    "        model = RandomForestClassifier(**params, random_state=42)\n",
    "\n",
    "    elif model_class == GradientBoostingClassifier:\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500, step=50),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5),\n",
    "        }\n",
    "        model = GradientBoostingClassifier(**params, random_state=42)\n",
    "\n",
    "    elif model_class == SVC:\n",
    "        params = {\n",
    "            \"C\": trial.suggest_float(\"C\", 0.1, 100, log=True),  # Adjusted range\n",
    "            \"kernel\": trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\"]),  # Removed \"poly\"\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.001, 10, log=True),  # Avoid too small values\n",
    "        }\n",
    "        model = SVC(**params, probability=True, random_state=42)  # Keep probability only if needed\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model class: {model_class.__name__}\")\n",
    "\n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate F1-score\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_classical_model(\n",
    "    model, \n",
    "    X_train, X_test, y_train, y_test, X_test_full,\n",
    "    save_name=\"classification_model\", \n",
    "    experiment_name=\"classical_models_experiment\",\n",
    "    target_column=\"fall_binary\",\n",
    "    feature_columns=None,\n",
    "    if_optuna=True,\n",
    "    n_trials=250,  # Number of hyperparameter tuning trials\n",
    "    dataset_name = \"fall_data.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a classical ML model (RandomForest, XGBoost, GradientBoosting, or SVM) with optional hyperparameter tuning\n",
    "    and logs everything to MLflow.\n",
    "\n",
    "    Args:\n",
    "        model: A classical ML model (RandomForest, XGBoost, GradientBoosting, or SVM).\n",
    "        X_train, X_test, y_train, y_test: Pre-split training and testing datasets.\n",
    "        save_name: Name to save the trained model.\n",
    "        experiment_name: Name of the MLflow experiment.\n",
    "        n_trials: Number of hyperparameter tuning trials.\n",
    "\n",
    "    Returns:\n",
    "        The trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Start MLflow experiment\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        # Identify model class\n",
    "        model_class = type(model)\n",
    "\n",
    "        if if_optuna:\n",
    "            # ✅ Perform hyperparameter optimization\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            study.optimize(lambda trial: objective(trial, model_class, X_train, X_test, y_train, y_test), n_trials=n_trials)\n",
    "        \n",
    "            best_params = study.best_params\n",
    "            print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "            # ✅ Train final model with best params\n",
    "            if model_class == XGBClassifier:\n",
    "                best_model = XGBClassifier(**best_params, objective=\"binary:logistic\", use_label_encoder=False)\n",
    "            elif model_class.__name__ == \"GradientBoostingClassifier\":\n",
    "                best_model = model_class(**best_params, random_state=42)\n",
    "            elif model_class.__name__ == \"RandomForestClassifier\":\n",
    "                best_model = model_class(**best_params, random_state=42)\n",
    "            elif model_class == SVC:\n",
    "                best_model = SVC(**best_params, probability=True, random_state=42)  # ✅ Enable probability for ROC AUC\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model class: {model_class.__name__}\")\n",
    "        else:\n",
    "            best_model = model\n",
    "\n",
    "        best_model.fit(X_train, y_train)\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "        # SVM needs probability predictions for AUC\n",
    "        if hasattr(best_model, \"predict_proba\"):\n",
    "            roc_auc = roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1])\n",
    "        else:\n",
    "            roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "        # ✅ Log model parameters\n",
    "        mlflow.log_param(\"model_type\", best_model.__class__.__name__)\n",
    "        if if_optuna:\n",
    "            for param, value in best_params.items():\n",
    "                mlflow.log_param(param, value)\n",
    "        mlflow.log_param(\"num_features\", X_train.shape[1])\n",
    "\n",
    "        # Log features used for training as list\n",
    "        if feature_columns is not None:\n",
    "            mlflow.log_param(\"features\", feature_columns)\n",
    "        \n",
    "        mlflow.log_param(\"dataset\", dataset_name)\n",
    "            \n",
    "        # ✅ Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "\n",
    "        # ✅ Save best model\n",
    "        models_folder = \"models\"\n",
    "        if not os.path.exists(models_folder):\n",
    "            os.makedirs(models_folder)\n",
    "        save_path = os.path.join(models_folder, f\"{save_name}.pkl\")\n",
    "        joblib.dump(best_model, save_path)\n",
    "        mlflow.log_artifact(save_path)\n",
    "\n",
    "        # Generate and save confusion matrix\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        conf_matrix = np.array([\n",
    "            [conf_matrix[0, 0], conf_matrix[1, 0]],  # True Positives, False Positives\n",
    "            [conf_matrix[0, 1], conf_matrix[1, 1]]   # False Negatives, True Negatives\n",
    "        ])\n",
    "\n",
    "        # Flip the order of labels for visualization\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(\n",
    "            conf_matrix[::-1, ::-1],  # Reverse rows and keep columns the same\n",
    "            annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Actual Fall\", \"Actual No Fall\"], \n",
    "            yticklabels=[\"Predicted Fall\", \"Predicted No Fall\"],\n",
    "        )\n",
    "        plt.xlabel(\" \")\n",
    "        plt.ylabel(\" \")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        conf_matrix_path = \"confusion_matrix.png\"\n",
    "        plt.savefig(conf_matrix_path)\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(conf_matrix_path)\n",
    "\n",
    "        print(f\"Model trained with accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "        # ✅ After training and evaluation, call the per-activity evaluation\n",
    "        evaluate_per_activity(best_model, X_test_full, y_test, feature_columns)\n",
    "\n",
    "        # ✅ After training and evaluation, call the per-distance evaluation\n",
    "        evaluate_per_distance(best_model, X_test_full, y_test, feature_columns)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipe(\n",
    "    model, \n",
    "    dataset_name, \n",
    "    save_name=\"fall_model\", \n",
    "    feature_columns=None, \n",
    "    target_column=\"fall_binary\", \n",
    "    experiment_name=\"default_experiment\",\n",
    "    use_early_stopping=True,\n",
    "    if_optuna=True,\n",
    "    n_trials=250\n",
    "):\n",
    "    \"\"\"\n",
    "    General training pipeline for both classical models (XGBoost, RF, SVM) and deep learning models.\n",
    "\n",
    "    Args:\n",
    "        model: The initialized model (LSTM, RNN, RF, XGB, etc.).\n",
    "        dataset_name: The dataset CSV file.\n",
    "        save_name: Name to save the trained model.\n",
    "        feature_columns: List of feature column names.\n",
    "        target_column: The name of the target column.\n",
    "        experiment_name: MLflow experiment name.\n",
    "        use_early_stopping: Whether to use early stopping (for neural networks).\n",
    "        if_optuna: Whether to perform hyperparameter tuning (for tree-based models).\n",
    "\n",
    "    Returns:\n",
    "        The trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    # ✅ Prevent overwriting existing models unless explicitly handled\n",
    "    if os.path.exists(f\"models/{save_name}.pkl\") or os.path.exists(f\"models/{save_name}.keras\"):\n",
    "        raise ValueError(f\"Model name '{save_name}' already exists. Choose a new name or delete the existing model.\")\n",
    "\n",
    "    # ✅ Load dataset\n",
    "    df = load_data(dataset_name)\n",
    "\n",
    "    # ✅ Auto-detect feature columns if not explicitly provided\n",
    "    value_columns = [col for col in df.columns if col.startswith(\"value\")]\n",
    "    if value_columns:\n",
    "        feature_columns = value_columns  \n",
    "    elif feature_columns is None:\n",
    "        raise ValueError(\"Feature columns must be specified.\")\n",
    "\n",
    "    # ✅ Ensure target column exists\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in dataset.\")\n",
    "\n",
    "    data_train, data_test = stratified_activity_split(df, target_column=target_column, test_size=0.2, random_state=42)\n",
    "    print(f\"Train shape: {data_train.shape}, Test shape: {data_test.shape}\")\n",
    "\n",
    "    # ✅ Keep full test data (including metadata like 'activity' or 'distance_m')\n",
    "    X_test_full = data_test.copy()  \n",
    "\n",
    "    # ✅ Keep `X_train`, `X_test` as DataFrames & `y_train`, `y_test` as Series\n",
    "    X_train = data_train[feature_columns]  \n",
    "    X_test = data_test[feature_columns]    \n",
    "    y_train = data_train[target_column]  \n",
    "    y_test = data_test[target_column]    \n",
    "\n",
    "    # Define list will all unique values in column acitvity of df\n",
    "    activities = df[\"activity\"].unique()\n",
    "\n",
    "    for activity in activities:\n",
    "\n",
    "        # Calculate how often in df\n",
    "        activity_count = df[\"activity\"].value_counts()[activity]\n",
    "\n",
    "        # Calculate how often in X_test\n",
    "        activity_count_test = X_test_full[\"activity\"].value_counts()[activity]\n",
    "\n",
    "        # Print activity_count_test / activity_count * 100\n",
    "        print(f\"Activity: {activity}\")\n",
    "        print(f\"Percentage in test set: {activity_count_test / activity_count * 100:.2f}%\")\n",
    "\n",
    "    # ✅ Check model type\n",
    "    is_tree = isinstance(model, (XGBClassifier, RandomForestClassifier, GradientBoostingClassifier))\n",
    "    is_svm = isinstance(model, SVC)\n",
    "\n",
    "    # ✅ Train tree-based models with optional Optuna hyperparameter tuning\n",
    "    if is_tree or is_svm:\n",
    "        model = train_and_log_classical_model(\n",
    "            model, X_train, X_test, y_train, y_test, X_test_full,\n",
    "            save_name=save_name,\n",
    "            experiment_name=experiment_name,\n",
    "            target_column=target_column,\n",
    "            feature_columns=feature_columns,\n",
    "            if_optuna=if_optuna,\n",
    "            n_trials=n_trials,\n",
    "            dataset_name=dataset_name\n",
    "        )\n",
    "    \n",
    "    # Delete the files activity_results.csv and distance_results.csv\n",
    "    os.remove(\"activity_results.csv\")\n",
    "    os.remove(\"distance_results.csv\")\n",
    "    os.remove(\"confusion_matrices_distance.png\")\n",
    "    os.remove(\"confusion_matrix.png\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mlflow.tensorflow\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "def train_model(\n",
    "    model, \n",
    "    dataset_name, \n",
    "    save_name=\"lstm_fall_model\", \n",
    "    feature_columns=None, \n",
    "    target_column=\"fall_binary\", \n",
    "    experiment_name=\"default_experiment\",\n",
    "    use_early_stopping=True  # ✅ Option to enable/disable early stopping\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads the dataset, reshapes it for RNN, trains the model, and logs everything to MLflow.\n",
    "\n",
    "    Args:\n",
    "        model: The RNN model (already defined).\n",
    "        dataset_name: The name of the dataset CSV file.\n",
    "        save_name: Name to save the trained model.\n",
    "        feature_columns: List of feature column names (overwritten if 'value' columns exist).\n",
    "        target_column: The name of the target column.\n",
    "        experiment_name: Name of the MLflow experiment.\n",
    "        use_early_stopping: Whether to enable early stopping based on validation loss.\n",
    "\n",
    "    Returns:\n",
    "        The trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load dataset\n",
    "    df = load_data(dataset_name)\n",
    "\n",
    "    # Auto-detect feature columns if they start with \"value\"\n",
    "    value_columns = [col for col in df.columns if col.startswith(\"value\")]\n",
    "    if value_columns:\n",
    "        feature_columns = value_columns  # Override feature selection\n",
    "    elif feature_columns is None:\n",
    "        raise ValueError(\"Feature columns must be specified if no 'value' columns exist.\")\n",
    "\n",
    "    # Ensure target column exists\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in dataset.\")\n",
    "\n",
    "    # Train-test split but keep the full dataset for evaluation\n",
    "    data_train, data_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    print(f\"Train shape: {data_train.shape}, Test shape: {data_test.shape}\")\n",
    "\n",
    "    X_train = data_train[feature_columns].values\n",
    "    X_test = data_test[feature_columns].values\n",
    "\n",
    "    y_train = data_train[target_column].values\n",
    "    y_test = data_test[target_column].values\n",
    "    \n",
    "    X_test_full = X_test.copy()  # Keep a copy of the test data for evaluation\n",
    "    y_test_full = y_test.copy()\n",
    "\n",
    "    X_train = X_train[feature_columns].values\n",
    "    X_test = X_test[feature_columns].values\n",
    "\n",
    "    # Reshape X: (num_samples, timesteps=500, num_features)\n",
    "    if value_columns:\n",
    "        num_features = len(feature_columns) // 500  # Calculate how many features per timestep\n",
    "        X_test = X_test.reshape(-1, 500, num_features)\n",
    "        X_train = X_train.reshape(-1, 500, num_features)\n",
    "    else:\n",
    "        num_features = len(feature_columns)\n",
    "\n",
    "    # Print shapes\n",
    "    print(f\"X_test_full shape: {X_test_full.shape}\")\n",
    "    print(f\"y_test_full shape: {y_test_full.shape}\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "    is_lstm = isinstance(model, tf.keras.Model)\n",
    "\n",
    "    if is_lstm:\n",
    "        # ✅ Set up Early Stopping (if enabled)\n",
    "        callbacks = []\n",
    "        if use_early_stopping:\n",
    "            early_stopping = EarlyStopping(\n",
    "                monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1\n",
    "            )\n",
    "            callbacks.append(early_stopping)\n",
    "\n",
    "    # Start MLflow experiment\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        print(f\"X_train shape: {X_train.shape}\")  # Should be (num_samples, timesteps, num_features)\n",
    "        print(f\"y_train shape: {y_train.shape}\")  # Should be (num_samples,)\n",
    "\n",
    "        # ✅ Log dataset name\n",
    "        mlflow.log_param(\"dataset_name\", dataset_name)  \n",
    "\n",
    "        if is_lstm:\n",
    "            # Train the model and log history\n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                epochs=EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                validation_data=(X_test, y_test),\n",
    "                callbacks=callbacks  # ✅ Apply Early Stopping if enabled\n",
    "            )\n",
    "        else:\n",
    "            model.fit(X_train, y_train)  # No need for epochs, batch size, or callbacks\n",
    "            \n",
    "        if is_lstm:\n",
    "            # Evaluate model\n",
    "            loss, accuracy = model.evaluate(X_test, y_test)\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        # ✅ After training and evaluation, call the per-activity evaluation\n",
    "        results_df = evaluate_per_activity(model, X_test_full, y_test_full, target_column, feature_columns)\n",
    "        \n",
    "        # ✅ After training and evaluation, call the per-distance evaluation\n",
    "        evaluate_per_distance(model, X_test_full, y_test_full, target_column, feature_columns)\n",
    "\n",
    "        # ✅ Log per-activity accuracy in MLflow\n",
    "        mlflow.log_artifact(\"activity_results.csv\")\n",
    "\n",
    "        # ✅ Log model parameters\n",
    "        mlflow.log_param(\"num_features_used\", len(feature_columns))\n",
    "        mlflow.log_param(\"model_type\", model.__class__.__name__)\n",
    "\n",
    "        if is_lstm:\n",
    "            mlflow.log_param(\"epochs\", EPOCHS)\n",
    "            mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "            mlflow.log_param(\"optimizer\", model.optimizer.__class__.__name__)\n",
    "            mlflow.log_param(\"loss_function\", model.loss)\n",
    "            mlflow.log_param(\"early_stopping\", use_early_stopping)  # ✅ Log whether early stopping was used\n",
    "            mlflow.log_metric(\"final_loss\", loss)\n",
    "\n",
    "        # ✅ Log metrics\n",
    "        mlflow.log_metric(\"final_accuracy\", accuracy)\n",
    "\n",
    "        if is_lstm:\n",
    "            # ✅ Log accuracy and loss per epoch\n",
    "            for epoch, (train_acc, val_acc, train_loss, val_loss) in enumerate(zip(history.history['accuracy'], history.history['val_accuracy'], history.history['loss'], history.history['val_loss'])):\n",
    "                mlflow.log_metric(\"train_accuracy_epoch\", train_acc, step=epoch)\n",
    "                mlflow.log_metric(\"val_accuracy_epoch\", val_acc, step=epoch)\n",
    "                mlflow.log_metric(\"train_loss_epoch\", train_loss, step=epoch)\n",
    "                mlflow.log_metric(\"val_loss_epoch\", val_loss, step=epoch)\n",
    "\n",
    "        # ✅ Ensure models folder exists\n",
    "        models_folder = \"models\"\n",
    "        if not os.path.exists(models_folder):\n",
    "            os.makedirs(models_folder)\n",
    "\n",
    "        save_path = os.path.join(models_folder, save_name)\n",
    "\n",
    "        if is_lstm:\n",
    "            model.save(save_path + \".keras\")  # ✅ Save Keras model\n",
    "            mlflow.log_artifact(save_path + \".keras\")\n",
    "        else:\n",
    "            import joblib\n",
    "            joblib.dump(model, save_path + \".pkl\")  # ✅ Save tree-based model\n",
    "            mlflow.log_artifact(save_path + \".pkl\")\n",
    "\n",
    "        # ✅ Log the saved model file as an artifact in MLflow\n",
    "        mlflow.log_artifact(save_path + \".pkl\")\n",
    "\n",
    "        # ✅ Save confusion matrix\n",
    "        y_pred = model.predict(X_test).round()\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "        # Compute additional metrics\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        roc_auc = roc_auc_score(y_test, model.predict(X_test))\n",
    "\n",
    "        # Log additional metrics\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "\n",
    "        # ✅ Save and log confusion matrix\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No Fall\", \"Fall\"], yticklabels=[\"No Fall\", \"Fall\"])\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        conf_matrix_path = \"confusion_matrix.png\"\n",
    "        plt.savefig(conf_matrix_path)\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(conf_matrix_path)\n",
    "\n",
    "        print(f\"Model trained with accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "        # Delete the files activity_results.csv and distance_results.csv\n",
    "        os.remove(\"activity_results.csv\")\n",
    "        os.remove(\"distance_results.csv\")\n",
    "        os.remove(\"confusion_matrices_distance.png\")\n",
    "        os.remove(\"confusion_matrix.png\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Collection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# XGBoost Model\n",
    "# ==========================\n",
    "\n",
    "def build_xgboost_model():\n",
    "    \"\"\"\n",
    "    Builds an XGBoost model for binary classification.\n",
    "\n",
    "    Returns:\n",
    "        A compiled XGBoost model.\n",
    "    \"\"\"\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.11,\n",
    "        objective=\"binary:logistic\",\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# ==========================\n",
    "# Random Forest Model\n",
    "# ==========================\n",
    "\n",
    "def build_random_forest_model():\n",
    "    \"\"\"\n",
    "    Builds a Random Forest model for binary classification.\n",
    "\n",
    "    Returns:\n",
    "        A compiled Random Forest model.\n",
    "    \"\"\"\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# ==========================\n",
    "# GBM Model\n",
    "# ==========================\n",
    "def build_gradient_boosting_model():\n",
    "    \"\"\"\n",
    "    Builds a Gradient Boosting (GBM) model for binary classification.\n",
    "\n",
    "    Returns:\n",
    "        A compiled Gradient Boosting model.\n",
    "    \"\"\"\n",
    "    return GradientBoostingClassifier(\n",
    "        n_estimators=200,  # Number of boosting stages\n",
    "        learning_rate=0.1,  # Step size shrinkage to prevent overfitting\n",
    "        max_depth=5,  # Maximum depth of the trees\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "# ==========================\n",
    "# SVM Model\n",
    "# ==========================\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def build_svm_model():\n",
    "    \"\"\"\n",
    "    Builds an SVM model for binary classification.\n",
    "\n",
    "    Returns:\n",
    "        A configured SVM model.\n",
    "    \"\"\"\n",
    "    model = SVC(\n",
    "        kernel=\"rbf\",  # Radial Basis Function (RBF) kernel (default)\n",
    "        C=1.0,         # Regularization parameter\n",
    "        gamma=\"scale\",  # Kernel coefficient\n",
    "        probability=True,  # Enable probability estimates (needed for ROC AUC)\n",
    "        random_state=42\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# RNN Model\n",
    "# ==========================\n",
    "def build_rnn_model():\n",
    "    \"\"\"\n",
    "    Builds a simple RNN model using Keras.\n",
    "\n",
    "    Returns:\n",
    "        A compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        SimpleRNN(RNN_UNITS, input_shape=INPUT_SHAPE, return_sequences=True),\n",
    "        SimpleRNN(RNN_UNITS),\n",
    "        Dense(DENSE_UNITS, activation=\"relu\"),\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNCTION, metrics=METRICS)\n",
    "    return model\n",
    "\n",
    "# ==========================\n",
    "# LSTM Model\n",
    "# ==========================\n",
    "def build_lstm_model(l2_lambda=0.001, dropout_rate=0.2, clipnorm=1.0):\n",
    "    \"\"\"\n",
    "    Builds an LSTM-based model with L2 regularization, dropout, and gradient clipping.\n",
    "\n",
    "    Args:\n",
    "        l2_lambda: Strength of L2 regularization (default: 0.001).\n",
    "        dropout_rate: Dropout rate to reduce overfitting (default: 0.2).\n",
    "        clipnorm: Gradient clipping norm (default: 1.0).\n",
    "\n",
    "    Returns:\n",
    "        A compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(RNN_UNITS, input_shape=INPUT_SHAPE, return_sequences=True, \n",
    "             kernel_regularizer=l2(l2_lambda)),\n",
    "        Dropout(dropout_rate),  # Dropout after first LSTM layer\n",
    "        LSTM(RNN_UNITS, kernel_regularizer=l2(l2_lambda)),\n",
    "        Dropout(dropout_rate),  # Dropout after second LSTM layer\n",
    "        Dense(DENSE_UNITS, activation=\"relu\", kernel_regularizer=l2(l2_lambda)),\n",
    "        Dropout(dropout_rate),  # Dropout before final layer\n",
    "        Dense(1, activation=\"sigmoid\")  # Output layer for binary classification\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(clipnorm=clipnorm)\n",
    "    model.compile(optimizer=optimizer, loss=LOSS_FUNCTION, metrics=METRICS)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Bidirectional LSTM Model (Optional)\n",
    "# ==========================\n",
    "def build_bidirectional_lstm():\n",
    "    \"\"\"\n",
    "    Builds a Bidirectional LSTM model for improved sequence learning.\n",
    "\n",
    "    Returns:\n",
    "        A compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(RNN_UNITS, return_sequences=True), input_shape=INPUT_SHAPE),\n",
    "        Bidirectional(LSTM(RNN_UNITS)),\n",
    "        Dense(DENSE_UNITS, activation=\"relu\"),\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNCTION, metrics=METRICS)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# CONFIGURABLE VARIABLES\n",
    "# ==========================\n",
    "RNN_UNITS = 128           # Number of RNN/LSTM units\n",
    "DENSE_UNITS = 64         # Number of neurons in the dense layer\n",
    "DROPOUT_RATE = 0.1      # Dropout rate for regularization\n",
    "OPTIMIZER = \"adam\"       # Optimizer: \"adam\", \"sgd\", \"rmsprop\", etc.\n",
    "LOSS_FUNCTION = \"binary_crossentropy\"  # \"binary_crossentropy\" for classification\n",
    "METRICS = [\"accuracy\"]   # Metrics to monitor\n",
    "EPOCHS = 10              # Number of training epochs\n",
    "BATCH_SIZE = 16          # Batch size for training\n",
    "INPUT_SHAPE = (500, 1)   # (Time steps, Features) - Adjust based on your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.11, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=5, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=400, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=None, ...)\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Initialize the Model\n",
    "# ==========================\n",
    "model = build_xgboost_model()  # Change this to your desired model\n",
    "# print(model.summary())  # Print model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 19:54:23,813] A new study created in memory with name: no-name-c0667735-96d0-4a54-bb8c-3fae1c40602a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1016, 18), Test shape: (254, 18)\n",
      "Activity: FCS\n",
      "Percentage in test set: 20.00%\n",
      "Activity: LC\n",
      "Percentage in test set: 20.00%\n",
      "Activity: RBS\n",
      "Percentage in test set: 20.00%\n",
      "Activity: K\n",
      "Percentage in test set: 20.00%\n",
      "Activity: LAF\n",
      "Percentage in test set: 20.00%\n",
      "Activity: SC\n",
      "Percentage in test set: 20.00%\n",
      "Activity: PUF\n",
      "Percentage in test set: 20.00%\n",
      "Activity: FOL\n",
      "Percentage in test set: 20.00%\n",
      "Activity: FCF\n",
      "Percentage in test set: 20.00%\n",
      "Activity: WBS\n",
      "Percentage in test set: 20.00%\n",
      "Activity: FR\n",
      "Percentage in test set: 20.00%\n",
      "Activity: TF\n",
      "Percentage in test set: 20.00%\n",
      "Activity: STC\n",
      "Percentage in test set: 20.00%\n",
      "Activity: LSF\n",
      "Percentage in test set: 20.00%\n",
      "Activity: SLB\n",
      "Percentage in test set: 20.00%\n",
      "Activity: MA\n",
      "Percentage in test set: 20.00%\n",
      "Activity: S\n",
      "Percentage in test set: 20.00%\n",
      "Activity: SFB\n",
      "Percentage in test set: 20.00%\n",
      "Activity: KD\n",
      "Percentage in test set: 20.00%\n",
      "Activity: FOB\n",
      "Percentage in test set: 20.00%\n",
      "Activity: KID\n",
      "Percentage in test set: 20.00%\n",
      "Activity: CD\n",
      "Percentage in test set: 20.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:23] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:24,537] Trial 0 finished with value: 0.9367088607594937 and parameters: {'n_estimators': 500, 'max_depth': 11, 'learning_rate': 0.015630555627815002, 'subsample': 0.8410361229939605, 'colsample_bytree': 0.8972539366738643, 'gamma': 1.6723226966557214, 'scale_pos_weight': 1.2982128836076783}. Best is trial 0 with value: 0.9367088607594937.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:24,780] Trial 1 finished with value: 0.9032258064516129 and parameters: {'n_estimators': 250, 'max_depth': 13, 'learning_rate': 0.02237446619309868, 'subsample': 0.9183850737969056, 'colsample_bytree': 0.8409058366508211, 'gamma': 6.934924053041387, 'scale_pos_weight': 1.3416634643942646}. Best is trial 0 with value: 0.9367088607594937.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:24,919] Trial 2 finished with value: 0.8783783783783784 and parameters: {'n_estimators': 50, 'max_depth': 12, 'learning_rate': 0.017272656360376337, 'subsample': 0.6299453629380181, 'colsample_bytree': 0.6980577328338535, 'gamma': 0.4505316002367643, 'scale_pos_weight': 1.1586582246369432}. Best is trial 0 with value: 0.9367088607594937.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:24] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:25,408] Trial 3 finished with value: 0.9135802469135802 and parameters: {'n_estimators': 50, 'max_depth': 6, 'learning_rate': 0.1549124184068399, 'subsample': 0.6624821574846934, 'colsample_bytree': 0.720170944669428, 'gamma': 3.6625351619060753, 'scale_pos_weight': 2.2628845302187948}. Best is trial 0 with value: 0.9367088607594937.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:25,525] Trial 4 finished with value: 0.9240506329113924 and parameters: {'n_estimators': 50, 'max_depth': 10, 'learning_rate': 0.19676011182382522, 'subsample': 0.9994179557679629, 'colsample_bytree': 0.6733486134827903, 'gamma': 2.0197709771013486, 'scale_pos_weight': 2.075802892830148}. Best is trial 0 with value: 0.9367088607594937.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:25,737] Trial 5 finished with value: 0.9044585987261147 and parameters: {'n_estimators': 300, 'max_depth': 13, 'learning_rate': 0.29747744153551803, 'subsample': 0.9226938233792521, 'colsample_bytree': 0.8485181571574323, 'gamma': 8.097122404739238, 'scale_pos_weight': 2.1324075214400975}. Best is trial 0 with value: 0.9367088607594937.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:25,892] Trial 6 finished with value: 0.9240506329113924 and parameters: {'n_estimators': 50, 'max_depth': 9, 'learning_rate': 0.06551510350406439, 'subsample': 0.7912987486468303, 'colsample_bytree': 0.7915028058093103, 'gamma': 2.8617746829474977, 'scale_pos_weight': 1.5984017773061272}. Best is trial 0 with value: 0.9367088607594937.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:25] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:26,166] Trial 7 finished with value: 0.8961038961038961 and parameters: {'n_estimators': 350, 'max_depth': 11, 'learning_rate': 0.05147477500099555, 'subsample': 0.7315348660423389, 'colsample_bytree': 0.9370650589287641, 'gamma': 5.742672112244765, 'scale_pos_weight': 1.1490784655032888}. Best is trial 0 with value: 0.9367088607594937.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:26,374] Trial 8 finished with value: 0.9259259259259259 and parameters: {'n_estimators': 350, 'max_depth': 13, 'learning_rate': 0.16922532082723432, 'subsample': 0.8206323978193818, 'colsample_bytree': 0.8481849009974265, 'gamma': 7.373751292776229, 'scale_pos_weight': 2.6545165756020612}. Best is trial 0 with value: 0.9367088607594937.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:26,504] Trial 9 finished with value: 0.9202453987730062 and parameters: {'n_estimators': 150, 'max_depth': 3, 'learning_rate': 0.07715932348487572, 'subsample': 0.9770093030711183, 'colsample_bytree': 0.7639922733773402, 'gamma': 3.1788035881765286, 'scale_pos_weight': 2.7168130362994867}. Best is trial 0 with value: 0.9367088607594937.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:26] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:27,997] Trial 10 finished with value: 0.9433962264150944 and parameters: {'n_estimators': 500, 'max_depth': 15, 'learning_rate': 0.012694329674994785, 'subsample': 0.8495281132391621, 'colsample_bytree': 0.9878910265637622, 'gamma': 0.19283628211459258, 'scale_pos_weight': 1.697651305834433}. Best is trial 10 with value: 0.9433962264150944.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:28] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:29,189] Trial 11 finished with value: 0.9433962264150944 and parameters: {'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.01137442615227626, 'subsample': 0.8380480118636219, 'colsample_bytree': 0.9876392056019081, 'gamma': 0.3370840518545004, 'scale_pos_weight': 1.6937824682878246}. Best is trial 10 with value: 0.9433962264150944.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:29] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:30,240] Trial 12 finished with value: 0.95 and parameters: {'n_estimators': 500, 'max_depth': 15, 'learning_rate': 0.029671760270307303, 'subsample': 0.8764282939090191, 'colsample_bytree': 0.9986883954284806, 'gamma': 0.049677908601566234, 'scale_pos_weight': 1.748839030874359}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:30,489] Trial 13 finished with value: 0.9171974522292994 and parameters: {'n_estimators': 450, 'max_depth': 15, 'learning_rate': 0.028830303068208565, 'subsample': 0.89076493988661, 'colsample_bytree': 0.9948851555760517, 'gamma': 9.629607861637545, 'scale_pos_weight': 1.7787683202640119}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:30] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:31,310] Trial 14 finished with value: 0.9375 and parameters: {'n_estimators': 400, 'max_depth': 15, 'learning_rate': 0.0393013114981539, 'subsample': 0.7507877991963225, 'colsample_bytree': 0.6062344281449955, 'gamma': 0.22122499066788387, 'scale_pos_weight': 1.8646149978769895}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:31] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:32,248] Trial 15 finished with value: 0.9325153374233128 and parameters: {'n_estimators': 450, 'max_depth': 15, 'learning_rate': 0.010600368901411163, 'subsample': 0.8821730422733828, 'colsample_bytree': 0.9237165353855371, 'gamma': 5.14903503930082, 'scale_pos_weight': 2.384344537032641}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:32,876] Trial 16 finished with value: 0.9375 and parameters: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.033200007393484846, 'subsample': 0.7687823714734306, 'colsample_bytree': 0.9448774465770322, 'gamma': 1.5423183629573398, 'scale_pos_weight': 3.0455509622228116}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:32] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:33,644] Trial 17 finished with value: 0.925 and parameters: {'n_estimators': 450, 'max_depth': 14, 'learning_rate': 0.018539185251510878, 'subsample': 0.6858561207025461, 'colsample_bytree': 0.8917965856037754, 'gamma': 3.948483060621646, 'scale_pos_weight': 1.550093096374262}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:33] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:34,057] Trial 18 finished with value: 0.9390243902439024 and parameters: {'n_estimators': 400, 'max_depth': 4, 'learning_rate': 0.08961303133222813, 'subsample': 0.9541354030168645, 'colsample_bytree': 0.9757671936069044, 'gamma': 1.7435722204957897, 'scale_pos_weight': 2.007470632118049}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:34] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:35,216] Trial 19 finished with value: 0.9433962264150944 and parameters: {'n_estimators': 500, 'max_depth': 14, 'learning_rate': 0.02358625564764855, 'subsample': 0.8679785295499121, 'colsample_bytree': 0.8832011689546247, 'gamma': 0.9432932458095704, 'scale_pos_weight': 1.4833923344080342}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:35,819] Trial 20 finished with value: 0.9316770186335404 and parameters: {'n_estimators': 400, 'max_depth': 11, 'learning_rate': 0.04651864022312467, 'subsample': 0.7144095475068598, 'colsample_bytree': 0.9500443379752795, 'gamma': 2.534885516853664, 'scale_pos_weight': 1.8887540966886907}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:35] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:37,411] Trial 21 finished with value: 0.9433962264150944 and parameters: {'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.010716093819260392, 'subsample': 0.8373553270326582, 'colsample_bytree': 0.9979608249051382, 'gamma': 0.3515306624907669, 'scale_pos_weight': 1.7122568123785886}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:37] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:38,565] Trial 22 finished with value: 0.9367088607594937 and parameters: {'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.01447739717480482, 'subsample': 0.7947421257260794, 'colsample_bytree': 0.9666889616475643, 'gamma': 0.9389017342729311, 'scale_pos_weight': 1.426092225558891}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:38] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:39,301] Trial 23 finished with value: 0.9433962264150944 and parameters: {'n_estimators': 450, 'max_depth': 5, 'learning_rate': 0.012282586832074546, 'subsample': 0.867740834601496, 'colsample_bytree': 0.9955413662582593, 'gamma': 0.22537824537264964, 'scale_pos_weight': 1.7413095585564273}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:39,835] Trial 24 finished with value: 0.9433962264150944 and parameters: {'n_estimators': 350, 'max_depth': 9, 'learning_rate': 0.025495531496412475, 'subsample': 0.9173463556194866, 'colsample_bytree': 0.9161989905601097, 'gamma': 1.1166406748494164, 'scale_pos_weight': 1.642557352818179}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:39] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:41,530] Trial 25 finished with value: 0.9433962264150944 and parameters: {'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.013758306276562314, 'subsample': 0.830708719666537, 'colsample_bytree': 0.9640693430661883, 'gamma': 0.005325606327803323, 'scale_pos_weight': 1.9856356996334363}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:41] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:42,135] Trial 26 finished with value: 0.9375 and parameters: {'n_estimators': 450, 'max_depth': 14, 'learning_rate': 0.019038870712855135, 'subsample': 0.8031745497582027, 'colsample_bytree': 0.8691169767408226, 'gamma': 2.2336716602694473, 'scale_pos_weight': 2.4643054573351186}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:42,411] Trial 27 finished with value: 0.9316770186335404 and parameters: {'n_estimators': 400, 'max_depth': 12, 'learning_rate': 0.03952719516433289, 'subsample': 0.9447330337100227, 'colsample_bytree': 0.9247862342274358, 'gamma': 4.487608593322433, 'scale_pos_weight': 2.203193762540545}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:42,881] Trial 28 finished with value: 0.9433962264150944 and parameters: {'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.03143850580968266, 'subsample': 0.851524543239505, 'colsample_bytree': 0.9706432844950922, 'gamma': 0.9782057575203575, 'scale_pos_weight': 1.9061842490113063}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:42] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:43,974] Trial 29 finished with value: 0.9230769230769231 and parameters: {'n_estimators': 500, 'max_depth': 12, 'learning_rate': 0.010002831064808182, 'subsample': 0.8893177406607368, 'colsample_bytree': 0.8100283716658009, 'gamma': 1.5495416291848614, 'scale_pos_weight': 1.283802620510728}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:44,472] Trial 30 finished with value: 0.925 and parameters: {'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.015394675516279733, 'subsample': 0.7707459320613301, 'colsample_bytree': 0.8981420637082375, 'gamma': 3.291510188737896, 'scale_pos_weight': 1.4757457778835097}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:44] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:45,135] Trial 31 finished with value: 0.9367088607594937 and parameters: {'n_estimators': 500, 'max_depth': 14, 'learning_rate': 0.02327995188722957, 'subsample': 0.8596348451149431, 'colsample_bytree': 0.889351388160572, 'gamma': 0.9414483905106799, 'scale_pos_weight': 1.3492062916704455}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:45,891] Trial 32 finished with value: 0.9433962264150944 and parameters: {'n_estimators': 450, 'max_depth': 15, 'learning_rate': 0.019226976895908732, 'subsample': 0.9146672198841673, 'colsample_bytree': 0.9802039794803359, 'gamma': 0.816506103330342, 'scale_pos_weight': 1.5223036641520529}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:45] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:47,309] Trial 33 finished with value: 0.9433962264150944 and parameters: {'n_estimators': 500, 'max_depth': 14, 'learning_rate': 0.0230689348502992, 'subsample': 0.8251336393180894, 'colsample_bytree': 0.9469413883180008, 'gamma': 0.015589324598923121, 'scale_pos_weight': 1.2613117703867636}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:47] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:48,088] Trial 34 finished with value: 0.9367088607594937 and parameters: {'n_estimators': 450, 'max_depth': 13, 'learning_rate': 0.013074318343665525, 'subsample': 0.8727588500623449, 'colsample_bytree': 0.9108392111972686, 'gamma': 1.4387206447375998, 'scale_pos_weight': 1.6849130112176514}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:48,644] Trial 35 finished with value: 0.9090909090909091 and parameters: {'n_estimators': 400, 'max_depth': 13, 'learning_rate': 0.01590540942864668, 'subsample': 0.6128300917137882, 'colsample_bytree': 0.8735254939405483, 'gamma': 2.3384327585533615, 'scale_pos_weight': 1.0823343125045253}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:48] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:49,524] Trial 36 finished with value: 0.9433962264150944 and parameters: {'n_estimators': 500, 'max_depth': 12, 'learning_rate': 0.01996781025709045, 'subsample': 0.8956489744778943, 'colsample_bytree': 0.8185476293198292, 'gamma': 0.5317000869002146, 'scale_pos_weight': 1.8409122548914176}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:49,787] Trial 37 finished with value: 0.9299363057324841 and parameters: {'n_estimators': 150, 'max_depth': 10, 'learning_rate': 0.027472012916325558, 'subsample': 0.8064066995954948, 'colsample_bytree': 0.9512261131492307, 'gamma': 2.012348045854994, 'scale_pos_weight': 1.3900547360271618}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:49] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:50,246] Trial 38 finished with value: 0.9433962264150944 and parameters: {'n_estimators': 250, 'max_depth': 14, 'learning_rate': 0.037342651321391127, 'subsample': 0.8495796361253128, 'colsample_bytree': 0.7712522403737214, 'gamma': 0.7039375672701216, 'scale_pos_weight': 1.61259908167507}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:50,402] Trial 39 finished with value: 0.9182389937106918 and parameters: {'n_estimators': 350, 'max_depth': 15, 'learning_rate': 0.1138485837361575, 'subsample': 0.9071649580496471, 'colsample_bytree': 0.7297192252371355, 'gamma': 6.684720143527752, 'scale_pos_weight': 2.0165352453427925}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:50,650] Trial 40 finished with value: 0.9299363057324841 and parameters: {'n_estimators': 450, 'max_depth': 9, 'learning_rate': 0.06074959271229382, 'subsample': 0.7768942858118778, 'colsample_bytree': 0.863527782142893, 'gamma': 2.821612676642607, 'scale_pos_weight': 1.2104905174970433}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:50] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:51,597] Trial 41 finished with value: 0.9433962264150944 and parameters: {'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.011901433972219672, 'subsample': 0.8432160027947703, 'colsample_bytree': 0.9997482984544442, 'gamma': 0.5120962087237935, 'scale_pos_weight': 1.730779990076545}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:51] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:52,374] Trial 42 finished with value: 0.9367088607594937 and parameters: {'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.011561556920461041, 'subsample': 0.8359886848973185, 'colsample_bytree': 0.9893539830870945, 'gamma': 1.1061174093843098, 'scale_pos_weight': 1.523329156467128}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:52] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:53,125] Trial 43 finished with value: 0.9433962264150944 and parameters: {'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.016029665564398565, 'subsample': 0.9350985768534721, 'colsample_bytree': 0.965457097676065, 'gamma': 0.49742479573009885, 'scale_pos_weight': 1.8053869762623118}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:53,769] Trial 44 finished with value: 0.9367088607594937 and parameters: {'n_estimators': 450, 'max_depth': 6, 'learning_rate': 0.010446359988615041, 'subsample': 0.8184430692646627, 'colsample_bytree': 0.9256620269633086, 'gamma': 1.5430264865083738, 'scale_pos_weight': 1.6539197440696987}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:53] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:54,114] Trial 45 finished with value: 0.9056603773584906 and parameters: {'n_estimators': 400, 'max_depth': 11, 'learning_rate': 0.013536741572435179, 'subsample': 0.8640311983573704, 'colsample_bytree': 0.99973358057179, 'gamma': 8.61657288900851, 'scale_pos_weight': 2.099280376943109}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:54,391] Trial 46 finished with value: 0.9161290322580645 and parameters: {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.016985668795382384, 'subsample': 0.9695407469184215, 'colsample_bytree': 0.8326155554247656, 'gamma': 0.03584505032932971, 'scale_pos_weight': 1.4201427253968995}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:54,600] Trial 47 finished with value: 0.9308176100628931 and parameters: {'n_estimators': 450, 'max_depth': 15, 'learning_rate': 0.23968427061507316, 'subsample': 0.878034232520899, 'colsample_bytree': 0.981216753598706, 'gamma': 1.9041959956538972, 'scale_pos_weight': 1.925226315295376}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:54] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:55,170] Trial 48 finished with value: 0.9433962264150944 and parameters: {'n_estimators': 500, 'max_depth': 8, 'learning_rate': 0.023615458937941907, 'subsample': 0.8142784537362557, 'colsample_bytree': 0.9566102930213372, 'gamma': 1.2127043227654157, 'scale_pos_weight': 1.7314887796489862}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "[I 2025-03-13 19:54:55,484] Trial 49 finished with value: 0.9032258064516129 and parameters: {'n_estimators': 450, 'max_depth': 13, 'learning_rate': 0.02109417340434984, 'subsample': 0.747898312010211, 'colsample_bytree': 0.6462543841479922, 'gamma': 5.991775451426968, 'scale_pos_weight': 1.561379708259462}. Best is trial 12 with value: 0.95.\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [19:54:55] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 500, 'max_depth': 15, 'learning_rate': 0.029671760270307303, 'subsample': 0.8764282939090191, 'colsample_bytree': 0.9986883954284806, 'gamma': 0.049677908601566234, 'scale_pos_weight': 1.748839030874359}\n",
      "Model trained with accuracy: 0.9606\n",
      "Precision: 0.9737, Recall: 0.9024, F1-score: 0.9367, ROC-AUC: 0.9924\n",
      "Confusion Matrix:\n",
      "[[170   8]\n",
      " [  2  74]]\n",
      "Activity: CD, X shape: (4, 13), y shape: (4,)\n",
      "Activity CD: 4 correct, 0 incorrect\n",
      "Activity: FCF, X shape: (12, 13), y shape: (12,)\n",
      "Activity FCF: 11 correct, 1 incorrect\n",
      "Activity: FCS, X shape: (10, 13), y shape: (10,)\n",
      "Activity FCS: 7 correct, 3 incorrect\n",
      "Activity: FOB, X shape: (12, 13), y shape: (12,)\n",
      "Activity FOB: 12 correct, 0 incorrect\n",
      "Activity: FOL, X shape: (12, 13), y shape: (12,)\n",
      "Activity FOL: 12 correct, 0 incorrect\n",
      "Activity: FR, X shape: (12, 13), y shape: (12,)\n",
      "Activity FR: 11 correct, 1 incorrect\n",
      "Activity: K, X shape: (24, 13), y shape: (24,)\n",
      "Activity K: 23 correct, 1 incorrect\n",
      "Activity: KD, X shape: (2, 13), y shape: (2,)\n",
      "Activity KD: 2 correct, 0 incorrect\n",
      "Activity: KID, X shape: (4, 13), y shape: (4,)\n",
      "Activity KID: 4 correct, 0 incorrect\n",
      "Activity: LAF, X shape: (12, 13), y shape: (12,)\n",
      "Activity LAF: 10 correct, 2 incorrect\n",
      "Activity: LC, X shape: (12, 13), y shape: (12,)\n",
      "Activity LC: 12 correct, 0 incorrect\n",
      "Activity: LSF, X shape: (12, 13), y shape: (12,)\n",
      "Activity LSF: 12 correct, 0 incorrect\n",
      "Activity: MA, X shape: (9, 13), y shape: (9,)\n",
      "Activity MA: 9 correct, 0 incorrect\n",
      "Activity: PUF, X shape: (12, 13), y shape: (12,)\n",
      "Activity PUF: 12 correct, 0 incorrect\n",
      "Activity: RBS, X shape: (18, 13), y shape: (18,)\n",
      "Activity RBS: 18 correct, 0 incorrect\n",
      "Activity: S, X shape: (9, 13), y shape: (9,)\n",
      "Activity S: 9 correct, 0 incorrect\n",
      "Activity: SC, X shape: (12, 13), y shape: (12,)\n",
      "Activity SC: 12 correct, 0 incorrect\n",
      "Activity: SFB, X shape: (12, 13), y shape: (12,)\n",
      "Activity SFB: 12 correct, 0 incorrect\n",
      "Activity: SLB, X shape: (12, 13), y shape: (12,)\n",
      "Activity SLB: 10 correct, 2 incorrect\n",
      "Activity: STC, X shape: (12, 13), y shape: (12,)\n",
      "Activity STC: 12 correct, 0 incorrect\n",
      "Activity: TF, X shape: (12, 13), y shape: (12,)\n",
      "Activity TF: 12 correct, 0 incorrect\n",
      "Activity: WBS, X shape: (18, 13), y shape: (18,)\n",
      "Activity WBS: 18 correct, 0 incorrect\n",
      "\n",
      "📊 Per-Activity Results:\n",
      "                               activity Actual Fall  correct  incorrect  \\\n",
      "0                            Close Door     No Fall        4          0   \n",
      "1                 Chair - Fall to Front        Fall       11          1   \n",
      "2                  Chair - Fall to side        Fall        7          3   \n",
      "3             Fall of object (Backpack)     No Fall       12          0   \n",
      "4         Fall of object (FaszienRolle)     No Fall       12          0   \n",
      "5                         Fall Recovery     No Fall       11          1   \n",
      "6        Kneeling down then standing up     No Fall       23          1   \n",
      "7                            Knock Door     No Fall        2          0   \n",
      "8                          Kids Running     No Fall        4          0   \n",
      "9                    Lying - Awake Fall        Fall       10          2   \n",
      "10                 Laying down on couch     No Fall       12          0   \n",
      "11                  Lying - Asleep Fall        Fall       12          0   \n",
      "12  Minor Ambience (Sitting and Eating)     No Fall        9          0   \n",
      "13      Picking something up from floor     No Fall       12          0   \n",
      "14                       Rush by Sensor     No Fall       18          0   \n",
      "15                                Still     No Fall        9          0   \n",
      "16                Sitting down on chair     No Fall       12          0   \n",
      "17            Slip and Fall - Backwards        Fall       12          0   \n",
      "18                Standing Lost Balance        Fall       10          2   \n",
      "19                  Stand up from Chair     No Fall       12          0   \n",
      "20             Trip and Fall - Forwards        Fall       12          0   \n",
      "21                       Walk by Sensor     No Fall       18          0   \n",
      "\n",
      "    total  accuracy  \n",
      "0       4  1.000000  \n",
      "1      12  0.916667  \n",
      "2      10  0.700000  \n",
      "3      12  1.000000  \n",
      "4      12  1.000000  \n",
      "5      12  0.916667  \n",
      "6      24  0.958333  \n",
      "7       2  1.000000  \n",
      "8       4  1.000000  \n",
      "9      12  0.833333  \n",
      "10     12  1.000000  \n",
      "11     12  1.000000  \n",
      "12      9  1.000000  \n",
      "13     12  1.000000  \n",
      "14     18  1.000000  \n",
      "15      9  1.000000  \n",
      "16     12  1.000000  \n",
      "17     12  1.000000  \n",
      "18     12  0.833333  \n",
      "19     12  1.000000  \n",
      "20     12  1.000000  \n",
      "21     18  1.000000  \n",
      "Distance: 0, X shape: (13, 13), y shape: (13,)\n",
      "Distance 0: 13 correct, 0 incorrect\n",
      "Distance: 1, X shape: (64, 13), y shape: (64,)\n",
      "Distance 1: 63 correct, 1 incorrect\n",
      "Distance: 2, X shape: (92, 13), y shape: (92,)\n",
      "Distance 2: 87 correct, 5 incorrect\n",
      "Distance: 3, X shape: (85, 13), y shape: (85,)\n",
      "Distance 3: 81 correct, 4 incorrect\n",
      "\n",
      "📊 Per-Distance Results:\n",
      "   distance  correct  incorrect  total  accuracy\n",
      "0         0       13          0     13  1.000000\n",
      "1         1       63          1     64  0.984375\n",
      "2         2       87          5     92  0.945652\n",
      "3         3       81          4     85  0.952941\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trained_model = train_pipe(\n",
    "    model=model,\n",
    "    dataset_name=\"MPU_features.csv\",\n",
    "    save_name=\"TEST\",\n",
    "    feature_columns=[\"max\", \"min\", \"mean\", \"std\", \"median\", \"peak\", \"p2p\", \"energy\", \"rms\", \"crest\", \"shape\", \"impulse\", \"margin\"],\n",
    "    target_column=\"fall_binary\",\n",
    "    experiment_name=\"Classic Variants MPU\",\n",
    "    use_early_stopping=False,\n",
    "    if_optuna=True,\n",
    "    n_trials=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtMAAAIjCAYAAADBZpcoAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAATD5JREFUeJzt3Qd4FGX3//8TkhBCQkLvSO8mSJHeVJoKgigoIKEJiICigooUqaIEFAQUBaWJIAIqikhHFKkBpIrUB9QgiJhQJECy/+vc/+/uLxuSQMYku5t9v65rnuzOzM7cu/OAnxzO3Otjs9lsAgAAACDNsqX9JQAAAAAUYRoAAACwiDANAAAAWESYBgAAACwiTAMAAAAWEaYBAAAAiwjTAAAAgEWEaQAAAMAiwjQAAABgEWEaAAAAsIgwDQAiMnfuXPHx8Ul2efXVVzPknD/99JOMGjVK/vnnH3HXz2PXrl3iqd577z3zPgAgI/ll6NEBwMOMGTNGSpcu7bTu7rvvzrAwPXr0aOnevbvkzp07Q87hzTRM58+f33y+AJBRCNMAkMiDDz4otWrVEk925coVCQoKEm919epVyZkzp6uHAcBL0OYBAGmwatUqadSokQmruXLlkocfflgOHjzotM++fftMNbRMmTKSI0cOKVy4sPTs2VMuXLjg2EfbO4YMGWIeayXc3lJy6tQps+jj5FoUdL2+NvFxdN2hQ4ekc+fOkidPHmnYsKFj+yeffCI1a9aUwMBAyZs3rzz55JNy5swZS+9d31NwcLCcPn1aWrdubR4XK1ZMZsyYYbbv379f7r//fvPZlCxZUj799NNkW0c2b94sffv2lXz58klISIhERETIxYsXk60sV61aVQICAqRo0aLSv3//W1pimjZtav7lICoqSho3bmxC9GuvvSalSpUy1+X77793fLa6r/r7779l8ODBEhYWZt6DjkF/ifr555+djr1p0ybzuiVLlsj48eOlePHi5no+8MADcuzYsVvGu337dnnooYfMNdDPIDw8XKZOneq0zy+//CKPP/64uRZ6LP3FbcWKFZauBwD3QGUaABKJiYmRv/76y2mdtgqoBQsWSLdu3aRly5by1ltvmQro+++/b8Lrnj17TIBTa9eulRMnTkiPHj1MkNZQ9+GHH5qf27ZtMwGtffv28uuvv8qiRYvknXfecZyjQIECcv78+TSPu0OHDlK+fHl54403xGazmXUaAEeMGCEdO3aUp59+2hx32rRpJnTqeK20lsTHx5vgqceYOHGiLFy4UAYMGGDC47Bhw6RLly7mvc2cOdOE5Hr16t3SNqP767n1F4EjR46Yz/B///ufI7wq3aYtMM2aNZN+/fo59tu5c6ds2bJF/P39HcfTX1J0TPqLwlNPPSWFChUywXngwIEmLOu4lK5Xem2+/PJL85np2P7880/54IMPpEmTJuaXEg3uib355puSLVs2E8D1/x/6vvV9ani202uuv2AUKVJEnn/+eXPdDx8+LN988415rvT6N2jQwPwCon34+plpUG/Xrp0sW7ZMHn300TRfDwBuwAYAsM2ZM0cTaLKLunTpki137ty23r17O73u7NmzttDQUKf1V69eveX4ixYtMsfavHmzY11kZKRZd/LkSad99bmu1zElpetff/11x3N9rOs6derktN+pU6dsvr6+tvHjxzut379/v83Pz++W9Sl9Hjt37nSs69atm1n3xhtvONZdvHjRFhgYaPPx8bEtXrzYsf6XX365Zaz2Y9asWdN2/fp1x/qJEyea9V999ZV5fu7cOVv27NltLVq0sMXHxzv2mz59utnv448/dqxr0qSJWTdz5sxb3kPVqlXN9qSuXbvmdFz7Zx4QEGAbM2aMY93GjRvNsStXrmyLi4tzrJ86dapZr5+lunnzpq106dK2kiVLms8jsYSEBMfjBx54wBYWFmbOn3h7/fr1beXLl79lnAA8A20eAJCItixolTHxovSnthh06tTJVK7ti6+vr9SpU0c2btzoOIa2VNhdu3bN7Fe3bl3zfPfu3Rky7meeecbp+fLlyyUhIcFUpROPVyumWsFOPN600iq3nVaYK1asaKqsei47XafbtAqcVJ8+fZwqy1p59vPzk2+//dY8X7dunVy/fl0GDRpkKsJ2vXv3Ni0ZK1eudDqetoHovwLcKd3fflyttGtlWyvYOubkro8eO3v27I7n2uaj7O9Nq/wnT540401a7bdX2rW1ZMOGDeYzunTpkuN66Ln1XzqOHj0qv//++x2/BwDugzYPAEikdu3ayd6AqGFHaU9wcjTk2Wlw0haFxYsXy7lz55z20zaBjJC0lULHq4VsDc7JSRxm00L7fLUVJbHQ0FDTT2wPjonXJ9cLnXRMGmS1PUJ7xZW2fCgNt4lpoNU+dPt2O22bSBx2b0d/ydBeZu3J1hCsgdpO+7iTuuuuu5yea0+0sr+348eP33bWF+2x1uuhbTe6JEf/v6LvBYBnIUwDwB0GMHvftFZ3k9LKqp1WH3XaO73B8J577jFhUV/fqlUrx3FSkzSU2iUOfUklrobbx6vH0RsmtXqelI7JiuSOldp6e/92Rkr63m9H+8o10OpNoWPHjjU3A2qlWivLyV2f9Hhv9uNq37VWopNTrly5Oz4eAPdBmAaAO1C2bFnzs2DBguamuJRotXL9+vWmMj1y5MhbKtt3Eprtlc+kM1ckrcjebrwa9rRiXaFCBXEn+lncd999jueXL1+W6OhoMxOG0plAlN50qJVoO2390Epyap//nXy+S5cuNef/6KOPnNbr522/EdTK/zcOHDiQ4tjs70P/ReBOxw/AM9AzDQB3QKuJ2sqhVc0bN27cst0+A4e9ipm0ajllypRbXmOfCzppaNbzaKjTKeQS07aEO6UzauhYNNQnHYs+TzxNX2bTmU0Sf4Y6S8fNmzfNjBxKw6a2bbz77rtOY9fwq20yOh3hndDPN7lvl9TPJeln8vnnn1vuWa5Ro4b5pUWvcdLz2c+jv4TpDCM6a4j+4pCUlRlcALgHKtMAcAc04Gro69q1qwlPOg2b9g7rnMt6Q5xOeTZ9+nSzn33aOA2M2gO7Zs0aU1FNSud/Vjp1mx5Pq5Zt2rQxIVBv8tMp2fSn9nBrsNap9NJSLR03bpwMHTrU9CLr9Gs6L7aO44svvjA3AWrLgStohVnnatZ2GK0+6y8JOr3gI488Yrbr56rj1l8EtDVG19v3u/fee830d3dCP1+9Zvo5aAuFBlrtedcp7PSbLvXGwvr165v5sXWKv8RV8LTQFhE9j147bevR42oPuM4prdPhrV692nFzq75Pnd9ab6bU8+m0fFu3bpXffvvtlnmuAXgIV08nAgDuILmp4JKj06W1bNnSTIeXI0cOW9myZW3du3e37dq1y7HPb7/9Znv00UfNVHq6X4cOHWx//PHHLVPFqbFjx9qKFStmy5Ytm9M0eTq9Xq9evczrc+XKZevYsaOZMi6lqfHOnz+f7HiXLVtma9iwoS0oKMgslSpVsvXv39925MgRS1Pj6TGS0unndBq6pHSquIcffviWY37//fe2Pn362PLkyWMLDg62denSxXbhwoVbXq9T4el4/f39bYUKFbL169fvlqnnUjq3fdpCPb9+fnpe+zR5OjXdSy+9ZCtSpIiZ1q9Bgwa2rVu3mu2Jp9KzT433+eef39HUhT/++KOtefPm5nz6OYWHh9umTZvmtM/x48dtERERtsKFC5v3pde+devWtqVLlyb7HgC4Px/9H1cHegBA1qffgKhVW/3iFU//ynYAsKNnGgAAALCIMA0AAABYRJgGAAAALKJnGgAAALCIyjQAAABgEWEaAAAAsIgvbXGBhIQE+eOPP8wXKKT0dbcAAABwHe2EvnTpkhQtWtR8OVNKCNMuoEG6RIkSrh4GAAAAbuPMmTNSvHjxFLcTpl1AK9L2i6NfPQwAAAD3Ehsba4qf9tyWEsK0C9hbOzRIE6YBAADc1+1acgnTLtR4+CLxDQh09TAAAADcWlRkhLgrZvMAAAAALCJMAwAAABYRpgEAAACLCNMAAACARYRpAAAAwCLCNAAAAGARYRoAAACwiDANAAAAWESYBgAAACzyqjDdtGlTGTRokON5qVKlZMqUKS4dEwAAADyXV3+d+M6dOyUoKMjVwwAAAICH8uowXaBAAVcPAQAAAB4sm7u0XwwcONC0YOTJk0cKFSoks2bNkitXrkiPHj0kV65cUq5cOVm1apXjNQcOHJAHH3xQgoODzf5du3aVv/76y7FdXxsREWG2FylSRCZPnnzLeZO2ebz99tsSFhZmqtUlSpSQZ599Vi5fvuzYPnfuXMmdO7esXr1aKleubI7dqlUriY6OTvX9xcXFSWxsrNMCAAAAz+cWYVrNmzdP8ufPLzt27DDBul+/ftKhQwepX7++7N69W1q0aGEC89WrV+Wff/6R+++/X6pXry67du2S7777Tv7880/p2LGj43hDhgyR77//Xr766itZs2aNbNq0yRwnNdmyZZN3331XDh48aMazYcMGefnll5320fNPmjRJFixYIJs3b5bTp0/L4MGDUz3uhAkTJDQ01LFoUAcAAIDn87HZbDZ3qEzHx8fLDz/8YJ7rYw2d7du3l/nz55t1Z8+eNRXmrVu3yrp168y+WiG2++2330xIPXLkiBQtWlTy5csnn3zyiQnk6u+//5bixYtLnz59HNVorUxrNTzxTYmJLV26VJ555hlHxVsr01opP3bsmJQtW9ase++992TMmDFmfKlVpnWx08q0jrXawJniGxCYDp8gAABA1hUVGZHp59S8pnk0JiZGQkJC3L9nOjw83PHY19fXhGFtubDTVg517tw5+fnnn2Xjxo2mzSKp48ePy7///ivXr1+XOnXqONbnzZtXKlasmOoYNKRrFfmXX34xH+DNmzfl2rVrphqdM2dOs4/+tAdppQFfx5SagIAAswAAACBrcZs2D39/f6fnPj4+Tuv0uUpISDB9zG3atJG9e/c6LUePHpXGjRtbOv+pU6ekdevWJtQvW7ZMoqKiZMaMGWabBvPUxukGxX0AAAC4gNtUptOiRo0aJvBqm4af361vQSvHGnq3b98ud911l1l38eJF+fXXX6VJkybJHlPDswZ1vVFRe6fVkiVLMvidAAAAwJO5TWU6Lfr37296oDt16mTmitbWDu2f1n5m7bfW9o9evXqZmxD1JkKd+aN79+6OkJwcnS3kxo0bMm3aNDlx4oS5wXDmzJmZ+r4AAADgWTwyTOsNhlu2bDHBWWf50N5qvYlQp62zB+bIyEhp1KiRaQdp1qyZNGzYUGrWrJniMatVq2amxnvrrbfk7rvvloULF5r+aQAAAMCtZ/PwNva7Q5nNAwAAwLNn8/DIyjQAAADgDgjTAAAAgEWEaQAAAMAiwjQAAABgEWEaAAAAsIgwDQAAAFhEmAYAAAC86evEs4rN4zqlOm8hAAAA3BuVaQAAAMAiwjQAAABgEWEaAAAAsIgwDQAAAFhEmAYAAAAsIkwDAAAAFjE1ngs1Hr5IfAMCXT0M/J+oyAhXDwEAAHgYKtMAAACARYRpAAAAwCLCNAAAAGARYRoAAACwiDANAAAAWESYBgAAACwiTAMAAAAWEaYBAAAAiwjTadS0aVMZNGiQq4cBAAAAN0CYBgAAACwiTAMAAAAWZctqLRgDBgwwS2hoqOTPn19GjBghNpvNbI+Li5PBgwdLsWLFJCgoSOrUqSObNm1yvP7ChQvSqVMnsz1nzpwSFhYmixYtSvWcK1euNOdauHBhhr8/AAAAuJcsFabVvHnzxM/PT3bs2CFTp06Vt99+W2bPnm22acjeunWrLF68WPbt2ycdOnSQVq1aydGjR832a9euSc2aNU1APnDggPTp00e6du1qjpWcTz/91IRvDdJdunRJcUwa4mNjY50WAAAAeD4fm71sm0Uq0+fOnZODBw+Kj4+PWffqq6/KihUr5LvvvpMyZcrI6dOnpWjRoo7XNGvWTGrXri1vvPFGssds3bq1VKpUSSZNmuQ4xz333CPly5eXYcOGyVdffSVNmjRJdVyjRo2S0aNH37K+2sCZ4hsQ+B/fNdJLVGSEq4cAAADchBY/tfsgJiZGQkJCUtzPT7KYunXrOoK0qlevnkyePFn2798v8fHxUqFChVuqxvny5TOPdbuG6iVLlsjvv/8u169fN9u15SOxpUuXmtC+ZcsWuffee287pqFDh8qLL77odHFKlCiRDu8WAAAArpTlwnRKLl++LL6+vhIVFWV+JhYcHGx+RkZGmtaQKVOmmH5p7avWafA0VCdWvXp12b17t3z88cdSq1Ytp/CenICAALMAAAAga8lyYXr79u1Oz7dt22ZaMjQAa+VZK8qNGjVK9rVaaW7btq089dRT5nlCQoL8+uuvUqVKFaf9ypYta6rd2vKhwXz69OkZ+I4AAADgrrLcDYjaE60tFUeOHDEzcUybNk2ef/55096hNwlGRETI8uXL5eTJk+bGwgkTJpgbDpWG7rVr18pPP/0khw8flr59+8qff/6Z7Hn0eBs3bpRly5bxJS4AAABeKstVpjUs//vvv+amQq0aa5DWWTnUnDlzZNy4cfLSSy+ZnmidOk97rPUmQzV8+HA5ceKEtGzZ0vRJ6+vatWtnGs+TU7FiRdmwYYOjQq3VagAAAHiPLDebh860oT3PnnB3KLN5uBdm8wAAAGmdzSPLtXkAAAAAmYUwDQAAAFiUpXqmE381OAAAAJDRqEwDAAAAFhGmAQAAAIsI0wAAAIBFhGkAAADAoix1A6Kn2TyuU6rzFgIAAMC9UZkGAAAALCJMAwAAABYRpgEAAACLCNMAAACARYRpAAAAwCLCNAAAAGARU+O5UOPhi8Q3INDVw/AIUZERrh4CAADALahMAwAAABYRpgEAAACLCNMAAACARYRpAAAAwCLCNAAAAGARYRoAAACwiDANAAAAWESYBgAAACwiTAMAAAAWEaYBAAAAiwjTAAAAgEWEaQAAAMDbwnTTpk1l4MCBMmjQIMmTJ48UKlRIZs2aJVeuXJEePXpIrly5pFy5crJq1SrHaw4cOCAPPvigBAcHm/27du0qf/31l2P7d999Jw0bNpTcuXNLvnz5pHXr1nL8+HHH9lOnTomPj48sX75c7rvvPsmZM6dUq1ZNtm7dmunvHwAAAK7nsWFazZs3T/Lnzy87duwwwbpfv37SoUMHqV+/vuzevVtatGhhAvPVq1fln3/+kfvvv1+qV68uu3btMsH5zz//lI4dOzqOp0H8xRdfNNvXr18v2bJlk0cffVQSEhKczjts2DAZPHiw7N27VypUqCCdOnWSmzdvpjjOuLg4iY2NdVoAAADg+XxsNptNPLQyHR8fLz/88IN5ro9DQ0Olffv2Mn/+fLPu7NmzUqRIEVM5Xrdundl39erVjmP89ttvUqJECTly5IgJxUlp1bpAgQKyf/9+ufvuu01lunTp0jJ79mzp1auX2efQoUNStWpVOXz4sFSqVCnZsY4aNUpGjx59y/pqA2eKb0Bgun0mWVlUZISrhwAAALxIbGysyZYxMTESEhKSNSvT4eHhjse+vr6mNSMsLMyxTls51Llz5+Tnn3+WjRs3mhYP+2IPv/ZWjqNHj5oqc5kyZcyHVqpUKbP+9OnTKZ5Xw7r9HCkZOnSouRD25cyZM+n0CQAAAMCV/MSD+fv7Oz3XfubE6/S50jaNy5cvS5s2beStt9665Tj2QKzbS5YsaXqvixYtal6nFenr16+neN7E50hJQECAWQAAAJC1eHSYTosaNWrIsmXLTLXZz+/Wt33hwgXT7qFBulGjRmbdjz/+6IKRAgAAwFN4dJtHWvTv31/+/vtv08axc+dO09qh/dM684f2W+uMINom8uGHH8qxY8dkw4YN5mZEAAAAQLw9TGvbxpYtW0xw1lk+tLdap9XTafB01g5dFi9eLFFRUaa144UXXpDIyEhXDxsAAABuzGNn88gKd4cym8edYzYPAACQmbxiNg8AAADAlQjTAAAAgEWEaQAAAMAiwjQAAABgEWEaAAAAsIgwDQAAAFhEmAYAAAAs8pqvE3dHm8d1SnXeQgAAALg3KtMAAACARYRpAAAAwCLCNAAAAGARYRoAAACwiDANAAAAWESYBgAAACxiajwXajx8kfgGBLp6GG4hKjLC1UMAAABIMyrTAAAAgEWEaQAAAMAiwjQAAABgEWEaAAAAsIgwDQAAAFhEmAYAAAAsIkwDAAAAFhGmAQAAAIsI0wAAAIBFhGkAAADAIsI0AAAAYFGWDtNNmzaVgQMHyqBBgyRPnjxSqFAhmTVrlly5ckV69OghuXLlknLlysmqVavM/vHx8dKrVy8pXbq0BAYGSsWKFWXq1KmO4127dk2qVq0qffr0caw7fvy4Oc7HH3/skvcIAAAA18nSYVrNmzdP8ufPLzt27DDBul+/ftKhQwepX7++7N69W1q0aCFdu3aVq1evSkJCghQvXlw+//xzOXTokIwcOVJee+01WbJkiTlWjhw5ZOHCheaYX331lQnfTz31lDRv3lx69uyZ4hji4uIkNjbWaQEAAIDn87HZbDbJwpVpDbw//PCDea6PQ0NDpX379jJ//nyz7uzZs1KkSBHZunWr1K1b95ZjDBgwwOyzdOlSx7rIyEiZOHGiPPnkk7Js2TLZv3+/5MuXL8VxjBo1SkaPHn3L+moDZ4pvQGA6vVvPFhUZ4eohAAAAOGjxU3NjTEyMhISEiNdWpsPDwx2PfX19TegNCwtzrNPWD3Xu3Dnzc8aMGVKzZk0pUKCABAcHy4cffiinT592OuZLL70kFSpUkOnTp5v2jtSCtBo6dKi5EPblzJkz6fwuAQAA4ApZPkz7+/s7Pffx8XFap8+VtngsXrxYBg8ebPqm16xZI3v37jW91devX3c6hgbvX3/91YTzo0eP3nYMAQEB5jeaxAsAAAA8n5+rB+BOtmzZYnqpn332WacbDJPS/mitbmvo7t27tzRr1kwqV66cyaMFAACAqxGmEylfvrzppV69erWZ0WPBggWyc+dO89hO20C0v3rfvn1SokQJWblypXTp0kW2bdsm2bNnd+n4AQAAkLmyfJtHWvTt29fcnPjEE09InTp15MKFC05V6l9++UWGDBki7733ngnSSh//9ddfMmLECBeOHAAAAK6QpWfzcPe7Q5nN4/9hNg8AAOBOmM0DAAAAyGCEaQAAAMAiwjQAAABgEWEaAAAAsIgwDQAAAFhEmAYAAAAsIkwDAAAAFvENiC60eVynVOctBAAAgHujMg0AAABYRJgGAAAALCJMAwAAABYRpgEAAACLCNMAAACARYRpAAAAwCKmxnOhxsMXiW9AoLi7qMgIVw8BAADALVGZBgAAACwiTAMAAAAWEaYBAAAAiwjTAAAAgEWEaQAAAMAiwjQAAABgEWEaAAAAsIgwDQAAAFhEmAYAAAAsIkwDAAAAFhGm/8+NGzfklVdekbCwMAkKCpKiRYtKRESE/PHHH64eGgAAANwUYfr/XL16VXbv3i0jRowwP5cvXy5HjhyRRx55xNVDAwAAgJvyqjDdtGlTGTBggFlCQ0Mlf/78JjzbbDbzfO3atdKxY0epWLGi1K1bV6ZPny5RUVFy+vRp8/pTp06Jj4+PLF68WOrXry85cuSQu+++W77//ntXvzUAAAC4gFeFaTVv3jzx8/OTHTt2yNSpU+Xtt9+W2bNnJ7tvTEyMCc+5c+d2Wj9kyBB56aWXZM+ePVKvXj1p06aNXLhwIcVzxsXFSWxsrNMCAAAAz+d1YbpEiRLyzjvvmOpzly5dZODAgeZ5UteuXTM91J06dZKQkBCnbVrZfuyxx6Ry5cry/vvvm6r2Rx99lOI5J0yYYPaxLzoGAAAAeD6vC9PavqHVZjutLB89elTi4+OdbkbUdg9t/9CwnJS+xk6r3LVq1ZLDhw+neM6hQ4eaKrd9OXPmTLq+JwAAALiGn4vO67bsQfp///ufbNiw4ZaqtBUBAQFmAQAAQNbidZXp7du3Oz3ftm2blC9fXnx9fR1BWivV69atk3z58iV7DH2N3c2bN81NitryAQAAAO/idZVpnZnjxRdflL59+5op8KZNmyaTJ082Qfrxxx8367755hvT9nH27Fnzmrx580r27Nkdx5gxY4YJ4Bqgtd/64sWL0rNnTxe+KwAAALiC14Vp/SKWf//9V2rXrm2q0c8//7z06dPHtHWsWLHC7HPPPfc4vWbjxo1mWj27N9980yx79+6VcuXKmdfpNHsAAADwLl4Xpv39/WXKlCm33FhYqlQpc8PhndCKdNJ2EQAAAHgfr+uZBgAAANILYRoAAACwyKvaPDZt2vSfXp+WVhAAAABkfVSmAQAAAIsI0wAAAIBFhGkAAADAIsI0AAAAYJFX3YDobjaP6yQhISGuHgYAAAAsojINAAAAWESYBgAAACwiTAMAAAAWEaYBAAAAiwjTAAAAgEWEaQAAAMAipsZzocbDF4lvQGCmnzcqMiLTzwkAAJAVUZkGAAAALCJMAwAAABYRpgEAAACLCNMAAACARYRpAAAAwCLCNAAAAGARYRoAAACwiDANAAAAWESYTsWmTZvEx8dH/vnnH1cPBQAAAG6IMJ2K+vXrS3R0tISGhrp6KAAAAHBDfJ14KrJnzy6FCxd29TAAAADgpryqMt20aVMZOHCgDBo0SPLkySOFChWSWbNmyZUrV6RHjx6SK1cuKVeunKxatSrZNo+5c+dK7ty5ZfXq1VK5cmUJDg6WVq1ameo1AAAAvI9XhWk1b948yZ8/v+zYscME6379+kmHDh1MS8fu3bulRYsW0rVrV7l69Wqyr9f1kyZNkgULFsjmzZvl9OnTMnjw4FTPGRcXJ7GxsU4LAAAAPJ/Xhelq1arJ8OHDpXz58jJ06FDJkSOHCde9e/c260aOHCkXLlyQffv2Jfv6GzduyMyZM6VWrVpSo0YNGTBggKxfvz7Vc06YMMH0XduXEiVKZNC7AwAAQGbyujAdHh7ueOzr6yv58uWTsLAwxzpt/VDnzp1L9vU5c+aUsmXLOp4XKVIkxX3tNLTHxMQ4ljNnzqTDOwEAAICred0NiP7+/k7PtSc68Tp9rhISEu749TabLdVzBgQEmAUAAABZi9dVpgEAAID0QpgGAAAALCJMAwAAABb52G7X8It0p1Pj6awe1QbOFN+AwEw/f1RkRKafEwAAwBPzmk4eERISkuJ+VKYBAAAAiwjTAAAAgEWEaQAAAMAiwjQAAABgEWEaAAAAsIgwDQAAAFhEmAYAAAAs8rP6wgULFsjMmTPl5MmTsnXrVilZsqRMmTJFSpcuLW3btrV6WK+yeVynVOctBAAAQBasTL///vvy4osvykMPPST//POPxMfHm/W5c+c2gRoAAADwBpbC9LRp02TWrFkybNgw8fX1dayvVauW7N+/Pz3HBwAAAGStMK2tHdWrV79lfUBAgFy5ciU9xgUAAABkzTCtfdF79+69Zf13330nlStXTo9xAQAAAFnzBkTtl+7fv79cu3ZNbDab7NixQxYtWiQTJkyQ2bNnp/8oAQAAgKwSpp9++mkJDAyU4cOHy9WrV6Vz585StGhRmTp1qjz55JPpP0oAAAAgK4TpmzdvyqeffiotW7aULl26mDB9+fJlKViwYMaMEAAAAHBTPjbt00ijnDlzyuHDh83c0ki72NhYCQ0NlWoDZ4pvQGCmnTcqMiLTzgUAAJAV8lpMTEyq3wti6QbE2rVry549e/7L+AAAAADv7Jl+9tln5aWXXpLffvtNatasKUFBQU7bw8PD02t8AAAAQNYK0/abDJ977jnHOh8fHzOzh/60fyMiAAAAkJX5Wf3SFgAAAMDbWQrT3HgIAAAAWAzT8+fPT3V7RASzRgAAACDrsxSmn3/+eafnN27cMPNNZ8+e3UybR5gGAACAN7A0Nd7FixedFv3SliNHjkjDhg3N14oDAAAA3sBSmE5O+fLl5c0337ylap3VnTp1ysxgsnfvXlcPBQAAAJ4appWfn5/88ccfkhV0795d2rVr5+phAAAAIKv1TK9YscLpuc4vHR0dLdOnT5cGDRqk19gAAACArBemk1Zstc2hQIECcv/998vkyZPFkyxdulRGjx4tx44dMzdPVq9e3Szz5s1zvDe1ceNGadq0qezYsUP69u0rhw8flrvvvluGDRvm4ncAAAAAjwrTCQkJkhVoNb1Tp04yceJEefTRR+XSpUvyww8/mNlITp8+LbGxsTJnzhyzb968ec2Nlq1bt5bmzZvLJ598Yr685k56xOPi4sxip8cFAACAl/ZMjxkzxkyFl9S///5rtnlSmL5586a0b99eSpUqJWFhYfLss89KcHCwBAYGSkBAgBQuXNgsOu3fp59+an6R+Oijj6Rq1aomWA8ZMuS255kwYYKEhoY6lhIlSmTK+wMAAIAbhmlti9AqbVIasHWbp6hWrZo88MADJkR36NBBZs2aZab6S4m2doSHh0uOHDkc6+rVq3fb8wwdOlRiYmIcy5kzZ9LtPQAAAMDDwrTecGjvJU7s559/Nu0QnsLX11fWrl0rq1atkipVqsi0adOkYsWKpn0jPWmFOyQkxGkBAACAl/VM58mTx4RoXSpUqOAUqOPj4021+plnnhFPou9BZyDRZeTIkVKyZEn54osvTFuHvqfEKleuLAsWLJBr1645qtPbtm1z0cgBAADgUWF6ypQppirds2dP086h/b92Gj617/hO2h7cxfbt22X9+vXSokULKViwoHl+/vx5E5o1MK9evdp8s2O+fPnMe+3cubOZvaN3796mdUO/sGXSpEmufhsAAADwhDDdrVs387N06dJSv3598ff3F0+m7RabN282vyToDBtaldap/R588EGpVauWbNq0yfzUirt9aryvv/7aVN91+jxtDXnrrbfksccec/VbAQAAgAv42LTU/B9oBff69etO6+gJTp0Gd610Vxs4U3wDAjPtvFGREZl2LgAAgKyQ13TyiNSyraUbEHXWjgEDBpjWiKCgINNLnXgBAAAAvIGlMK1zK2/YsEHef/99M1PF7NmzTQ910aJFZf78+ek/SgAAACCrfAOi9g1raNYe4h49ekijRo2kXLlypud44cKF0qVLl/QfKQAAAJAVKtN///23lClTxjzWHhJ9rho2bGhu6AMAAAC8gaUwrUHa/sUmlSpVkiVLljgq1rlz507fEQIAAABZKUxra4d+26F69dVXZcaMGeZLTF544QXTTw0AAAB4A0s90xqa7Zo1aya//PKLREVFmb7p8PDw9BwfAAAAkLXnmbZ/tTbSd95CAAAAZMF5puPj42Xs2LFSrFgxCQ4OlhMnTpj1I0aMkI8++sj6qAEAAAAPYilMjx8/XubOnSsTJ06U7NmzO9bffffdZs5pAAAAwBtYCtM6x/SHH35o5pP29fV1rK9WrZrpnwYAAAC8gaUw/fvvv5ubDZNKSEiQGzdupMe4AAAAgKwZpqtUqSI//PDDLeuXLl0q1atXT49xAQAAAFlzaryRI0dKt27dTIVaq9HLly+XI0eOmPaPb775Jv1HCQAAAHj61Hg6a0fp0qXFx8fHVKbHjBljvrzl8uXLUqNGDROyW7RokbEjzkJTrVQbOFN8AwIz5ZxRkRGZch4AAABvmhovTZXp8uXLS3R0tBQsWFAaNWokefPmlf3790uhQoXSY8wAAABA1u2ZTlrEXrVqlVy5ciW9xwQAAABk3RsQ7f7jlycCAAAA3hOmtVdal6TrAAAAAG/kl9ZKdPfu3SUgIMA8v3btmjzzzDMSFBTktJ/O7gEAAABkdWkK0zodXmJPPfVUeo8HAAAAyJphes6cORk3EgAAAMCbbkAEAAAAvBlhGgAAALCIMP0f6WwmX375pauHAQAAABfw6jB948YNVw8BAAAAHizLhemEhASZOHGilCtXzkzhd9ddd8n48ePl1KlTpor82WefSZMmTSRHjhyycOFC85rZs2dL5cqVzbpKlSrJe++95zje9evXZcCAAVKkSBGzvWTJkjJhwgSzrVSpUubno48+ao5tfw4AAADvkKbZPDzB0KFDZdasWfLOO+9Iw4YNJTo6Wn755RfH9ldffVUmT54s1atXdwTqkSNHyvTp0826PXv2SO/evc3c2ToV4LvvvisrVqyQJUuWmGB+5swZs6idO3dKwYIFzSwnrVq1El9f32THFBcXZxa72NjYTPgkAAAAkNGyVJi+dOmSTJ061QRj+5zYZcuWNaFaK9Nq0KBB0r59e8drXn/9dROu7etKly4thw4dkg8++MAc4/Tp01K+fHlzDK0+a2XarkCBAuZn7ty5pXDhwimOSyvZo0ePzrD3DQAAANfIUm0ehw8fNhXgBx54IMV9atWq5Xh85coVOX78uPTq1UuCg4Mdy7hx48x6pd/4uHfvXqlYsaI899xzsmbNGkvV8piYGMdir2wDAADAs2WpynRgYOBt90n81eeXL182P7UtpE6dOk772Vs2atSoISdPnpRVq1bJunXrpGPHjtKsWTNZunTpHY9Le7ftX8EOAACArCNLVaa1HUMD9fr16+9o/0KFCknRokXlxIkT5obFxIu2e9iFhITIE088YUK33sC4bNky+fvvv802f39/iY+Pz7D3BAAAAPeVpSrTekPhK6+8Ii+//LJkz55dGjRoIOfPn5eDBw+m2PqhvczavhEaGmpuItQ2kV27dsnFixflxRdflLffftvM5KE3J2bLlk0+//xz0x+tfdJKZ/DQ8K7n0upznjx5MvldAwAAwFWyVJhWI0aMED8/PzNDxx9//GGC8DPPPJPi/k8//bTkzJlTIiMjZciQIaYNJCwszNyoqHLlymWm2jt69Khp/bj33nvl22+/NcFa6c2LGrq1al2sWDHHjY4AAADI+nxsNpvN1YPwNjo1nlbCqw2cKb4Bt+/zTg9RkRGZch4AAICslNd08ght+fWKnmkAAAAgMxGmAQAAAIsI0wAAAIBFhGkAAADAIsI0AAAAYBFhGgAAALCIMA0AAABYlOW+tMWTbB7XKdV5CwEAAODeqEwDAAAAFhGmAQAAAIsI0wAAAIBFhGkAAADAIsI0AAAAYBFhGgAAALCIMA0AAABYxDzTLtR4+CLxDQjM0HNERUZk6PEBAAC8GZVpAAAAwCLCNAAAAGARYRoAAACwiDANAAAAWESYBgAAACwiTAMAAAAWEaYBAAAAiwjTAAAAgEWEaQAAAMAiwjQAAABgkVeH6evXr7t6CAAAAPBgXhWmmzZtKgMGDJBBgwZJ/vz5JSAgQHx8fGT16tVSvXp1CQwMlPvvv1/OnTsnq1atksqVK0tISIh07txZrl696jjO0qVLJSwszOyfL18+adasmVy5csWl7w0AAACZz6vCtJo3b55kz55dtmzZIjNnzjTrRo0aJdOnT5effvpJzpw5Ix07dpQpU6bIp59+KitXrpQ1a9bItGnTzL7R0dHSqVMn6dmzpxw+fFg2bdok7du3F5vNluI54+LiJDY21mkBAACA5/MTL1O+fHmZOHGiIxircePGSYMGDczjXr16ydChQ+X48eNSpkwZs+7xxx+XjRs3yiuvvGJec/PmTROgS5YsabZrlTo1EyZMkNGjR2fwOwMAAEBm87rKdM2aNW9ZFx4e7nhcqFAhyZkzpyNI29dp64eqVq2aPPDAAyZAd+jQQWbNmiUXL15M9ZwazmNiYhyLVr8BAADg+bwuTAcFBd2yzt/f3/FYe6gTP7evS0hIMI99fX1l7dq1pqe6SpUqpv2jYsWKcvLkyRTPqb3Z2nudeAEAAIDn87ownR40XGtbiLZu7Nmzx/Rgf/HFF64eFgAAADKZ1/VM/1fbt2+X9evXS4sWLaRgwYLm+fnz583MHwAAAPAuhOk00haNzZs3m9k+dFYOvQlx8uTJ8uCDD7p6aAAAAMhkPrbU5nRDhtAQHhoaKtUGzhTfgMAMPVdUZESGHh8AACAr5zWdPCK1+93omQYAAAAsIkwDAAAAFhGmAQAAAIsI0wAAAIBFhGkAAADAIsI0AAAAYBFhGgAAALCIL21xoc3jOqU6byEAAADcG5VpAAAAwCLCNAAAAGARYRoAAACwiDANAAAAWESYBgAAACwiTAMAAAAWMTWeCzUevkh8AwIz7PhRkREZdmwAAABQmQYAAAAsI0wDAAAAFhGmAQAAAIsI0wAAAIBFhGkAAADAIsI0AAAAYBFhGgAAALCIMA0AAAB4e5ju3r27tGvXztXDAAAAgBfJMmEaAAAAyGyEaQAAAMBbwvTSpUslLCxMAgMDJV++fNKsWTO5cuWKY/ukSZOkSJEiZlv//v3lxo0bjm0LFiyQWrVqSa5cuaRw4cLSuXNnOXfunGP7pk2bxMfHR1auXCnh4eGSI0cOqVu3rhw4cMBpDD/++KM0atTIjKFEiRLy3HPPOY0BAAAA3sGjwnR0dLR06tRJevbsKYcPHzbht3379mKz2cz2jRs3yvHjx83PefPmydy5c81ip8F67Nix8vPPP8uXX34pp06dMr3WSQ0ZMkQmT54sO3fulAIFCkibNm0coVyP36pVK3nsscdk37598tlnn5lwPWDAgBTHHRcXJ7GxsU4LAAAAPJ+PzZ5EPcDu3bulZs2aJgSXLFnSaZuGYg3XGnZ9fX3Nuo4dO0q2bNlk8eLFyR5v165dcu+998qlS5ckODjYvP6+++4z+z/xxBNmn7///luKFy9uQrke7+mnnzbH/+CDDxzH0TDdpEkTU53WanZSo0aNktGjR9+yvtrAmeIbECgZJSoyIsOODQAAkJVp8TM0NFRiYmIkJCQka1Smq1WrJg888IBp8+jQoYPMmjVLLl686NhetWpVR5BW2u6RuI0jKirKVJnvuusu0+qhAVidPn3a6Tz16tVzPM6bN69UrFjRVMKVVrU1WGv4ti8tW7aUhIQEOXnyZLLjHjp0qLkQ9uXMmTPp+KkAAADAVfzEg2hQXrt2rfz000+yZs0amTZtmgwbNky2b99utvv7+zvtr/3PGnKVVo019OqycOFC076hIVqfX79+/Y7HcPnyZenbt6/pk05KQ3pyAgICzAIAAICsxaPCtD0gN2jQwCwjR4407R5ffPHFbV/3yy+/yIULF+TNN980Nw3a2zySs23bNkcw1sr3r7/+KpUrVzbPa9SoIYcOHZJy5cql6/sCAACA5/GoNg+tQL/xxhsmBGtVefny5XL+/HlH0E2NhuPs2bObavaJEydkxYoV5mbE5IwZM0bWr19vZvHQXuz8+fM7vhDmlVdeMZVxveFw7969cvToUfnqq69SvQERAAAAWZNHhWlt/t68ebM89NBDUqFCBRk+fLiZdePBBx+87Wu1rUN7nT///HOpUqWKqVDrNHrJ0W3PP/+8udnx7Nmz8vXXX5sgrnTKvO+//95Uq3V6vOrVq5sKedGiRdP9/QIAAMC9edRsHhnNPpuHtnbkzp07w+8OZTYPAAAA95QlZ/MAAAAA3AlhGgAAAPCW2TwyUtOmTR3fpggAAADcDpVpAAAAwCLCNAAAAGARYRoAAACwiDANAAAAWESYBgAAACxiNg8X2jyuU6qTgAMAAMC9UZkGAAAALCJMAwAAABYRpgEAAACLCNMAAACARYRpAAAAwCLCNAAAAGARU+O5UOPhi8Q3IDBDjh0VGZEhxwUAAMD/Q2UaAAAAsIgwDQAAAFhEmAYAAAAsIkwDAAAAFhGmAQAAAIsI0wAAAIBFhGkAAADAIsI0AAAA4IlhumnTpjJo0CBXDsGtxgEAAADP4tJvQFy+fLn4+/u7cggAAACAZ4bpvHnzuvL0AAAAQNZo8yhVqpSMGzdOIiIiJDg4WEqWLCkrVqyQ8+fPS9u2bc268PBw2bVrl+P1c+fOldy5c8uXX34p5cuXlxw5ckjLli3lzJkzjn26d+8u7dq1czqvnlPPnZL33nvPcbxChQrJ448/7tiWkJAgEyZMkNKlS0tgYKBUq1ZNli5dms6fDAAAADyBW92A+M4770iDBg1kz5498vDDD0vXrl1NuH7qqadk9+7dUrZsWfPcZrM5XnP16lUZP368zJ8/X7Zs2SL//POPPPnkk5bHoGH9ueeekzFjxsiRI0fku+++k8aNGzu2a5DWc82cOVMOHjwoL7zwghnf999/n+Ix4+LiJDY21mkBAACA53Npm0dSDz30kPTt29c8HjlypLz//vty7733SocOHcy6V155RerVqyd//vmnFC5c2Ky7ceOGTJ8+XerUqWOez5s3TypXriw7duyQ2rVrp3kMp0+flqCgIGndurXkypXLVMirV6/uCMVvvPGGrFu3zoxDlSlTRn788Uf54IMPpEmTJskeUwP46NGjLX4qAAAAcFduVZnWNg47ba9QYWFht6w7d+6cY52fn58J3HaVKlUyrR+HDx+2NIbmzZubAK0hWSvjCxcuNNVvdezYMfNY99G2E/uilerjx4+neMyhQ4dKTEyMY0nchgIAAADP5VaV6cQze/j4+KS4TvuW71S2bNmc2kLs1eyUaDVaW0o2bdoka9asMRXyUaNGyc6dO+Xy5ctmn5UrV0qxYsWcXhcQEJDiMXVbatsBAADgmdyqMm3FzZs3nW5K1D5n7ZvWVg9VoEABiY6OdnrN3r17Uz2mVrubNWsmEydOlH379smpU6dkw4YNUqVKFROKtRWkXLlyTkuJEiUy6B0CAADAXblVZdoKrVwPHDhQ3n33XROCBwwYIHXr1nX0S99///0SGRlpWjG0z/mTTz6RAwcOOPqgk/rmm2/kxIkT5qbDPHnyyLfffmsq4RUrVjRV68GDB5ubDnVdw4YNTduG3vgYEhIi3bp1y+R3DwAAAFfy+DCdM2dOc2Ni586d5ffff5dGjRrJRx995NiuU+WNGDFCXn75Zbl27Zr07NnTzAiyf//+ZI+n/db6ZTLa2qH76xR5ixYtkqpVq5rtY8eONdVuvalQQ7fuX6NGDXnttdcy7T0DAADAPfjYkjYUexCdZ1rnjNa2Dk+iU+OFhoZKtYEzxTcgMEPOERUZkSHHBQAA8Aax/5fXtAtBOxCybM80AAAA4CqEaQAAAMAbw7R+VbintXgAAAAg6/DoMA0AAAC4EmEaAAAAsIgwDQAAAFhEmAYAAAAs8vgvbfFkm8d1SnXeQgAAALg3KtMAAACARYRpAAAAwCLCNAAAAGARYRoAAACwiDANAAAAWESYBgAAACwiTAMAAAAWEaYBAAAAiwjTAAAAgEWEaQAAAMAiwjQAAABgEWEaAAAAsIgwDQAAAFhEmAYAAAAsIkwDAAAAFhGmAQAAAIsI0wAAAIBFhGkAAADAIsL0/7lx44arhwAAAAAP45ZhOiEhQSZMmCClS5eWwMBAqVatmixdutRs27Rpk/j4+Mj69eulVq1akjNnTqlfv74cOXLE6RhfffWV1KhRQ3LkyCFlypSR0aNHy82bNx3b9Rjvv/++PPLIIxIUFCTjx48368eNGycFCxaUXLlyydNPPy2vvvqq3HPPPWbb5s2bxd/fX86ePet0rkGDBkmjRo1SfD9xcXESGxvrtAAAAMDzuWWY1iA9f/58mTlzphw8eFBeeOEFeeqpp+T777937DNs2DCZPHmy7Nq1S/z8/KRnz56ObT/88INERETI888/L4cOHZIPPvhA5s6d6wjMdqNGjZJHH31U9u/fb16/cOFCs89bb70lUVFRctddd5nAbde4cWMTzBcsWOBU0dbXJT5/cu8nNDTUsZQoUSIdPy0AAAC4io/NZrOJG9Eqbt68eWXdunVSr149x3qtEl+9elX69Okj9913n9n+wAMPmG3ffvutPPzww/Lvv/+aSnSzZs3MtqFDhzpe/8knn8jLL78sf/zxh6MyrRXld955x7FP3bp1TbV7+vTpjnUNGzaUy5cvy969e83ziRMnmmCuIV0tX75cunXrZqrVWuFO6T3pYqeVaQ3UMTExEhISko6fHgAAANKD5jUtgt4ur7ldZfrYsWMmNDdv3lyCg4Mdi1aqjx8/7tgvPDzc8bhIkSLm57lz58zPn3/+WcaMGeP0+t69e0t0dLQ5tp0G58S0VaR27dpO65I+7969uxnjtm3bzHMN1h07dkwxSKuAgABzERIvAAAA8Hx+4ma0CqxWrlwpxYoVuyWU2gO19i7baZXZ3mttP4b2SLdv3/6W42vl2i61AJwS7adu06aNzJkzx/R0r1q1yvRxAwAAwPu4XZiuUqWKCc2nT5+WJk2a3LI9cXU6JXrjoVaZy5Url6ZzV6xYUXbu3Gn6re30eVLactKpUycpXry4lC1bVho0aJCm8wAAACBrcLswrbNoDB482Nx0qJVm7VnWXpUtW7aY9oiSJUve9hgjR46U1q1bmxsIH3/8ccmWLZtp/Thw4ICZrSMlAwcONO0g2v6hM4R89tlnsm/fPnPTYWItW7Y0Y9FjaTsJAAAAvJPb9UyrsWPHyogRI8wsGJUrV5ZWrVqZtg9tq7gTGna/+eYbWbNmjdx7773mxkK90fB2QbxLly7mpkUN81rdPnnypOmRTtwaojSc6/r4+HinKjYAAAC8i9vN5uFu9EbIwoULO02Hp3r16iXnz5+XFStWZNjdoQAAAHCNO81rbtfm4Uo604fOba2VbV9fX1m0aJGZgm/t2rWOffQD1XmpP/30U0tBGgAAAFkHYToRnRVE56zWL265du2auSFx2bJlZt5qu7Zt28qOHTvkmWeeMVVrAAAAeC/aPFyANg8AAAD35rFf2gIAAAB4CsI0AAAAYBFhGgAAALCIMA0AAABYRJgGAAAALCJMAwAAABYRpgEAAACLCNMAAACARYRpAAAAwCLCNAAAAGARYRoAAACwiDANAAAAWESYBgAAACwiTAMAAAAWEaYBAAAAiwjTAAAAgEWEaQAAAMAiwjQAAABgEWEaAAAAsIgwDQAAAFhEmE7Fpk2bxMfHR/755x9XDwUAAABuiDCdivr160t0dLSEhoa6eigAAABwQ14Zpq9fv35H+2XPnl0KFy5sqtMAAACA24fppk2bysCBA2XQoEGSJ08eKVSokMyaNUuuXLkiPXr0kFy5ckm5cuVk1apVZv/4+Hjp1auXlC5dWgIDA6VixYoydepUp2N2795d2rVrJ+PHj5eiRYuafdRPP/0k99xzj+TIkUNq1aolX375pQnOe/fuTbbNY+7cuZI7d25ZvXq1VK5cWYKDg6VVq1ameg0AAADv43ZhWs2bN0/y588vO3bsMMG6X79+0qFDB9N2sXv3bmnRooV07dpVrl69KgkJCVK8eHH5/PPP5dChQzJy5Eh57bXXZMmSJU7HXL9+vRw5ckTWrl0r33zzjcTGxkqbNm0kLCzMHHPs2LHyyiuv3HZses5JkybJggULZPPmzXL69GkZPHhwqq+Ji4sz50u8AAAAIAuwuZkmTZrYGjZs6Hh+8+ZNW1BQkK1r166OddHR0TYd+tatW5M9Rv/+/W2PPfaY43m3bt1shQoVssXFxTnWvf/++7Z8+fLZ/v33X8e6WbNmmePu2bPHPN+4caN5fvHiRfN8zpw55vmxY8ccr5kxY4Y5dmpef/1187qkS0xMTBo/HQAAAGQGzWl3ktfcsjIdHh7ueOzr6yv58uUzFWQ7bf1Q586dMz9nzJghNWvWlAIFCpjWiw8//NBUjBPT12sPtJ1WqfU82uJhV7t27duOLWfOnFK2bFnH8yJFijjGkZKhQ4dKTEyMYzlz5sxtzwMAAAD35yduyN/f3+m59i0nXme/IVBbPBYvXmzaLCZPniz16tUzPdWRkZGyfft2p2MEBQVl2NhsNv3FJWUBAQFmAQAAQNbilmE6LbZs2WJ6qZ999lnHuuPHj9/2dXoT4ieffGL6me1Bd+fOnRk6VgAAAGQtbtnmkRbly5eXXbt2mRk2fv31VxkxYsQdheLOnTubynafPn3k8OHD5vV6Y6FiKjwAAAB4RZju27evtG/fXp544gmpU6eOXLhwwalKnZKQkBD5+uuvzTR4Oj3esGHDzEwgKnEfNQAAAJASH70LMcWtXmbhwoVmLmu9SVDnrM4oOjWefquinkdDPQAAANzLneY1j++Z/i/mz58vZcqUkWLFisnPP/9s5pnu2LFjhgZpAAAAZB1eHabPnj1rWjv0p05xp18Mo9+SCAAAANwJ2jxcgDYPAACArJHXPP4GRAAAAMBVCNMAAACARYRpAAAAwCLCNAAAAGARYRoAAACwiDANAAAAWESYBgAAACzy6i9tcRX71N46fyEAAADcjz2n3e4rWQjTLnDhwgXzs0SJEq4eCgAAAFJx6dIl8+UtKSFMu0DevHnNz9OnT6d6ceC630T1F50zZ87wDZVuimvk3rg+7o9r5N64Pu5BK9IapIsWLZrqfoRpF8iW7f9vVdcgzR8S96XXhuvj3rhG7o3r4/64Ru6N6+N6d1L05AZEAAAAwCLCNAAAAGARYdoFAgIC5PXXXzc/4X64Pu6Pa+TeuD7uj2vk3rg+nsXHdrv5PgAAAAAki8o0AAAAYBFhGgAAALCIMA0AAABYRJgGAAAALCJMZ5AZM2ZIqVKlJEeOHFKnTh3ZsWNHqvt//vnnUqlSJbN/WFiYfPvtt5k2Vm+Ulutz8OBBeeyxx8z+Pj4+MmXKlEwdq7dKyzWaNWuWNGrUSPLkyWOWZs2a3fbPHDLv+ixfvlxq1aoluXPnlqCgILnnnntkwYIFmTpeb5TW/w7ZLV682Pxd165duwwfozdLy/WZO3euuSaJF30d3ANhOgN89tln8uKLL5ppbXbv3i3VqlWTli1byrlz55Ld/6effpJOnTpJr169ZM+ePeYvMF0OHDiQ6WP3Bmm9PlevXpUyZcrIm2++KYULF8708XqjtF6jTZs2mT9DGzdulK1bt5qv4W3RooX8/vvvmT52b5DW65M3b14ZNmyYuTb79u2THj16mGX16tWZPnZvkdZrZHfq1CkZPHiw+eUU7nV99JsQo6OjHcv//ve/TB0zUqFT4yF91a5d29a/f3/H8/j4eFvRokVtEyZMSHb/jh072h5++GGndXXq1LH17ds3w8fqjdJ6fRIrWbKk7Z133sngEeK/XCN18+ZNW65cuWzz5s3LwFF6r/96fVT16tVtw4cPz6ARwso10j839evXt82ePdvWrVs3W9u2bTNptN4nrddnzpw5ttDQ0EwcIdKCynQ6u379ukRFRZl/ZrbLli2bea5VmeTo+sT7K/0NNaX9kbnXB553jfRfE27cuGEqonCv66NfbbB+/Xo5cuSING7cOINH652sXqMxY8ZIwYIFzb+Swv2uz+XLl6VkyZLmX97atm1rWhDhHgjT6eyvv/6S+Ph4KVSokNN6fX727NlkX6Pr07I/Mvf6wPOu0SuvvCJFixa95ZdUuO76xMTESHBwsGTPnl0efvhhmTZtmjRv3jwTRux9rFyjH3/8UT766CNz/wHc7/pUrFhRPv74Y/nqq6/kk08+kYSEBKlfv7789ttvmTRqpMYv1a0A4GG0t11voNI+am7QcR+5cuWSvXv3muqaVqa1X1TvRWjatKmrh+b1Ll26JF27djVBOn/+/K4eDpJRr149s9hpkK5cubJ88MEHMnbsWJeODYTpdKd/Efn6+sqff/7ptF6fp3Tzmq5Py/7I3OsDz7lGkyZNMmF63bp1Eh4ensEj9U5Wr4/+M3a5cuXMY53N4/DhwzJhwgTCtBtco+PHj5sbD9u0aeNYp5VP5efnZ1pyypYtmwkj9w7p8d8hf39/qV69uhw7diyDRom0oM0jnek/YdasWdNUXhL/paTPE/9WmZiuT7y/Wrt2bYr7I3OvDzzjGk2cONFUaL777jszDRvc+8+QviYuLi6DRund0nqNdFrW/fv3m385sC+PPPKI3Hfffeax9ujCvf4MaZuIXrMiRYpk4Ehxx9J0uyLuyOLFi20BAQG2uXPn2g4dOmTr06ePLXfu3LazZ8+a7V27drW9+uqrjv23bNli8/Pzs02aNMl2+PBh2+uvv27z9/e37d+/34XvIutK6/WJi4uz7dmzxyxFihSxDR482Dw+evSoC99F1pbWa/Tmm2/asmfPblu6dKktOjrasVy6dMmF7yLrSuv1eeONN2xr1qyxHT9+3Oyvf9fp33mzZs1y4bvI2tJ6jZJiNg/3uj6jR4+2rV692vwZioqKsj355JO2HDly2A4ePOjCdwE7wnQGmTZtmu2uu+4y/4HXKXC2bdvm2NakSRPzF1ViS5YssVWoUMHsX7VqVdvKlStdMGrvkZbrc/LkSZv+3pl00f3gHtdIpyxM7hrpL6Zw/fUZNmyYrVy5cuY//nny5LHVq1fPhAm413+HEiNMu9f1GTRokGPfQoUK2R566CHb7t27XTRyJOWj/3PndWwAAAAAdvRMAwAAABYRpgEAAACLCNMAAACARYRpAAAAwCLCNAAAAGARYRoAAACwiDANAAAAWESYBgAAACwiTAMAAAAWEaYBwIN0795d2rVrJ+7o1KlT4uPjI3v37nX1UAAg0xCmAQD/2fXr1109BABwCcI0AHiopk2bysCBA2XQoEGSJ08eKVSokMyaNUuuXLkiPXr0kFy5ckm5cuVk1apVjtds2rTJVI9Xrlwp4eHhkiNHDqlbt64cOHDA6djLli2TqlWrSkBAgJQqVUomT57stF3XjR07ViIiIiQkJET69OkjpUuXNtuqV69uzqHjUzt37pTmzZtL/vz5JTQ0VJo0aSK7d+92Op7uP3v2bHn00UclZ86cUr58eVmxYoXTPgcPHpTWrVub8+l7a9SokRw/ftyxXV9fuXJl854qVaok7733Xjp+2gCQPMI0AHiwefPmmZC6Y8cOE6z79esnHTp0kPr165vA2qJFC+natatcvXrV6XVDhgwxAVmDboECBaRNmzZy48YNsy0qKko6duwoTz75pOzfv19GjRolI0aMkLlz5zodY9KkSVKtWjXZs2eP2a5jUOvWrZPo6GhZvny5eX7p0iXp1q2b/Pjjj7Jt2zYTlB966CGzPrHRo0eb8+7bt89s79Kli/z9999m2++//y6NGzc24X7Dhg1mjD179pSbN2+a7QsXLpSRI0fK+PHj5fDhw/LGG2+YMennAwAZygYA8BjdunWztW3b1jxu0qSJrWHDho5tN2/etAUFBdm6du3qWBcdHW3Tv+q3bt1qnm/cuNE8X7x4sWOfCxcu2AIDA22fffaZed65c2db8+bNnc47ZMgQW5UqVRzPS5YsaWvXrp3TPidPnjTH3rNnT6rvIT4+3pYrVy7b119/7Vinrxs+fLjj+eXLl826VatWmedDhw61lS5d2nb9+vVkj1m2bFnbp59+6rRu7Nixtnr16qU6FgD4r6hMA4AH01YNO19fX8mXL5+EhYU51mnrhzp37pzT6+rVq+d4nDdvXqlYsaKp6Cr92aBBA6f99fnRo0clPj7esa5WrVp3NMY///xTevfubSrS2uahbRqXL1+W06dPp/hegoKCzH72cetNjdrW4e/vf8vxta1F2z169eolwcHBjmXcuHFObSAAkBH8MuSoAIBMkTRcau9x4nX6XCUkJKT7uTXw3glt8bhw4YJMnTpVSpYsaVo1NMwnvWkxufdiH3dgYGCKx9dgrrRfvE6dOk7b9BcMAMhIhGkA8ELau3zXXXeZxxcvXpRff/3V3Lyn9OeWLVuc9tfnFSpUSDWcZs+e3fxMXL22v1ZvBtQ+aHXmzBn566+/0jRerVpr/7P2dScN3Vp9L1q0qJw4ccL0WQNAZiJMA4AXGjNmjGkJ0SA6bNgwcxOjff7ql156Se69914zW8cTTzwhW7dulenTp992doyCBQuaCvJ3330nxYsXN7NqaFuHtncsWLDAtIXExsaamx9TqzQnZ8CAATJt2jRzU+TQoUPNcfUXgtq1a5sWFb158bnnnjPrW7VqJXFxcbJr1y7zi8KLL774nz4rAEgNPdMA4IXefPNNef7556VmzZpy9uxZ+frrrx2V5Ro1asiSJUtk8eLFcvfdd5tZMjR86xfGpMbPz0/effdd+eCDD0yluG3btmb9Rx99ZEKtHldnFtHQq8E7LTT46ywe2tKhU+vpuLWtw16lfvrpp83UeHPmzDE947qPzj5in64PADKKj96FmGFHBwC4FZ1n+r777jPhNnfu3K4eDgB4PCrTAAAAgEWEaQAAAMAi2jwAAAAAi6hMAwAAABYRpgEAAACLCNMAAACARYRpAAAAwCLCNAAAAGARYRoAAACwiDANAAAAWESYBgAAAMSa/w+6XCkfxYBrEwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>median</td>\n",
       "      <td>0.565158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>peak</td>\n",
       "      <td>0.097246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mean</td>\n",
       "      <td>0.073363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>max</td>\n",
       "      <td>0.065185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>p2p</td>\n",
       "      <td>0.041108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>min</td>\n",
       "      <td>0.029528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>std</td>\n",
       "      <td>0.028338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>crest</td>\n",
       "      <td>0.026930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>rms</td>\n",
       "      <td>0.025336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>shape</td>\n",
       "      <td>0.024586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>impulse</td>\n",
       "      <td>0.023221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>energy</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>margin</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Feature  Importance\n",
       "4    median    0.565158\n",
       "5      peak    0.097246\n",
       "2      mean    0.073363\n",
       "0       max    0.065185\n",
       "6       p2p    0.041108\n",
       "1       min    0.029528\n",
       "3       std    0.028338\n",
       "9     crest    0.026930\n",
       "8       rms    0.025336\n",
       "10    shape    0.024586\n",
       "11  impulse    0.023221\n",
       "7    energy    0.000000\n",
       "12   margin    0.000000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plot_feature_importance(model, feature_columns):\n",
    "    \"\"\"\n",
    "    Plots feature importance for tree-based models.\n",
    "\n",
    "    Args:\n",
    "        model: Trained XGBoost, RandomForest, or GradientBoosting model.\n",
    "        feature_columns: List of feature names.\n",
    "\n",
    "    Returns:\n",
    "        None (Displays feature importance plot)\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"feature_importances_\"):  # Ensure model supports feature importance\n",
    "        importances = model.feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({\"Feature\": feature_columns, \"Importance\": importances})\n",
    "        feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance_df[:15])  # Top 15 features\n",
    "        plt.title(\"Feature Importance\")\n",
    "        plt.show()\n",
    "\n",
    "        return feature_importance_df\n",
    "    else:\n",
    "        raise ValueError(\"This model does not support feature importance.\")\n",
    "\n",
    "# Usage (after training a model)\n",
    "plot_feature_importance(trained_model, [\"max\", \"min\", \"mean\", \"std\", \"median\", \"peak\", \"p2p\", \"energy\", \"rms\", \"crest\", \"shape\", \"impulse\", \"margin\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def ensemble_predict(models_with_features, dataset_name, target_column):\n",
    "    \"\"\"\n",
    "    Loads a dataset, splits it, loads an ensemble of models (with different feature sets),\n",
    "    and predicts on the test set.\n",
    "\n",
    "    Args:\n",
    "        models_with_features: A list of tuples (model_path, feature_columns) specifying each model's file path\n",
    "                              and the corresponding feature columns used for training.\n",
    "        dataset_name: The name of the dataset CSV file.\n",
    "        target_column: The name of the target column.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    # ✅ Load dataset\n",
    "    df = load_data(dataset_name)\n",
    "\n",
    "    # ✅ Ensure target column exists\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in dataset.\")\n",
    "\n",
    "    # ✅ Train-test split\n",
    "    data_train, data_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    y_test = data_test[target_column].values  # Target remains the same for all models\n",
    "\n",
    "    # ✅ Load models and their feature columns\n",
    "    models = []\n",
    "    y_preds = []\n",
    "\n",
    "    for model_path, feature_columns in models_with_features:\n",
    "        model = joblib.load(model_path)  # Load model\n",
    "        models.append(model)\n",
    "\n",
    "        # ✅ Extract the correct feature set for this model\n",
    "        X_test = data_test[feature_columns].values  # Select only the features it was trained on\n",
    "\n",
    "        # ✅ Predict (check if the model has `predict_proba` for probability averaging)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]  # Get probability of the positive class\n",
    "        else:\n",
    "            y_pred_proba = model.predict(X_test)  # Some models might not have `predict_proba`\n",
    "\n",
    "        y_preds.append(y_pred_proba)\n",
    "\n",
    "    # ✅ Ensemble predictions (Average probabilities and threshold at 0.5)\n",
    "    y_pred = np.mean(y_preds, axis=0) > 0.5\n",
    "\n",
    "    # ✅ Compute metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    # ✅ Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # ✅ Print metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    # ✅ Plot confusion matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"No Fall\", \"Fall\"], yticklabels=[\"No Fall\", \"Fall\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9764\n",
      "Precision: 0.9615\n",
      "Recall: 0.9615\n",
      "F1 Score: 0.9615\n",
      "ROC AUC: 0.9722\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ensemble_predict(\n",
    "    models_with_features=[\n",
    "        (\"models/MF_XGB_Optuna.pkl\", [\"max\", \"min\", \"mean\", \"std\", \"median\", \"peak\", \"p2p\", \"energy\", \"rms\", \"crest\", \"shape\", \"impulse\", \"margin\"]),  \n",
    "        (\"models/MF_XGB_RD40_Optuna.pkl\", [\"median\", \"max\", \"peak\", \"mean\", \"p2p\"]),\n",
    "        (\"models/MF_XGB_RD41_Optuna.pkl\", [\"median\", \"max\", \"peak\", \"mean\", \"p2p\"])    \n",
    "    ],\n",
    "    dataset_name=\"MPU_features_pca.csv\",\n",
    "    target_column=\"fall_binary\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
