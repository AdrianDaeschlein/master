{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "\n",
    "# Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "\n",
    "# Machine Learning & Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "import optuna\n",
    "\n",
    "# Model Saving & Experiment Tracking\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow ui --port 5000\n"
     ]
    }
   ],
   "source": [
    "print(\"mlflow ui --port 5000\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACKING_URI = \"azureml://northeurope.api.azureml.ms/mlflow/v1.0/subscriptions/716d3e14-e009-4f92-89c9-01fa8347272a/resourceGroups/adda23ac-rg/providers/Microsoft.MachineLearningServices/workspaces/fall\"\n",
    "mlflow.set_tracking_uri(TRACKING_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    '''\n",
    "    Load data from a csv file into a pandas dataframe.\n",
    "    \n",
    "    Args:\n",
    "    file_name: The name of the csv file to load\n",
    "    \n",
    "    Returns:\n",
    "    df: A pandas dataframe containing the data from the csv file\n",
    "    '''\n",
    "    folder = \"datasets\"\n",
    "\n",
    "    df = pd.read_csv(f\"{folder}/{file_name}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratify Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_activity_split(df, target_column=\"fall_binary\", test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits the dataset while ensuring 20% of each 'activity' is in the test set.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The full dataset containing 'activity' and the target variable.\n",
    "        target_column (str): The column representing the target labels.\n",
    "        test_size (float): The fraction of each activity to be in the test set.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        data_train (pd.DataFrame): Training set.\n",
    "        data_test (pd.DataFrame): Test set.\n",
    "    \"\"\"\n",
    "\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "\n",
    "    # ✅ Loop through each activity and apply train-test split\n",
    "    for activity, group in df.groupby(\"activity\"):\n",
    "        train, test = train_test_split(group, test_size=test_size, random_state=random_state, stratify=group[target_column])\n",
    "        train_list.append(train)\n",
    "        test_list.append(test)\n",
    "\n",
    "    # ✅ Concatenate results into train & test datasets\n",
    "    data_train = pd.concat(train_list).reset_index(drop=True)\n",
    "    data_test = pd.concat(test_list).reset_index(drop=True)\n",
    "\n",
    "    return data_train, data_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Per Activity / Distance Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_per_activity(model, test_X, test_y, feature_columns=None):\n",
    "    \"\"\"\n",
    "    Evaluates model performance per activity type and logs the results in MLflow.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model (LSTM or Tree-based).\n",
    "        test_X: Test feature data (DataFrame).\n",
    "        test_y: Test target labels (Series).\n",
    "        target_column: The name of the target column.\n",
    "        feature_columns: The feature columns used for training.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing classification results per activity.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ✅ Ensure X_test is a DataFrame\n",
    "    if not isinstance(test_X, pd.DataFrame):\n",
    "        test_X = pd.DataFrame(test_X, columns=feature_columns)\n",
    "\n",
    "    # ✅ Ensure y_test is a Series\n",
    "    if isinstance(test_y, pd.DataFrame):\n",
    "        test_y = test_y.squeeze()  # Convert to Series if needed\n",
    "\n",
    "    # ✅ Ensure 'activity' column exists\n",
    "    if \"activity\" not in test_X.columns:\n",
    "        raise ValueError(\"Dataset does not contain an 'activity' column.\")\n",
    "\n",
    "    is_lstm = isinstance(model, tf.keras.Model)\n",
    "    results = []\n",
    "\n",
    "    # ✅ Loop through each unique activity and evaluate model performance\n",
    "    for activity in test_X[\"activity\"].unique():\n",
    "        # Filter test data for the current activity\n",
    "        X_test_activity = test_X[test_X[\"activity\"] == activity].copy()\n",
    "\n",
    "        # Extract the actual labels\n",
    "        y_test_activity = test_y.loc[X_test_activity.index].values.flatten()  # Ensure correct shape\n",
    "\n",
    "        # ✅ Check for missing values\n",
    "        if np.isnan(y_test_activity).any():\n",
    "            print(f\"Warning: NaN values found in y_test_activity for activity {activity}!\")\n",
    "            y_test_activity = np.nan_to_num(y_test_activity, nan=0)  # Replace NaN with 0 (No Fall)\n",
    "\n",
    "        # ✅ Ensure y_test_activity is integer\n",
    "        try:\n",
    "            y_test_activity = y_test_activity.astype(int)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error converting y_test_activity to integer for activity {activity}: {e}\")\n",
    "            continue  # Skip this activity if conversion fails\n",
    "\n",
    "        # ✅ Select feature columns\n",
    "        if is_lstm:\n",
    "            feature_columns = [col for col in X_test_activity.columns if col.startswith(\"value\")]\n",
    "\n",
    "        X_test_activity = X_test_activity[feature_columns].values  # Extract feature values\n",
    "\n",
    "        print(f\"Activity: {activity}, X shape: {X_test_activity.shape}, y shape: {y_test_activity.shape}\")\n",
    "\n",
    "        # ✅ Reshape X for LSTM input\n",
    "        if is_lstm:\n",
    "            X_test_activity = X_test_activity.reshape(-1, 500, 1)\n",
    "\n",
    "        # ✅ Predict fall_binary values\n",
    "        y_pred = model.predict(X_test_activity).round().astype(int).flatten()\n",
    "\n",
    "        # ✅ Compute correct and incorrect counts\n",
    "        correct = np.sum(y_pred == y_test_activity)\n",
    "        incorrect = len(y_pred) - correct\n",
    "\n",
    "        # ✅ Determine if the first sample of the activity is a fall or not\n",
    "        actual_fall = \"Fall\" if y_test_activity[0] == 1 else \"No Fall\"\n",
    "\n",
    "        print(f\"Activity {activity}: {correct} correct, {incorrect} incorrect\")\n",
    "\n",
    "        # ✅ Store results\n",
    "        results.append({\n",
    "            \"activity\": activity,\n",
    "            \"Actual Fall\": actual_fall,  # ✅ Single column for actual fall status\n",
    "            \"correct\": correct,\n",
    "            \"incorrect\": incorrect,\n",
    "            \"total\": len(y_pred),\n",
    "            \"accuracy\": correct / len(y_pred) if len(y_pred) > 0 else 0\n",
    "        })\n",
    "\n",
    "    # ✅ Convert results into a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # ✅ Mapping dictionary from abbreviations to full names\n",
    "    activity_mapping = {\n",
    "        \"S\": \"Still\",\n",
    "        \"CD\": \"Close Door\",\n",
    "        \"KD\": \"Knock Door\",\n",
    "        \"MA\": \"Minor Ambience (Sitting and Eating)\",\n",
    "        \"FOB\": \"Fall of object (Backpack)\",\n",
    "        \"FOL\": \"Fall of object (FaszienRolle)\",\n",
    "        \"WBS\": \"Walk by Sensor\",\n",
    "        \"RBS\": \"Rush by Sensor\",\n",
    "        \"SC\": \"Sitting down on chair\",\n",
    "        \"LC\": \"Laying down on couch\",\n",
    "        \"STC\": \"Stand up from Chair\",\n",
    "        \"PUF\": \"Picking something up from floor\",\n",
    "        \"K\": \"Kneeling down then standing up\",\n",
    "        \"SLB\": \"Standing Lost Balance\",\n",
    "        \"TF\": \"Trip and Fall - Forwards\",\n",
    "        \"SFB\": \"Slip and Fall - Backwards\",\n",
    "        \"FCS\": \"Chair - Fall to side\",\n",
    "        \"FCF\": \"Chair - Fall to Front\",\n",
    "        \"LAF\": \"Lying - Awake Fall\",\n",
    "        \"LSF\": \"Lying - Asleep Fall\",\n",
    "        \"FR\": \"Fall Recovery\",\n",
    "        \"KID\": \"Kids Running\"\n",
    "    }\n",
    "\n",
    "    # ✅ Function to extract the relevant part before the first '_'\n",
    "    def get_activity_name(code):\n",
    "        key = code.split('_')[0]  # Extract first part of activity code\n",
    "        return activity_mapping.get(key, code)  # Replace with full name if exists\n",
    "\n",
    "    # ✅ Apply the mapping to the results dataframe\n",
    "    results_df[\"activity\"] = results_df[\"activity\"].apply(get_activity_name)\n",
    "\n",
    "    # ✅ Save the results as a CSV file\n",
    "    results_path = \"activity_results.csv\"\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "\n",
    "    # ✅ Log results in MLflow\n",
    "    mlflow.log_artifact(results_path)\n",
    "\n",
    "    print(\"\\n📊 Per-Activity Results:\")\n",
    "    print(results_df)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_per_distance(model, test_X, test_y, feature_columns=None):\n",
    "    \"\"\"\n",
    "    Evaluates model performance per distance type and logs the results in MLflow.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model (LSTM or Tree-based).\n",
    "        test_X: Test feature data (DataFrame).\n",
    "        test_y: Test target labels (Series).\n",
    "        feature_columns: The feature columns used for training.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing classification results per distance.\n",
    "    \"\"\"\n",
    "\n",
    "    X_test = test_X.copy()\n",
    "    y_test = test_y.copy()\n",
    "\n",
    "    # ✅ Ensure X_test is a DataFrame\n",
    "    if not isinstance(X_test, pd.DataFrame):\n",
    "        X_test = pd.DataFrame(X_test, columns=feature_columns)\n",
    "\n",
    "    # ✅ Ensure y_test is a Series\n",
    "    if isinstance(y_test, pd.DataFrame):\n",
    "        y_test = y_test.squeeze()  # Convert to Series if needed\n",
    "\n",
    "    is_lstm = isinstance(model, tf.keras.Model)\n",
    "\n",
    "    # ✅ Ensure 'distance_m' column exists\n",
    "    if \"distance_m\" not in X_test.columns:\n",
    "        raise ValueError(\"Dataset does not contain a 'distance_m' column.\")\n",
    "\n",
    "    # ✅ Get unique distances and setup subplots dynamically\n",
    "    unique_distances = sorted(X_test[\"distance_m\"].unique())\n",
    "    fig, axes = plt.subplots(len(unique_distances), 1, figsize=(6, 4 * len(unique_distances))) \n",
    "\n",
    "    results = []\n",
    "    confusion_matrices = {}\n",
    "\n",
    "    # ✅ Loop through each unique distance and evaluate model performance\n",
    "    for i, distance in enumerate(unique_distances):\n",
    "        X_test_distance = X_test[X_test[\"distance_m\"] == distance].copy()\n",
    "        y_test_distance = y_test.loc[X_test_distance.index].values.flatten()  # Ensure correct shape\n",
    "\n",
    "        # ✅ Select feature columns\n",
    "        if is_lstm:\n",
    "            feature_columns = [col for col in X_test_distance.columns if col.startswith(\"value\")]\n",
    "\n",
    "        X_test_distance = X_test_distance[feature_columns].values  # Extract feature values\n",
    "\n",
    "        print(f\"Distance: {distance}, X shape: {X_test_distance.shape}, y shape: {y_test_distance.shape}\")\n",
    "\n",
    "        # ✅ Reshape X for LSTM input\n",
    "        if is_lstm:\n",
    "            X_test_distance = X_test_distance.reshape(-1, 500, 1)\n",
    "\n",
    "        # ✅ Predict fall_binary values\n",
    "        y_pred = model.predict(X_test_distance).round().astype(int).flatten()\n",
    "\n",
    "        # ✅ Compute correct and incorrect counts\n",
    "        correct = np.sum(y_pred == y_test_distance)\n",
    "        incorrect = len(y_pred) - correct\n",
    "\n",
    "        # ✅ Compute confusion matrix, ensuring both classes appear\n",
    "        cm = confusion_matrix(y_test_distance, y_pred, labels=[1, 0])\n",
    "\n",
    "        # ✅ Ensure confusion matrix always has shape (2,2)\n",
    "        if cm.shape == (1, 1):  \n",
    "            cm_fixed = np.array([[cm[0, 0], 0], [0, 0]])  \n",
    "        elif cm.shape == (1, 2):  \n",
    "            cm_fixed = np.vstack([cm, [0, 0]])  \n",
    "        elif cm.shape == (2, 1):  \n",
    "            cm_fixed = np.hstack([cm, [[0], [0]]])  \n",
    "        else:\n",
    "            cm_fixed = cm  \n",
    "\n",
    "        # ✅ Correct confusion matrix order:\n",
    "        # TP | FN\n",
    "        # FP | TN\n",
    "        cm_corrected = np.array([\n",
    "            [cm_fixed[0, 0], cm_fixed[1, 0]],  # True Positives, False Negatives\n",
    "            [cm_fixed[0, 1], cm_fixed[1, 1]]   # False Positives, True Negatives\n",
    "        ])\n",
    "\n",
    "        confusion_matrices[distance] = cm_corrected\n",
    "\n",
    "        # ✅ Plot confusion matrix with **correct** labels\n",
    "        sns.heatmap(cm_corrected, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                    xticklabels=[\"Actual Fall\", \"Actual No Fall\"], \n",
    "                    yticklabels=[\"Predicted Fall\", \"Predicted No Fall\"],\n",
    "                    ax=axes[i] if len(unique_distances) > 1 else axes)\n",
    "        axes[i].set_title(f\"Confusion Matrix - Distance {distance}\")\n",
    "        axes[i].set_xlabel(\" \")\n",
    "        axes[i].set_ylabel(\" \")\n",
    "\n",
    "        print(f\"Distance {distance}: {correct} correct, {incorrect} incorrect\")\n",
    "\n",
    "        # ✅ Store results\n",
    "        results.append({\n",
    "            \"distance\": distance,\n",
    "            \"correct\": correct,\n",
    "            \"incorrect\": incorrect,\n",
    "            \"total\": len(y_pred),\n",
    "            \"accuracy\": correct / len(y_pred) if len(y_pred) > 0 else 0\n",
    "        })\n",
    "\n",
    "    # ✅ Save and log confusion matrix plot\n",
    "    confusion_matrix_path = \"confusion_matrices_distance.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(confusion_matrix_path)\n",
    "    plt.close()\n",
    "\n",
    "    # ✅ Convert results into a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    # ✅ Save results as CSV\n",
    "    results_path = \"distance_results.csv\"\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "\n",
    "    # ✅ Log results in MLflow\n",
    "    mlflow.log_artifact(results_path)\n",
    "    mlflow.log_artifact(confusion_matrix_path)\n",
    "\n",
    "    print(\"\\n📊 Per-Distance Results:\")\n",
    "    print(results_df)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation and Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(data, noise_std=0.01):\n",
    "    \"\"\"\n",
    "    Augments raw sensor data by adding Gaussian noise.\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): Raw sensor data as a 1D numpy array (e.g., shape (500,)).\n",
    "        noise_std (float): Standard deviation of the Gaussian noise.\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Data after adding Gaussian noise.\n",
    "    \"\"\"\n",
    "    noise = np.random.normal(loc=0, scale=noise_std, size=data.shape)\n",
    "    return data + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_shift(data, baseline=9.44, factor=1.02):\n",
    "    \"\"\"\n",
    "    Augments raw sensor data by scaling the deviation from a baseline.\n",
    "    \n",
    "    The transformation applied is:\n",
    "        x_new = baseline + (x - baseline) * factor\n",
    "    \n",
    "    This increases the distance of each point from the baseline by the given factor.\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): Raw sensor data as a 1D numpy array.\n",
    "        baseline (float): The baseline value to compare against (default: 9.44).\n",
    "        factor (float): Scaling factor for the deviation (default: 1.05).\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Data after applying the baseline-based shift.\n",
    "    \"\"\"\n",
    "    return baseline + (data - baseline) * factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_time_shift(data, shift=25):\n",
    "    \"\"\"\n",
    "    Augments raw sensor data by shifting the time sequence.\n",
    "    \n",
    "    This function removes the first `shift` values, shifts the data to the left,\n",
    "    and appends the last value repeated `shift` times to maintain the original length.\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): Raw sensor data as a 1D numpy array (expected shape: (500,)).\n",
    "        shift (int): Number of points to shift (default: 25).\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Time-shifted data with the same shape as input.\n",
    "    \"\"\"\n",
    "    if len(data) < shift:\n",
    "        raise ValueError(\"Data length must be greater than the shift value.\")\n",
    "    shifted_data = np.concatenate([data[shift:], np.repeat(data[-1], shift)])\n",
    "    return shifted_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df, signal_columns):\n",
    "    \"\"\"\n",
    "    Extracts engineered features from raw sensor data and prints NaN counts for debugging.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing raw sensor data.\n",
    "        signal_columns (list): List of column names corresponding to the raw sensor data.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with the engineered features ['max', 'mean', 'median', 'p2p', 'impulse'].\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "    sampling_interval = 4.5 / 500  # equals 0.009\n",
    "\n",
    "    # Compute basic statistics\n",
    "    df_features['max'] = df_features[signal_columns].max(axis=1)\n",
    "    df_features['min'] = df_features[signal_columns].min(axis=1)\n",
    "    df_features['mean'] = df_features[signal_columns].mean(axis=1)\n",
    "    df_features['median'] = df_features[signal_columns].median(axis=1)\n",
    "\n",
    "    # Compute additional features\n",
    "    df_features['p2p'] = df_features['max'] - df_features['min']\n",
    "    df_features['peak'] = df_features[signal_columns].max(axis=1)\n",
    "    \n",
    "    # Compute impulse with a small epsilon to avoid division issues\n",
    "    df_features['impulse'] = df_features['peak'] * sampling_interval / (df_features['mean'])\n",
    "    \n",
    "    return df_features[['max', 'mean', 'median', 'p2p', 'impulse']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Classical Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, model_class, X_train, y_train, augment_data=None, signal_columns=None, feature_columns=None, data_train=None, data_test=None):\n",
    "    \"\"\"\n",
    "    Defines the Optuna optimization objective with K-Fold Cross-Validation for raw sensor data.\n",
    "    The raw sensor data (with 500 columns) is optionally augmented on the training folds,\n",
    "    and then engineered features are extracted before training.\n",
    "    \n",
    "    Augmentation options:\n",
    "        - \"noise\": adds Gaussian noise (appends one copy)\n",
    "        - \"baseline_shift\": applies the baseline shift augmentation (appends one copy)\n",
    "        - \"time_shift\": applies the time sequence shift augmentation (appends one copy)\n",
    "        - \"all\": applies all three augmentations and appends them (total training size becomes 4x)\n",
    "    \n",
    "    Args:\n",
    "        trial: Optuna trial object.\n",
    "        model_class: The model class (e.g., XGBClassifier, RandomForestClassifier, GradientBoostingClassifier, SVC).\n",
    "        X_train, y_train: Raw sensor training data and labels.\n",
    "        augment_data (str): Which augmentation method to apply (or \"all\" for all three).\n",
    "        signal_columns (list): List of column names corresponding to the raw sensor data.\n",
    "        feature_columns: Not used here in extraction; engineered features will always be ['max', 'mean', 'median', 'p2p', 'impulse'].\n",
    "        data_train, data_test: Provided for compatibility.\n",
    "        \n",
    "    Returns:\n",
    "        The average F1-score across all folds.\n",
    "    \"\"\"\n",
    "    # Suggest hyperparameters based on model type\n",
    "    if model_class == XGBClassifier:\n",
    "        num_no_falls = sum(y_train == 0)\n",
    "        num_falls = sum(y_train == 1)\n",
    "        default_scale_pos_weight = num_no_falls / num_falls\n",
    "\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500, step=25),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.5, log=True),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0, 10),\n",
    "            \"scale_pos_weight\": trial.suggest_float(\"scale_pos_weight\", default_scale_pos_weight * 0.5, default_scale_pos_weight * 1.5)\n",
    "        }\n",
    "        model = XGBClassifier(**params, objective=\"binary:logistic\", random_state=42)\n",
    "\n",
    "    elif model_class == RandomForestClassifier:\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500, step=25),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 15),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5),\n",
    "        }\n",
    "        model = RandomForestClassifier(**params, random_state=42)\n",
    "\n",
    "    elif model_class == GradientBoostingClassifier:\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500, step=25),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.5, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5),\n",
    "        }\n",
    "        model = GradientBoostingClassifier(**params, random_state=42)\n",
    "\n",
    "    elif model_class == SVC:\n",
    "        params = {\n",
    "            \"C\": trial.suggest_float(\"C\", 0.1, 100, log=True),\n",
    "            \"kernel\": trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\"]),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 0.001, 10, log=True),\n",
    "        }\n",
    "        model = SVC(**params, probability=True, random_state=42)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model class: {model_class.__name__}\")\n",
    "\n",
    "    is_svm = model_class == SVC\n",
    "\n",
    "    # K-Fold Cross-Validation (Stratified)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        # Split raw sensor data\n",
    "        X_fold_train = X_train.iloc[train_idx].copy()\n",
    "        X_fold_val = X_train.iloc[val_idx].copy()\n",
    "        y_fold_train = y_train.iloc[train_idx].copy()\n",
    "        y_fold_val = y_train.iloc[val_idx].copy()\n",
    "\n",
    "        # Apply augmentation on training fold if requested\n",
    "        if augment_data is not None:\n",
    "            if augment_data == \"all\":\n",
    "                augmented_dfs = []\n",
    "                for method in [\"noise\", \"baseline_shift\", \"time_shift\"]:\n",
    "                    if method == \"noise\":\n",
    "                        aug = X_fold_train.apply(lambda row: pd.Series(add_gaussian_noise(row.values), index=signal_columns), axis=1)\n",
    "                    elif method == \"baseline_shift\":\n",
    "                        aug = X_fold_train.apply(lambda row: pd.Series(augment_shift(row.values, baseline=9.44, factor=1.05), index=signal_columns), axis=1)\n",
    "                    elif method == \"time_shift\":\n",
    "                        aug = X_fold_train.apply(lambda row: pd.Series(augment_time_shift(row.values, shift=25), index=signal_columns), axis=1)\n",
    "                    \n",
    "                    augmented_dfs.append(aug)\n",
    "                # Append all augmented copies to the original training data\n",
    "                X_fold_train = pd.concat([X_fold_train] + augmented_dfs, ignore_index=False)\n",
    "                y_fold_train = pd.concat([y_fold_train] * (1 + len(augmented_dfs)), ignore_index=False)\n",
    "            else:\n",
    "                # Single augmentation option\n",
    "                if augment_data == \"noise\":\n",
    "                    aug = X_fold_train.apply(lambda row: pd.Series(add_gaussian_noise(row.values), index=signal_columns), axis=1)\n",
    "                elif augment_data == \"baseline_shift\":\n",
    "                    aug = X_fold_train.apply(lambda row: pd.Series(augment_shift(row.values, baseline=9.44, factor=1.05), index=signal_columns), axis=1)\n",
    "                elif augment_data == \"time_shift\":\n",
    "                    aug = X_fold_train.apply(lambda row: pd.Series(augment_time_shift(row.values, shift=25), index=signal_columns), axis=1)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported augmentation method: {augment_data}\")\n",
    "                \n",
    "                X_fold_train = pd.concat([X_fold_train, aug], ignore_index=False)\n",
    "                y_fold_train = pd.concat([y_fold_train, y_fold_train], ignore_index=False)\n",
    "\n",
    "        # Extract engineered features for both training and validation folds\n",
    "        X_fold_train_features = extract_features(X_fold_train, signal_columns)\n",
    "        X_fold_val_features = extract_features(X_fold_val, signal_columns)\n",
    "\n",
    "        features = ['max', 'mean', 'median', 'p2p', 'impulse']\n",
    "\n",
    "        if is_svm:\n",
    "            scaler = StandardScaler()\n",
    "            X_fold_train_features = scaler.fit_transform(X_fold_train_features)\n",
    "            X_fold_val_features = scaler.transform(X_fold_val_features)\n",
    "\n",
    "            X_fold_train_features = pd.DataFrame(X_fold_train_features, columns=features, index=X_fold_train.index)\n",
    "            X_fold_val_features = pd.DataFrame(X_fold_val_features, columns=features, index=X_fold_val.index)\n",
    "\n",
    "        \n",
    "        model.fit(X_fold_train_features, y_fold_train)\n",
    "        y_pred = model.predict(X_fold_val_features)\n",
    "        f1_scores.append(f1_score(y_fold_val, y_pred))\n",
    "\n",
    "    return np.mean(f1_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_log_classical_model(\n",
    "    model, \n",
    "    X_train, X_test, y_train, y_test, X_test_full,\n",
    "    save_name=\"classification_model\", \n",
    "    experiment_name=\"classical_models_experiment\",\n",
    "    target_column=\"fall_binary\",\n",
    "    feature_columns=None,   # expected to be the list of raw sensor columns (e.g., 500 columns)\n",
    "    if_optuna=True,\n",
    "    n_trials=250,  # Number of hyperparameter tuning trials\n",
    "    dataset_name=\"fall_data.csv\",\n",
    "    augment_data=None,   # Options: \"noise\", \"baseline_shift\", \"time_shift\", \"all\", or None\n",
    "    data_train=None,\n",
    "    data_test=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a classical ML model (RandomForest, XGBoost, GradientBoosting, or SVC) using raw sensor data.\n",
    "    The raw data is optionally augmented on the training set with one or all custom augmentations,\n",
    "    then engineered features (max, mean, median, p2p, impulse) are extracted before training.\n",
    "    All logging is performed via MLflow.\n",
    "    \n",
    "    Args:\n",
    "        model: A classical ML model instance.\n",
    "        X_train, X_test, y_train, y_test: Pre-split raw sensor data and labels.\n",
    "        X_test_full: Full test set with additional metadata.\n",
    "        save_name: Name to save the trained model.\n",
    "        experiment_name: Name of the MLflow experiment.\n",
    "        target_column: The target column name.\n",
    "        feature_columns: List of column names representing the raw sensor data.\n",
    "        if_optuna: Whether to perform hyperparameter tuning via Optuna.\n",
    "        n_trials: Number of trials for hyperparameter optimization.\n",
    "        dataset_name: Name of the dataset file.\n",
    "        augment_data: Which augmentation method to apply. Options: \"noise\", \"baseline_shift\", \"time_shift\", \"all\", or None.\n",
    "        \n",
    "    Returns:\n",
    "        The trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Start MLflow experiment\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run():\n",
    "        # Identify model class\n",
    "        model_class = type(model)\n",
    "        is_svm = isinstance(model, SVC)\n",
    "\n",
    "        if if_optuna:\n",
    "            # Perform hyperparameter optimization on the raw data with augmentation inside the CV folds\n",
    "            study = optuna.create_study(direction=\"maximize\")\n",
    "            study.optimize(\n",
    "                lambda trial: objective(trial, model_class, X_train, y_train, \n",
    "                                          augment_data=augment_data, signal_columns=feature_columns, feature_columns=[\"max\", \"mean\", \"median\", \"p2p\", \"impulse\"], data_train=data_train, data_test=data_test),\n",
    "                n_trials=n_trials\n",
    "            )\n",
    "            best_params = study.best_params\n",
    "            print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "            # Train final model with best parameters\n",
    "            if model_class == XGBClassifier:\n",
    "                best_model = XGBClassifier(**best_params, objective=\"binary:logistic\", use_label_encoder=False, random_state=42)\n",
    "            elif model_class.__name__ == \"GradientBoostingClassifier\":\n",
    "                best_model = model_class(**best_params, random_state=42)\n",
    "            elif model_class.__name__ == \"RandomForestClassifier\":\n",
    "                best_model = model_class(**best_params, random_state=42)\n",
    "            elif model_class == SVC:\n",
    "                best_model = SVC(**best_params, probability=True, random_state=42)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model class: {model_class.__name__}\")\n",
    "        else:\n",
    "            best_model = model\n",
    "\n",
    "        # Log the augmentation method (if any)\n",
    "        if augment_data is not None:\n",
    "            mlflow.log_param(\"augmentation_method\", augment_data)\n",
    "        \n",
    "        print(len(X_train), len(y_train))\n",
    "        print(len(X_test), len(y_test))\n",
    "\n",
    "        # Apply augmentation on training data (only on training raw data)\n",
    "        if augment_data is not None:\n",
    "            if augment_data == \"all\":\n",
    "                augmented_dfs = []\n",
    "                # Loop through all three augmentation methods\n",
    "                for method in [\"noise\", \"baseline_shift\", \"time_shift\"]:\n",
    "                    if method == \"noise\":\n",
    "                        aug = X_train.apply(lambda row: pd.Series(add_gaussian_noise(row.values), index=feature_columns), axis=1)\n",
    "                    elif method == \"baseline_shift\":\n",
    "                        aug = X_train.apply(lambda row: pd.Series(augment_shift(row.values, baseline=9.44, factor=1.05), index=feature_columns), axis=1)\n",
    "                    elif method == \"time_shift\":\n",
    "                        aug = X_train.apply(lambda row: pd.Series(augment_time_shift(row.values, shift=25), index=feature_columns), axis=1)\n",
    "                    \n",
    "                    # No need to reconvert; each lambda now returns a Series with proper columns.\n",
    "                    nans = aug.isnull().sum().sum()\n",
    "                    if nans > 0:\n",
    "                        print(f\"DEBUG: Augmentation method '{method}' in fold produced {nans} NaN values. Shape: {aug.shape}\")\n",
    "                    else:\n",
    "                        print(f\"DEBUG: Augmentation method '{method}' produced no NaN values. Shape: {aug.shape}\")\n",
    "                    \n",
    "                    augmented_dfs.append(aug)\n",
    "                # Append all augmented copies with the original data (original + 3 copies = 4x size)\n",
    "                X_train = pd.concat([X_train] + augmented_dfs, ignore_index=False)\n",
    "                y_train = pd.concat([y_train] * (1 + len(augmented_dfs)), ignore_index=False)\n",
    "            else:\n",
    "                # Single augmentation option\n",
    "                if augment_data == \"noise\":\n",
    "                    aug = X_train.apply(lambda row: pd.Series(add_gaussian_noise(row.values), index=feature_columns), axis=1)\n",
    "                elif augment_data == \"baseline_shift\":\n",
    "                    aug = X_train.apply(lambda row: pd.Series(augment_shift(row.values, baseline=9.44, factor=1.05), index=feature_columns), axis=1)\n",
    "                elif augment_data == \"time_shift\":\n",
    "                    aug = X_train.apply(lambda row: pd.Series(augment_time_shift(row.values, shift=25), index=feature_columns), axis=1)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported augmentation method: {augment_data}\")\n",
    "                \n",
    "                nans = aug.isnull().sum().sum()\n",
    "                if nans > 0:\n",
    "                    print(f\"DEBUG: Single augmentation '{augment_data}' produced {nans} NaN values. Shape: {aug.shape}\")\n",
    "                else:\n",
    "                    print(f\"DEBUG: Single augmentation '{augment_data}' produced no NaN values. Shape: {aug.shape}\")\n",
    "                \n",
    "                X_train = pd.concat([X_train, aug], ignore_index=False)\n",
    "                y_train = pd.concat([y_train, y_train], ignore_index=False)\n",
    "        \n",
    "        # Extract engineered features for training and test sets\n",
    "        X_train_features = extract_features(X_train, feature_columns)\n",
    "        X_test_features = extract_features(X_test, feature_columns)\n",
    "\n",
    "        print(len(X_train_features), len(y_train))\n",
    "        print(len(X_test_features), len(y_test))\n",
    "\n",
    "        features = ['max', 'mean', 'median', 'p2p', 'impulse']\n",
    "\n",
    "        # Append data_train three times for the three augmentations\n",
    "        data_train_aug = pd.concat([data_train] * 4, ignore_index=True) \n",
    "\n",
    "        if is_svm:\n",
    "            scaler = StandardScaler()\n",
    "            X_train_features = scaler.fit_transform(X_train_features)\n",
    "            X_test_features = scaler.transform(X_test_features)\n",
    "\n",
    "            # Convert X_train and X_test back to DataFrame if needed\n",
    "            X_train_features = pd.DataFrame(X_train_features, columns=features, index=data_train_aug.index)\n",
    "            X_test_features = pd.DataFrame(X_test_features, columns=features, index=data_test.index)\n",
    "\n",
    "        # Train final model using engineered features\n",
    "        best_model.fit(X_train_features, y_train)\n",
    "        y_pred = best_model.predict(X_test_features)\n",
    "\n",
    "        # Compute evaluation metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        if hasattr(best_model, \"predict_proba\"):\n",
    "            roc_auc = roc_auc_score(y_test, best_model.predict_proba(X_test_features)[:, 1])\n",
    "        else:\n",
    "            roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "        # Log model parameters\n",
    "        mlflow.log_param(\"model_type\", best_model.__class__.__name__)\n",
    "        if if_optuna:\n",
    "            for param, value in best_params.items():\n",
    "                mlflow.log_param(param, value)\n",
    "        mlflow.log_param(\"num_features\", X_train_features.shape[1])\n",
    "        mlflow.log_param(\"features\", \"max, mean, median, p2p, impulse\")\n",
    "        mlflow.log_param(\"target_column\", target_column)\n",
    "        mlflow.log_param(\"dataset\", dataset_name)\n",
    "\n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "\n",
    "        # Save best model\n",
    "        models_folder = \"models\"\n",
    "        if not os.path.exists(models_folder):\n",
    "            os.makedirs(models_folder)\n",
    "        save_path = os.path.join(models_folder, f\"{save_name}.pkl\")\n",
    "        joblib.dump(best_model, save_path)\n",
    "        mlflow.log_artifact(save_path)\n",
    "\n",
    "        # Generate and log confusion matrix\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        conf_matrix = np.array([\n",
    "            [conf_matrix[0, 0], conf_matrix[1, 0]],\n",
    "            [conf_matrix[0, 1], conf_matrix[1, 1]]\n",
    "        ])\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(\n",
    "            conf_matrix[::-1, ::-1],\n",
    "            annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=[\"Actual Fall\", \"Actual No Fall\"], \n",
    "            yticklabels=[\"Predicted Fall\", \"Predicted No Fall\"],\n",
    "        )\n",
    "        plt.xlabel(\" \")\n",
    "        plt.ylabel(\" \")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        conf_matrix_path = \"confusion_matrix.png\"\n",
    "        plt.savefig(conf_matrix_path)\n",
    "        plt.close()\n",
    "        mlflow.log_artifact(conf_matrix_path)\n",
    "\n",
    "        print(f\"Model trained with accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "        # Assume raw_sensor_columns is the list of raw sensor column names\n",
    "        raw_sensor_columns = [col for col in X_test_full.columns if col.startswith(\"value\")]\n",
    "\n",
    "        # Extract engineered features from the raw sensor columns\n",
    "        features_df = extract_features(X_test_full[raw_sensor_columns], raw_sensor_columns)\n",
    "\n",
    "        # Remove the raw sensor columns from the full test set but keep metadata\n",
    "        X_test_full_metadata = X_test_full.drop(columns=raw_sensor_columns)\n",
    "        X_test_full_features = pd.concat([X_test_full_metadata, features_df], axis=1)\n",
    "\n",
    "        engineered_feature_columns = ['max', 'mean', 'median', 'p2p', 'impulse']\n",
    "        evaluate_per_activity(best_model, X_test_full_features, y_test, engineered_feature_columns)\n",
    "        evaluate_per_distance(best_model, X_test_full_features, y_test, engineered_feature_columns)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pipe(\n",
    "    model, \n",
    "    dataset_name, \n",
    "    save_name=\"fall_model\", \n",
    "    feature_columns=None, \n",
    "    target_column=\"fall_binary\", \n",
    "    experiment_name=\"default_experiment\",\n",
    "    use_early_stopping=True,\n",
    "    if_optuna=True,\n",
    "    n_trials=250,\n",
    "    augment_data=None\n",
    "):\n",
    "    \"\"\"\n",
    "    General training pipeline for both classical models (XGBoost, RF, SVM) and deep learning models.\n",
    "\n",
    "    Args:\n",
    "        model: The initialized model (LSTM, RNN, RF, XGB, etc.).\n",
    "        dataset_name: The dataset CSV file.\n",
    "        save_name: Name to save the trained model.\n",
    "        feature_columns: List of feature column names.\n",
    "        target_column: The name of the target column.\n",
    "        experiment_name: MLflow experiment name.\n",
    "        use_early_stopping: Whether to use early stopping (for neural networks).\n",
    "        if_optuna: Whether to perform hyperparameter tuning (for tree-based models).\n",
    "\n",
    "    Returns:\n",
    "        The trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    # ✅ Prevent overwriting existing models unless explicitly handled\n",
    "    if os.path.exists(f\"models/{save_name}.pkl\") or os.path.exists(f\"models/{save_name}.keras\"):\n",
    "        raise ValueError(f\"Model name '{save_name}' already exists. Choose a new name or delete the existing model.\")\n",
    "\n",
    "    # ✅ Load dataset\n",
    "    df = load_data(dataset_name)\n",
    "\n",
    "    # ✅ Auto-detect feature columns if not explicitly provided\n",
    "    value_columns = [col for col in df.columns if col.startswith(\"value\")]\n",
    "    if value_columns:\n",
    "        feature_columns = value_columns  \n",
    "    elif feature_columns is None:\n",
    "        raise ValueError(\"Feature columns must be specified.\")\n",
    "\n",
    "    # ✅ Ensure target column exists\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in dataset.\")\n",
    "\n",
    "    data_train, data_test = stratified_activity_split(df, target_column=target_column, test_size=0.2, random_state=42)\n",
    "    print(f\"Train shape: {data_train.shape}, Test shape: {data_test.shape}\")\n",
    "\n",
    "    # ✅ Keep full test data (including metadata like 'activity' or 'distance_m')\n",
    "    X_test_full = data_test.copy()  \n",
    "\n",
    "    # ✅ Keep `X_train`, `X_test` as DataFrames & `y_train`, `y_test` as Series\n",
    "    X_train = data_train[feature_columns]  \n",
    "    X_test = data_test[feature_columns]    \n",
    "    y_train = data_train[target_column]  \n",
    "    y_test = data_test[target_column]    \n",
    "\n",
    "    # ✅ Check model type\n",
    "    is_tree = isinstance(model, (XGBClassifier, RandomForestClassifier, GradientBoostingClassifier))\n",
    "    is_svm = isinstance(model, SVC)\n",
    "\n",
    "    # ✅ Train tree-based models with optional Optuna hyperparameter tuning\n",
    "    if is_tree or is_svm:\n",
    "        model = train_and_log_classical_model(\n",
    "            model, X_train, X_test, y_train, y_test, X_test_full,\n",
    "            save_name=save_name,\n",
    "            experiment_name=experiment_name,\n",
    "            target_column=target_column,\n",
    "            feature_columns=feature_columns,\n",
    "            if_optuna=if_optuna,\n",
    "            n_trials=n_trials,\n",
    "            dataset_name=dataset_name,\n",
    "            augment_data=augment_data,\n",
    "            data_train=data_train,\n",
    "            data_test=data_test\n",
    "        )\n",
    "    \n",
    "    # Delete the files activity_results.csv and distance_results.csv\n",
    "    os.remove(\"activity_results.csv\")\n",
    "    os.remove(\"distance_results.csv\")\n",
    "    os.remove(\"confusion_matrices_distance.png\")\n",
    "    os.remove(\"confusion_matrix.png\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# XGBoost Model\n",
    "# ==========================\n",
    "\n",
    "def build_xgboost_model():\n",
    "    \"\"\"\n",
    "    Builds an XGBoost model for binary classification.\n",
    "\n",
    "    Returns:\n",
    "        A compiled XGBoost model.\n",
    "    \"\"\"\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=400,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.11,\n",
    "        objective=\"binary:logistic\",\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# ==========================\n",
    "# Random Forest Model\n",
    "# ==========================\n",
    "\n",
    "def build_random_forest_model():\n",
    "    \"\"\"\n",
    "    Builds a Random Forest model for binary classification.\n",
    "\n",
    "    Returns:\n",
    "        A compiled Random Forest model.\n",
    "    \"\"\"\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# ==========================\n",
    "# GBM Model\n",
    "# ==========================\n",
    "def build_gradient_boosting_model():\n",
    "    \"\"\"\n",
    "    Builds a Gradient Boosting (GBM) model for binary classification.\n",
    "\n",
    "    Returns:\n",
    "        A compiled Gradient Boosting model.\n",
    "    \"\"\"\n",
    "    return GradientBoostingClassifier(\n",
    "        n_estimators=200,  # Number of boosting stages\n",
    "        learning_rate=0.1,  # Step size shrinkage to prevent overfitting\n",
    "        max_depth=5,  # Maximum depth of the trees\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "# ==========================\n",
    "# SVM Model\n",
    "# ==========================\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def build_svm_model():\n",
    "    \"\"\"\n",
    "    Builds an SVM model for binary classification.\n",
    "\n",
    "    Returns:\n",
    "        A configured SVM model.\n",
    "    \"\"\"\n",
    "    model = SVC(\n",
    "        kernel=\"rbf\",  # Radial Basis Function (RBF) kernel (default)\n",
    "        C=1.0,         # Regularization parameter\n",
    "        gamma=\"scale\",  # Kernel coefficient\n",
    "        probability=True,  # Enable probability estimates (needed for ROC AUC)\n",
    "        random_state=42\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# RNN Model\n",
    "# ==========================\n",
    "def build_rnn_model():\n",
    "    \"\"\"\n",
    "    Builds a simple RNN model using Keras.\n",
    "\n",
    "    Returns:\n",
    "        A compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        SimpleRNN(RNN_UNITS, input_shape=INPUT_SHAPE, return_sequences=True),\n",
    "        SimpleRNN(RNN_UNITS),\n",
    "        Dense(DENSE_UNITS, activation=\"relu\"),\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNCTION, metrics=METRICS)\n",
    "    return model\n",
    "\n",
    "# ==========================\n",
    "# LSTM Model\n",
    "# ==========================\n",
    "def build_lstm_model(l2_lambda=0.001, dropout_rate=0.2, clipnorm=1.0):\n",
    "    \"\"\"\n",
    "    Builds an LSTM-based model with L2 regularization, dropout, and gradient clipping.\n",
    "\n",
    "    Args:\n",
    "        l2_lambda: Strength of L2 regularization (default: 0.001).\n",
    "        dropout_rate: Dropout rate to reduce overfitting (default: 0.2).\n",
    "        clipnorm: Gradient clipping norm (default: 1.0).\n",
    "\n",
    "    Returns:\n",
    "        A compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(RNN_UNITS, input_shape=INPUT_SHAPE, return_sequences=True, \n",
    "             kernel_regularizer=l2(l2_lambda)),\n",
    "        Dropout(dropout_rate),  # Dropout after first LSTM layer\n",
    "        LSTM(RNN_UNITS, kernel_regularizer=l2(l2_lambda)),\n",
    "        Dropout(dropout_rate),  # Dropout after second LSTM layer\n",
    "        Dense(DENSE_UNITS, activation=\"relu\", kernel_regularizer=l2(l2_lambda)),\n",
    "        Dropout(dropout_rate),  # Dropout before final layer\n",
    "        Dense(1, activation=\"sigmoid\")  # Output layer for binary classification\n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(clipnorm=clipnorm)\n",
    "    model.compile(optimizer=optimizer, loss=LOSS_FUNCTION, metrics=METRICS)\n",
    "    return model\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Bidirectional LSTM Model (Optional)\n",
    "# ==========================\n",
    "def build_bidirectional_lstm():\n",
    "    \"\"\"\n",
    "    Builds a Bidirectional LSTM model for improved sequence learning.\n",
    "\n",
    "    Returns:\n",
    "        A compiled Keras model.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(RNN_UNITS, return_sequences=True), input_shape=INPUT_SHAPE),\n",
    "        Bidirectional(LSTM(RNN_UNITS)),\n",
    "        Dense(DENSE_UNITS, activation=\"relu\"),\n",
    "        Dropout(DROPOUT_RATE),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=OPTIMIZER, loss=LOSS_FUNCTION, metrics=METRICS)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# CONFIGURABLE VARIABLES\n",
    "# ==========================\n",
    "RNN_UNITS = 128           # Number of RNN/LSTM units\n",
    "DENSE_UNITS = 64         # Number of neurons in the dense layer\n",
    "DROPOUT_RATE = 0.1      # Dropout rate for regularization\n",
    "OPTIMIZER = \"adam\"       # Optimizer: \"adam\", \"sgd\", \"rmsprop\", etc.\n",
    "LOSS_FUNCTION = \"binary_crossentropy\"  # \"binary_crossentropy\" for classification\n",
    "METRICS = [\"accuracy\"]   # Metrics to monitor\n",
    "EPOCHS = 10              # Number of training epochs\n",
    "BATCH_SIZE = 16          # Batch size for training\n",
    "INPUT_SHAPE = (500, 1)   # (Time steps, Features) - Adjust based on your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(max_depth=5, n_estimators=200, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Initialize the Model\n",
    "# ==========================\n",
    "model = build_gradient_boosting_model()  # Change this to your desired model\n",
    "# print(model.summary())  # Print model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1016, 505), Test shape: (254, 505)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-26 18:10:58,888] A new study created in memory with name: no-name-0e0d07ff-8478-46ca-9268-166dceb166ec\n",
      "[I 2025-03-26 18:11:06,979] Trial 0 finished with value: 0.8931632311735587 and parameters: {'n_estimators': 425, 'learning_rate': 0.21350130323257352, 'max_depth': 4, 'min_samples_split': 6, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.8931632311735587.\n",
      "[I 2025-03-26 18:11:26,828] Trial 1 finished with value: 0.8908091694787925 and parameters: {'n_estimators': 400, 'learning_rate': 0.07956979644814627, 'max_depth': 12, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.8931632311735587.\n",
      "[I 2025-03-26 18:11:35,331] Trial 2 finished with value: 0.9041445281592411 and parameters: {'n_estimators': 450, 'learning_rate': 0.07832374383829764, 'max_depth': 4, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 2 with value: 0.9041445281592411.\n",
      "[I 2025-03-26 18:11:40,580] Trial 3 finished with value: 0.9039723376934452 and parameters: {'n_estimators': 325, 'learning_rate': 0.1586594011177406, 'max_depth': 3, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 2 with value: 0.9041445281592411.\n",
      "[I 2025-03-26 18:11:53,938] Trial 4 finished with value: 0.883085738422181 and parameters: {'n_estimators': 250, 'learning_rate': 0.04225538280849472, 'max_depth': 11, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 2 with value: 0.9041445281592411.\n",
      "[I 2025-03-26 18:11:56,220] Trial 5 finished with value: 0.8963358281789097 and parameters: {'n_estimators': 50, 'learning_rate': 0.2003436004767281, 'max_depth': 5, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.9041445281592411.\n",
      "[I 2025-03-26 18:12:02,165] Trial 6 finished with value: 0.90049652982112 and parameters: {'n_estimators': 375, 'learning_rate': 0.4517875941638615, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 2 with value: 0.9041445281592411.\n",
      "[I 2025-03-26 18:12:09,732] Trial 7 finished with value: 0.8950737991239773 and parameters: {'n_estimators': 175, 'learning_rate': 0.2720164878212086, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 2 with value: 0.9041445281592411.\n",
      "[I 2025-03-26 18:12:14,620] Trial 8 finished with value: 0.9068470119406025 and parameters: {'n_estimators': 225, 'learning_rate': 0.06455093858186825, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 8 with value: 0.9068470119406025.\n",
      "[I 2025-03-26 18:12:20,783] Trial 9 finished with value: 0.9008336004082464 and parameters: {'n_estimators': 400, 'learning_rate': 0.01288871514874258, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 5}. Best is trial 8 with value: 0.9068470119406025.\n",
      "[I 2025-03-26 18:12:26,315] Trial 10 finished with value: 0.9010124593284289 and parameters: {'n_estimators': 150, 'learning_rate': 0.028113823910180347, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 8 with value: 0.9068470119406025.\n",
      "[I 2025-03-26 18:12:39,633] Trial 11 finished with value: 0.8860018559607621 and parameters: {'n_estimators': 500, 'learning_rate': 0.0864428024750345, 'max_depth': 6, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 8 with value: 0.9068470119406025.\n",
      "[I 2025-03-26 18:12:47,926] Trial 12 finished with value: 0.8880286095385618 and parameters: {'n_estimators': 225, 'learning_rate': 0.044722535205913724, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 8 with value: 0.9068470119406025.\n",
      "[I 2025-03-26 18:12:59,202] Trial 13 finished with value: 0.8915135161747456 and parameters: {'n_estimators': 500, 'learning_rate': 0.11788440943701045, 'max_depth': 5, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 8 with value: 0.9068470119406025.\n",
      "[I 2025-03-26 18:13:06,475] Trial 14 finished with value: 0.9041896459860649 and parameters: {'n_estimators': 300, 'learning_rate': 0.025189855531342814, 'max_depth': 5, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 8 with value: 0.9068470119406025.\n",
      "[I 2025-03-26 18:13:14,966] Trial 15 finished with value: 0.9052059033643569 and parameters: {'n_estimators': 300, 'learning_rate': 0.01619670998773796, 'max_depth': 6, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 8 with value: 0.9068470119406025.\n",
      "[I 2025-03-26 18:13:19,665] Trial 16 finished with value: 0.8966483513537569 and parameters: {'n_estimators': 125, 'learning_rate': 0.011653571073161083, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 8 with value: 0.9068470119406025.\n",
      "[I 2025-03-26 18:13:28,272] Trial 17 finished with value: 0.8917934809777142 and parameters: {'n_estimators': 200, 'learning_rate': 0.018913788681174232, 'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 8 with value: 0.9068470119406025.\n",
      "[I 2025-03-26 18:13:37,768] Trial 18 finished with value: 0.8949462451285892 and parameters: {'n_estimators': 325, 'learning_rate': 0.04646057103874978, 'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 8 with value: 0.9068470119406025.\n",
      "[I 2025-03-26 18:13:41,451] Trial 19 finished with value: 0.8984815012877488 and parameters: {'n_estimators': 100, 'learning_rate': 0.020329495162809838, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 8 with value: 0.9068470119406025.\n",
      "[I 2025-03-26 18:13:51,849] Trial 20 finished with value: 0.9002205799182033 and parameters: {'n_estimators': 275, 'learning_rate': 0.03554866399616461, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 8 with value: 0.9068470119406025.\n",
      "[I 2025-03-26 18:13:59,697] Trial 21 finished with value: 0.9020701254536225 and parameters: {'n_estimators': 325, 'learning_rate': 0.01901303855589419, 'max_depth': 5, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 8 with value: 0.9068470119406025.\n",
      "[I 2025-03-26 18:14:05,788] Trial 22 finished with value: 0.9037329386427247 and parameters: {'n_estimators': 300, 'learning_rate': 0.028892452493588645, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 5}. Best is trial 8 with value: 0.9068470119406025.\n",
      "[I 2025-03-26 18:14:12,042] Trial 23 finished with value: 0.8929023051223075 and parameters: {'n_estimators': 250, 'learning_rate': 0.010074551066830863, 'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 8 with value: 0.9068470119406025.\n",
      "[I 2025-03-26 18:14:21,591] Trial 24 finished with value: 0.8984499448552979 and parameters: {'n_estimators': 350, 'learning_rate': 0.054131822025529665, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 8 with value: 0.9068470119406025.\n",
      "[I 2025-03-26 18:14:27,281] Trial 25 finished with value: 0.9110070889316075 and parameters: {'n_estimators': 275, 'learning_rate': 0.02134664680536897, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 25 with value: 0.9110070889316075.\n",
      "[I 2025-03-26 18:14:32,178] Trial 26 finished with value: 0.8938268769804345 and parameters: {'n_estimators': 225, 'learning_rate': 0.013904096235246547, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 25 with value: 0.9110070889316075.\n",
      "[I 2025-03-26 18:14:35,921] Trial 27 finished with value: 0.9115672354608474 and parameters: {'n_estimators': 200, 'learning_rate': 0.05941784809075417, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 27 with value: 0.9115672354608474.\n",
      "[I 2025-03-26 18:14:39,357] Trial 28 finished with value: 0.914428886023301 and parameters: {'n_estimators': 175, 'learning_rate': 0.06340285065365261, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:14:41,558] Trial 29 finished with value: 0.9046386450514209 and parameters: {'n_estimators': 75, 'learning_rate': 0.1019287849943636, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:14:44,991] Trial 30 finished with value: 0.9109895280815925 and parameters: {'n_estimators': 175, 'learning_rate': 0.1268590277358549, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:14:48,425] Trial 31 finished with value: 0.9096935943387556 and parameters: {'n_estimators': 175, 'learning_rate': 0.13215585233054014, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:14:51,548] Trial 32 finished with value: 0.9113226709152006 and parameters: {'n_estimators': 150, 'learning_rate': 0.06210720176341893, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:14:54,833] Trial 33 finished with value: 0.910996261552359 and parameters: {'n_estimators': 125, 'learning_rate': 0.06204500263501885, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:14:58,534] Trial 34 finished with value: 0.9057226332175699 and parameters: {'n_estimators': 150, 'learning_rate': 0.09206703462482868, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:15:02,120] Trial 35 finished with value: 0.9012456951650499 and parameters: {'n_estimators': 200, 'learning_rate': 0.03663715235174101, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 5}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:15:07,809] Trial 36 finished with value: 0.8945945771757685 and parameters: {'n_estimators': 100, 'learning_rate': 0.06552433772904966, 'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:15:12,164] Trial 37 finished with value: 0.9028541543012638 and parameters: {'n_estimators': 250, 'learning_rate': 0.19809303195055708, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:15:14,245] Trial 38 finished with value: 0.8972607850860754 and parameters: {'n_estimators': 50, 'learning_rate': 0.0747943525550563, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:15:17,989] Trial 39 finished with value: 0.912995691731205 and parameters: {'n_estimators': 200, 'learning_rate': 0.053100824858457504, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:15:28,464] Trial 40 finished with value: 0.892408897720373 and parameters: {'n_estimators': 200, 'learning_rate': 0.05336360966220814, 'max_depth': 12, 'min_samples_split': 8, 'min_samples_leaf': 5}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:15:31,586] Trial 41 finished with value: 0.899369397931544 and parameters: {'n_estimators': 150, 'learning_rate': 0.03760362152641958, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:15:35,319] Trial 42 finished with value: 0.9078308601079378 and parameters: {'n_estimators': 200, 'learning_rate': 0.051649203736620436, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:15:40,611] Trial 43 finished with value: 0.902387636677271 and parameters: {'n_estimators': 250, 'learning_rate': 0.07668497355219436, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:15:43,428] Trial 44 finished with value: 0.8988069448205138 and parameters: {'n_estimators': 125, 'learning_rate': 0.34446129399837067, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:15:47,535] Trial 45 finished with value: 0.8967495945209789 and parameters: {'n_estimators': 175, 'learning_rate': 0.160657642542499, 'max_depth': 4, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:15:52,049] Trial 46 finished with value: 0.9044633772809924 and parameters: {'n_estimators': 275, 'learning_rate': 0.030029993535262076, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:15:57,816] Trial 47 finished with value: 0.8936830611360589 and parameters: {'n_estimators': 225, 'learning_rate': 0.10655625068341779, 'max_depth': 5, 'min_samples_split': 9, 'min_samples_leaf': 5}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:16:01,506] Trial 48 finished with value: 0.892131140546326 and parameters: {'n_estimators': 150, 'learning_rate': 0.023432458775584464, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:16:08,319] Trial 49 finished with value: 0.9035044404540994 and parameters: {'n_estimators': 450, 'learning_rate': 0.05955093211229043, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:16:11,241] Trial 50 finished with value: 0.8953493742684266 and parameters: {'n_estimators': 100, 'learning_rate': 0.041120709696680774, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:16:14,529] Trial 51 finished with value: 0.9108622106575595 and parameters: {'n_estimators': 125, 'learning_rate': 0.06451563274441552, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:16:17,306] Trial 52 finished with value: 0.9047303217452832 and parameters: {'n_estimators': 75, 'learning_rate': 0.08758166777958527, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:16:20,746] Trial 53 finished with value: 0.9031606698042817 and parameters: {'n_estimators': 175, 'learning_rate': 0.04632816592650086, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:16:24,032] Trial 54 finished with value: 0.909450537423943 and parameters: {'n_estimators': 125, 'learning_rate': 0.07173499818483672, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 5}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:16:31,758] Trial 55 finished with value: 0.8920180053029789 and parameters: {'n_estimators': 150, 'learning_rate': 0.0588405672078052, 'max_depth': 11, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:16:35,502] Trial 56 finished with value: 0.8973853296417253 and parameters: {'n_estimators': 200, 'learning_rate': 0.03499940945036067, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:16:45,166] Trial 57 finished with value: 0.8907632152787786 and parameters: {'n_estimators': 225, 'learning_rate': 0.08839609601097334, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:16:51,780] Trial 58 finished with value: 0.8979375230460009 and parameters: {'n_estimators': 275, 'learning_rate': 0.04875705704744908, 'max_depth': 5, 'min_samples_split': 9, 'min_samples_leaf': 5}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:16:54,264] Trial 59 finished with value: 0.8936447639050777 and parameters: {'n_estimators': 75, 'learning_rate': 0.03091295847487601, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:17:00,157] Trial 60 finished with value: 0.9050727735368957 and parameters: {'n_estimators': 375, 'learning_rate': 0.02518671595394197, 'max_depth': 3, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:17:03,592] Trial 61 finished with value: 0.9064361009859174 and parameters: {'n_estimators': 175, 'learning_rate': 0.1332666256319538, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:17:07,282] Trial 62 finished with value: 0.896660019465056 and parameters: {'n_estimators': 150, 'learning_rate': 0.15799391672073682, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:17:10,718] Trial 63 finished with value: 0.9124695943493937 and parameters: {'n_estimators': 175, 'learning_rate': 0.10561703344131818, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:17:14,013] Trial 64 finished with value: 0.9063077288086421 and parameters: {'n_estimators': 125, 'learning_rate': 0.1052272522172598, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 28 with value: 0.914428886023301.\n",
      "[I 2025-03-26 18:17:17,754] Trial 65 finished with value: 0.9159664689824002 and parameters: {'n_estimators': 200, 'learning_rate': 0.08096400970844743, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:17:21,800] Trial 66 finished with value: 0.9092194598241283 and parameters: {'n_estimators': 225, 'learning_rate': 0.07861897880251016, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:17:26,154] Trial 67 finished with value: 0.9094874216618157 and parameters: {'n_estimators': 250, 'learning_rate': 0.11647088995103151, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:17:33,060] Trial 68 finished with value: 0.8867241952046889 and parameters: {'n_estimators': 200, 'learning_rate': 0.09659564803023821, 'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:17:36,346] Trial 69 finished with value: 0.9034697519216499 and parameters: {'n_estimators': 175, 'learning_rate': 0.042084788613176397, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:17:40,084] Trial 70 finished with value: 0.9105559009571598 and parameters: {'n_estimators': 200, 'learning_rate': 0.15145666432384916, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:17:42,967] Trial 71 finished with value: 0.9042552241297427 and parameters: {'n_estimators': 100, 'learning_rate': 0.05946013618866774, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:17:47,106] Trial 72 finished with value: 0.9079767859013044 and parameters: {'n_estimators': 175, 'learning_rate': 0.06596833047726332, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:17:50,234] Trial 73 finished with value: 0.9068383190068742 and parameters: {'n_estimators': 150, 'learning_rate': 0.08467773304376511, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:17:54,276] Trial 74 finished with value: 0.9108391225047441 and parameters: {'n_estimators': 225, 'learning_rate': 0.06739137973627694, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:17:58,051] Trial 75 finished with value: 0.9092805977306819 and parameters: {'n_estimators': 125, 'learning_rate': 0.05439561937141807, 'max_depth': 5, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:18:01,782] Trial 76 finished with value: 0.8942232775599877 and parameters: {'n_estimators': 200, 'learning_rate': 0.014935910322979895, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:18:09,984] Trial 77 finished with value: 0.898437169225445 and parameters: {'n_estimators': 175, 'learning_rate': 0.07938475792544225, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:18:13,670] Trial 78 finished with value: 0.8996418460499637 and parameters: {'n_estimators': 150, 'learning_rate': 0.18301424892636423, 'max_depth': 4, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:18:22,824] Trial 79 finished with value: 0.8932965326388574 and parameters: {'n_estimators': 250, 'learning_rate': 0.23291502463624378, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:18:26,729] Trial 80 finished with value: 0.9092752724669536 and parameters: {'n_estimators': 225, 'learning_rate': 0.07220569275194245, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:18:31,705] Trial 81 finished with value: 0.9111076337671047 and parameters: {'n_estimators': 300, 'learning_rate': 0.1161657294400245, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:18:37,601] Trial 82 finished with value: 0.9079654827598977 and parameters: {'n_estimators': 375, 'learning_rate': 0.1135624178976777, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:18:43,701] Trial 83 finished with value: 0.8923230020381034 and parameters: {'n_estimators': 300, 'learning_rate': 0.09741997985721135, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:18:48,659] Trial 84 finished with value: 0.908412435109262 and parameters: {'n_estimators': 300, 'learning_rate': 0.14017954955599563, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 65 with value: 0.9159664689824002.\n",
      "[I 2025-03-26 18:18:53,932] Trial 85 finished with value: 0.9173041267223645 and parameters: {'n_estimators': 325, 'learning_rate': 0.05872944901574193, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 85 with value: 0.9173041267223645.\n",
      "[I 2025-03-26 18:18:59,213] Trial 86 finished with value: 0.9132841812532024 and parameters: {'n_estimators': 325, 'learning_rate': 0.05140115378951899, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 85 with value: 0.9173041267223645.\n",
      "[I 2025-03-26 18:19:04,485] Trial 87 finished with value: 0.914305716213611 and parameters: {'n_estimators': 325, 'learning_rate': 0.049286249582619915, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 85 with value: 0.9173041267223645.\n",
      "[I 2025-03-26 18:19:10,098] Trial 88 finished with value: 0.9037261766421997 and parameters: {'n_estimators': 350, 'learning_rate': 0.05172844836841572, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 85 with value: 0.9173041267223645.\n",
      "[I 2025-03-26 18:19:15,671] Trial 89 finished with value: 0.9179463151482927 and parameters: {'n_estimators': 350, 'learning_rate': 0.04291036916062236, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:19:21,250] Trial 90 finished with value: 0.914882061179078 and parameters: {'n_estimators': 350, 'learning_rate': 0.04145597370383835, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:19:26,831] Trial 91 finished with value: 0.9071288274488968 and parameters: {'n_estimators': 350, 'learning_rate': 0.04043511710045534, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:19:31,965] Trial 92 finished with value: 0.909279254079254 and parameters: {'n_estimators': 325, 'learning_rate': 0.0461255306358776, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:19:37,547] Trial 93 finished with value: 0.9066643346112437 and parameters: {'n_estimators': 350, 'learning_rate': 0.049401941044415726, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:19:42,989] Trial 94 finished with value: 0.9093353645291096 and parameters: {'n_estimators': 325, 'learning_rate': 0.056157833442863486, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:19:49,036] Trial 95 finished with value: 0.9085905887223094 and parameters: {'n_estimators': 375, 'learning_rate': 0.034835333628887105, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:19:56,819] Trial 96 finished with value: 0.8975547394054342 and parameters: {'n_estimators': 400, 'learning_rate': 0.04303185178603249, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:20:02,212] Trial 97 finished with value: 0.9097057340210689 and parameters: {'n_estimators': 325, 'learning_rate': 0.0326138384623171, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:20:09,261] Trial 98 finished with value: 0.9024051651923406 and parameters: {'n_estimators': 350, 'learning_rate': 0.038503133606083796, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:20:15,689] Trial 99 finished with value: 0.9053291064172212 and parameters: {'n_estimators': 400, 'learning_rate': 0.04883976591138263, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:20:23,660] Trial 100 finished with value: 0.8986887439574673 and parameters: {'n_estimators': 375, 'learning_rate': 0.06961208178332642, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 5}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:20:29,045] Trial 101 finished with value: 0.9066733880213608 and parameters: {'n_estimators': 300, 'learning_rate': 0.06207243031114108, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:20:34,830] Trial 102 finished with value: 0.900314415090592 and parameters: {'n_estimators': 325, 'learning_rate': 0.05598347151891351, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:20:40,861] Trial 103 finished with value: 0.905367097329888 and parameters: {'n_estimators': 350, 'learning_rate': 0.08491785366819266, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:20:51,378] Trial 104 finished with value: 0.9089888988272709 and parameters: {'n_estimators': 325, 'learning_rate': 0.0470578489620027, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:20:57,381] Trial 105 finished with value: 0.9087233384001513 and parameters: {'n_estimators': 375, 'learning_rate': 0.04407895540648495, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:21:05,510] Trial 106 finished with value: 0.8993517946286718 and parameters: {'n_estimators': 425, 'learning_rate': 0.06324589224059492, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:21:09,313] Trial 107 finished with value: 0.9095118341459674 and parameters: {'n_estimators': 200, 'learning_rate': 0.05660149500761196, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:21:14,925] Trial 108 finished with value: 0.9094818003008214 and parameters: {'n_estimators': 350, 'learning_rate': 0.03914082158483861, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:21:19,033] Trial 109 finished with value: 0.9057065904872335 and parameters: {'n_estimators': 175, 'learning_rate': 0.05133922551615742, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:21:22,766] Trial 110 finished with value: 0.8988484163400381 and parameters: {'n_estimators': 200, 'learning_rate': 0.027303796699923152, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:21:28,116] Trial 111 finished with value: 0.911803931838406 and parameters: {'n_estimators': 325, 'learning_rate': 0.06858773221190838, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:21:33,418] Trial 112 finished with value: 0.907929966831475 and parameters: {'n_estimators': 325, 'learning_rate': 0.07576832764920198, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:21:38,088] Trial 113 finished with value: 0.9164237345319682 and parameters: {'n_estimators': 275, 'learning_rate': 0.061379272484923315, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:21:42,761] Trial 114 finished with value: 0.917706180279082 and parameters: {'n_estimators': 275, 'learning_rate': 0.06859328127671717, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:21:47,431] Trial 115 finished with value: 0.9065525930252708 and parameters: {'n_estimators': 275, 'learning_rate': 0.081521519136648, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:21:51,964] Trial 116 finished with value: 0.90420592303871 and parameters: {'n_estimators': 275, 'learning_rate': 0.09053260093055826, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:21:56,631] Trial 117 finished with value: 0.912617309908738 and parameters: {'n_estimators': 275, 'learning_rate': 0.0701896670685113, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:22:02,820] Trial 118 finished with value: 0.9046576882529695 and parameters: {'n_estimators': 300, 'learning_rate': 0.07396238919349904, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:22:08,603] Trial 119 finished with value: 0.9042635473609583 and parameters: {'n_estimators': 275, 'learning_rate': 0.05993957440588341, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:22:13,032] Trial 120 finished with value: 0.9080360858397928 and parameters: {'n_estimators': 250, 'learning_rate': 0.0516425646236172, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:22:18,153] Trial 121 finished with value: 0.905240983276005 and parameters: {'n_estimators': 300, 'learning_rate': 0.0687186213123845, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:22:23,519] Trial 122 finished with value: 0.9100764568580653 and parameters: {'n_estimators': 325, 'learning_rate': 0.06633371824790102, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:22:29,257] Trial 123 finished with value: 0.9066222470131867 and parameters: {'n_estimators': 350, 'learning_rate': 0.044160967219974104, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:22:34,266] Trial 124 finished with value: 0.9144076371165916 and parameters: {'n_estimators': 300, 'learning_rate': 0.053831476402615805, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:22:44,141] Trial 125 finished with value: 0.9046057989718752 and parameters: {'n_estimators': 300, 'learning_rate': 0.05479674056132176, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:22:49,198] Trial 126 finished with value: 0.9114598290598291 and parameters: {'n_estimators': 275, 'learning_rate': 0.04707469293971394, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:22:53,574] Trial 127 finished with value: 0.9093190692218809 and parameters: {'n_estimators': 250, 'learning_rate': 0.05957977135572364, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:22:59,563] Trial 128 finished with value: 0.9042993155915864 and parameters: {'n_estimators': 300, 'learning_rate': 0.033339645848803244, 'max_depth': 4, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:23:04,257] Trial 129 finished with value: 0.9080669497237428 and parameters: {'n_estimators': 275, 'learning_rate': 0.04158540489769013, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:23:09,574] Trial 130 finished with value: 0.8982874347020571 and parameters: {'n_estimators': 325, 'learning_rate': 0.07844258734166482, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:23:15,026] Trial 131 finished with value: 0.9103408299898706 and parameters: {'n_estimators': 325, 'learning_rate': 0.07015713652871006, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:23:20,089] Trial 132 finished with value: 0.9051681886012741 and parameters: {'n_estimators': 300, 'learning_rate': 0.06476071071519732, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:23:25,679] Trial 133 finished with value: 0.9120452050611363 and parameters: {'n_estimators': 350, 'learning_rate': 0.05198788043952495, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:23:31,306] Trial 134 finished with value: 0.9084678840288323 and parameters: {'n_estimators': 350, 'learning_rate': 0.05252937324922464, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:23:37,041] Trial 135 finished with value: 0.9133387949384304 and parameters: {'n_estimators': 350, 'learning_rate': 0.03736852071346815, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:23:43,974] Trial 136 finished with value: 0.9101209865904561 and parameters: {'n_estimators': 350, 'learning_rate': 0.036641727704154664, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:23:48,050] Trial 137 finished with value: 0.9049576181021639 and parameters: {'n_estimators': 225, 'learning_rate': 0.09599268489538366, 'max_depth': 3, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:23:53,958] Trial 138 finished with value: 0.915970251223001 and parameters: {'n_estimators': 375, 'learning_rate': 0.04364971693170889, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:23:59,871] Trial 139 finished with value: 0.9111606920842877 and parameters: {'n_estimators': 375, 'learning_rate': 0.04546175677863845, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:24:07,037] Trial 140 finished with value: 0.9095567623740177 and parameters: {'n_estimators': 375, 'learning_rate': 0.03787747683614618, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:24:13,262] Trial 141 finished with value: 0.9108238933769494 and parameters: {'n_estimators': 400, 'learning_rate': 0.04174333633291355, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:24:18,561] Trial 142 finished with value: 0.9163687385860682 and parameters: {'n_estimators': 325, 'learning_rate': 0.05829174837686179, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:24:23,883] Trial 143 finished with value: 0.911359100100222 and parameters: {'n_estimators': 325, 'learning_rate': 0.05584335031955446, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:24:28,887] Trial 144 finished with value: 0.909540588610356 and parameters: {'n_estimators': 300, 'learning_rate': 0.04881950833915513, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:24:34,490] Trial 145 finished with value: 0.9087694240771047 and parameters: {'n_estimators': 350, 'learning_rate': 0.05606291080469614, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:24:39,860] Trial 146 finished with value: 0.9098876317969152 and parameters: {'n_estimators': 325, 'learning_rate': 0.06104296330426653, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:24:45,520] Trial 147 finished with value: 0.9066894356321843 and parameters: {'n_estimators': 350, 'learning_rate': 0.032013294293469226, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:25:01,572] Trial 148 finished with value: 0.8953752915103494 and parameters: {'n_estimators': 375, 'learning_rate': 0.04823298752028769, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:25:06,992] Trial 149 finished with value: 0.911650075135529 and parameters: {'n_estimators': 325, 'learning_rate': 0.040421564364127055, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:25:11,774] Trial 150 finished with value: 0.9161305968884917 and parameters: {'n_estimators': 275, 'learning_rate': 0.04477420695125113, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:25:16,909] Trial 151 finished with value: 0.9125488131939745 and parameters: {'n_estimators': 275, 'learning_rate': 0.04353402138624817, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:25:21,721] Trial 152 finished with value: 0.911670368195559 and parameters: {'n_estimators': 250, 'learning_rate': 0.061114524114485654, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:25:28,340] Trial 153 finished with value: 0.9068888215021975 and parameters: {'n_estimators': 300, 'learning_rate': 0.0514645883389259, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:25:35,183] Trial 154 finished with value: 0.9117747706651465 and parameters: {'n_estimators': 425, 'learning_rate': 0.03568274207830523, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:25:39,874] Trial 155 finished with value: 0.9125979804010363 and parameters: {'n_estimators': 275, 'learning_rate': 0.044469946275886074, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:25:45,176] Trial 156 finished with value: 0.9161326146634199 and parameters: {'n_estimators': 325, 'learning_rate': 0.07310977355030224, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:26:02,118] Trial 157 finished with value: 0.8963998971388596 and parameters: {'n_estimators': 325, 'learning_rate': 0.05722894411025687, 'max_depth': 12, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:26:09,117] Trial 158 finished with value: 0.8870383595713509 and parameters: {'n_estimators': 350, 'learning_rate': 0.4824547425827953, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:26:15,009] Trial 159 finished with value: 0.9116442352818611 and parameters: {'n_estimators': 375, 'learning_rate': 0.04847958570105224, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:26:19,990] Trial 160 finished with value: 0.9045323789015501 and parameters: {'n_estimators': 300, 'learning_rate': 0.03904296381877033, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:26:25,274] Trial 161 finished with value: 0.9097135385271209 and parameters: {'n_estimators': 325, 'learning_rate': 0.07384459881454519, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:26:29,803] Trial 162 finished with value: 0.9052191733209213 and parameters: {'n_estimators': 275, 'learning_rate': 0.06466908330679712, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:26:34,783] Trial 163 finished with value: 0.902531420304722 and parameters: {'n_estimators': 300, 'learning_rate': 0.07982063281529032, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:26:39,148] Trial 164 finished with value: 0.911232286236894 and parameters: {'n_estimators': 250, 'learning_rate': 0.07013520331535204, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:26:44,814] Trial 165 finished with value: 0.9037630136374581 and parameters: {'n_estimators': 350, 'learning_rate': 0.05344572942623353, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:26:49,857] Trial 166 finished with value: 0.8994410340156722 and parameters: {'n_estimators': 300, 'learning_rate': 0.08469649333935361, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:26:55,147] Trial 167 finished with value: 0.904288860989297 and parameters: {'n_estimators': 325, 'learning_rate': 0.063554767068804, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:27:02,056] Trial 168 finished with value: 0.9027904870813479 and parameters: {'n_estimators': 350, 'learning_rate': 0.05816075225854047, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:27:07,341] Trial 169 finished with value: 0.9080758960337908 and parameters: {'n_estimators': 325, 'learning_rate': 0.047640549355590145, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:27:12,012] Trial 170 finished with value: 0.91298861655809 and parameters: {'n_estimators': 275, 'learning_rate': 0.07176206391606291, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:27:16,684] Trial 171 finished with value: 0.915541229829602 and parameters: {'n_estimators': 275, 'learning_rate': 0.07515315050023784, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:27:21,039] Trial 172 finished with value: 0.8848375682365559 and parameters: {'n_estimators': 250, 'learning_rate': 0.37635736640666045, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:27:30,933] Trial 173 finished with value: 0.8886671507446836 and parameters: {'n_estimators': 275, 'learning_rate': 0.06628909254507506, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:27:35,601] Trial 174 finished with value: 0.9147864139860495 and parameters: {'n_estimators': 275, 'learning_rate': 0.07679853933735586, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:27:40,584] Trial 175 finished with value: 0.9089837947545721 and parameters: {'n_estimators': 300, 'learning_rate': 0.0922360624431593, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:27:45,887] Trial 176 finished with value: 0.907764713887737 and parameters: {'n_estimators': 325, 'learning_rate': 0.07915521925807564, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:27:51,798] Trial 177 finished with value: 0.9113525898078529 and parameters: {'n_estimators': 375, 'learning_rate': 0.04415663526341652, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:28:01,469] Trial 178 finished with value: 0.8967385650563138 and parameters: {'n_estimators': 350, 'learning_rate': 0.05266769559691667, 'max_depth': 6, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 89 with value: 0.9179463151482927.\n",
      "[I 2025-03-26 18:28:06,153] Trial 179 finished with value: 0.9181255984027639 and parameters: {'n_estimators': 275, 'learning_rate': 0.05919560535298311, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:28:10,827] Trial 180 finished with value: 0.906586243158222 and parameters: {'n_estimators': 275, 'learning_rate': 0.06120027296264631, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:28:15,491] Trial 181 finished with value: 0.9072689029940939 and parameters: {'n_estimators': 275, 'learning_rate': 0.07545474379542622, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:28:19,848] Trial 182 finished with value: 0.906676466710941 and parameters: {'n_estimators': 250, 'learning_rate': 0.05679751534633036, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:28:24,823] Trial 183 finished with value: 0.9030257741563867 and parameters: {'n_estimators': 300, 'learning_rate': 0.04986207771161154, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:28:29,814] Trial 184 finished with value: 0.9081196133837592 and parameters: {'n_estimators': 300, 'learning_rate': 0.06511481545211861, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:28:34,024] Trial 185 finished with value: 0.903185529368654 and parameters: {'n_estimators': 250, 'learning_rate': 0.04106925457601124, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:28:38,078] Trial 186 finished with value: 0.9034032150252319 and parameters: {'n_estimators': 225, 'learning_rate': 0.08482417047567581, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:28:45,493] Trial 187 finished with value: 0.9018924075080562 and parameters: {'n_estimators': 500, 'learning_rate': 0.05464138377265118, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:28:50,842] Trial 188 finished with value: 0.9115839652935083 and parameters: {'n_estimators': 325, 'learning_rate': 0.046112233711831235, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:28:56,800] Trial 189 finished with value: 0.8992451746147214 and parameters: {'n_estimators': 275, 'learning_rate': 0.03677129236086508, 'max_depth': 4, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:29:03,946] Trial 190 finished with value: 0.9141265899084081 and parameters: {'n_estimators': 475, 'learning_rate': 0.05935518555807876, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:29:11,069] Trial 191 finished with value: 0.9035526351690212 and parameters: {'n_estimators': 475, 'learning_rate': 0.05965354468518088, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:29:16,065] Trial 192 finished with value: 0.9096489114008385 and parameters: {'n_estimators': 300, 'learning_rate': 0.07339843314566577, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:29:19,976] Trial 193 finished with value: 0.9125488131939745 and parameters: {'n_estimators': 200, 'learning_rate': 0.06359290277033593, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:29:24,656] Trial 194 finished with value: 0.907190130788881 and parameters: {'n_estimators': 275, 'learning_rate': 0.052335465902328515, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:29:32,093] Trial 195 finished with value: 0.9074036539805952 and parameters: {'n_estimators': 500, 'learning_rate': 0.05969670934606161, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:29:37,687] Trial 196 finished with value: 0.9027698026947581 and parameters: {'n_estimators': 350, 'learning_rate': 0.05546501452461227, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:29:42,824] Trial 197 finished with value: 0.9028919927226727 and parameters: {'n_estimators': 325, 'learning_rate': 0.06860981130503108, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:29:49,971] Trial 198 finished with value: 0.9143887026558808 and parameters: {'n_estimators': 475, 'learning_rate': 0.04983651975763272, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 179 with value: 0.9181255984027639.\n",
      "[I 2025-03-26 18:29:56,798] Trial 199 finished with value: 0.9176744584661396 and parameters: {'n_estimators': 450, 'learning_rate': 0.04223931045033256, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 179 with value: 0.9181255984027639.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 275, 'learning_rate': 0.05919560535298311, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 4}\n",
      "1016 1016\n",
      "254 254\n",
      "DEBUG: Augmentation method 'noise' produced no NaN values. Shape: (1016, 500)\n",
      "DEBUG: Augmentation method 'baseline_shift' produced no NaN values. Shape: (1016, 500)\n",
      "DEBUG: Augmentation method 'time_shift' produced no NaN values. Shape: (1016, 500)\n",
      "4064 4064\n",
      "254 254\n",
      "Model trained with accuracy: 0.9488\n",
      "Precision: 0.9600, Recall: 0.8780, F1-score: 0.9172, ROC-AUC: 0.9872\n",
      "Confusion Matrix:\n",
      "[[169  10]\n",
      " [  3  72]]\n",
      "Activity: CD, X shape: (4, 5), y shape: (4,)\n",
      "Activity CD: 4 correct, 0 incorrect\n",
      "Activity: FCF, X shape: (12, 5), y shape: (12,)\n",
      "Activity FCF: 9 correct, 3 incorrect\n",
      "Activity: FCS, X shape: (10, 5), y shape: (10,)\n",
      "Activity FCS: 6 correct, 4 incorrect\n",
      "Activity: FOB, X shape: (12, 5), y shape: (12,)\n",
      "Activity FOB: 12 correct, 0 incorrect\n",
      "Activity: FOL, X shape: (12, 5), y shape: (12,)\n",
      "Activity FOL: 12 correct, 0 incorrect\n",
      "Activity: FR, X shape: (12, 5), y shape: (12,)\n",
      "Activity FR: 10 correct, 2 incorrect\n",
      "Activity: K, X shape: (24, 5), y shape: (24,)\n",
      "Activity K: 23 correct, 1 incorrect\n",
      "Activity: KD, X shape: (2, 5), y shape: (2,)\n",
      "Activity KD: 2 correct, 0 incorrect\n",
      "Activity: KID, X shape: (4, 5), y shape: (4,)\n",
      "Activity KID: 4 correct, 0 incorrect\n",
      "Activity: LAF, X shape: (12, 5), y shape: (12,)\n",
      "Activity LAF: 9 correct, 3 incorrect\n",
      "Activity: LC, X shape: (12, 5), y shape: (12,)\n",
      "Activity LC: 12 correct, 0 incorrect\n",
      "Activity: LSF, X shape: (12, 5), y shape: (12,)\n",
      "Activity LSF: 12 correct, 0 incorrect\n",
      "Activity: MA, X shape: (9, 5), y shape: (9,)\n",
      "Activity MA: 9 correct, 0 incorrect\n",
      "Activity: PUF, X shape: (12, 5), y shape: (12,)\n",
      "Activity PUF: 12 correct, 0 incorrect\n",
      "Activity: RBS, X shape: (18, 5), y shape: (18,)\n",
      "Activity RBS: 18 correct, 0 incorrect\n",
      "Activity: S, X shape: (9, 5), y shape: (9,)\n",
      "Activity S: 9 correct, 0 incorrect\n",
      "Activity: SC, X shape: (12, 5), y shape: (12,)\n",
      "Activity SC: 12 correct, 0 incorrect\n",
      "Activity: SFB, X shape: (12, 5), y shape: (12,)\n",
      "Activity SFB: 12 correct, 0 incorrect\n",
      "Activity: SLB, X shape: (12, 5), y shape: (12,)\n",
      "Activity SLB: 12 correct, 0 incorrect\n",
      "Activity: STC, X shape: (12, 5), y shape: (12,)\n",
      "Activity STC: 12 correct, 0 incorrect\n",
      "Activity: TF, X shape: (12, 5), y shape: (12,)\n",
      "Activity TF: 12 correct, 0 incorrect\n",
      "Activity: WBS, X shape: (18, 5), y shape: (18,)\n",
      "Activity WBS: 18 correct, 0 incorrect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Per-Activity Results:\n",
      "                               activity Actual Fall  correct  incorrect  \\\n",
      "0                            Close Door     No Fall        4          0   \n",
      "1                 Chair - Fall to Front        Fall        9          3   \n",
      "2                  Chair - Fall to side        Fall        6          4   \n",
      "3             Fall of object (Backpack)     No Fall       12          0   \n",
      "4         Fall of object (FaszienRolle)     No Fall       12          0   \n",
      "5                         Fall Recovery     No Fall       10          2   \n",
      "6        Kneeling down then standing up     No Fall       23          1   \n",
      "7                            Knock Door     No Fall        2          0   \n",
      "8                          Kids Running     No Fall        4          0   \n",
      "9                    Lying - Awake Fall        Fall        9          3   \n",
      "10                 Laying down on couch     No Fall       12          0   \n",
      "11                  Lying - Asleep Fall        Fall       12          0   \n",
      "12  Minor Ambience (Sitting and Eating)     No Fall        9          0   \n",
      "13      Picking something up from floor     No Fall       12          0   \n",
      "14                       Rush by Sensor     No Fall       18          0   \n",
      "15                                Still     No Fall        9          0   \n",
      "16                Sitting down on chair     No Fall       12          0   \n",
      "17            Slip and Fall - Backwards        Fall       12          0   \n",
      "18                Standing Lost Balance        Fall       12          0   \n",
      "19                  Stand up from Chair     No Fall       12          0   \n",
      "20             Trip and Fall - Forwards        Fall       12          0   \n",
      "21                       Walk by Sensor     No Fall       18          0   \n",
      "\n",
      "    total  accuracy  \n",
      "0       4  1.000000  \n",
      "1      12  0.750000  \n",
      "2      10  0.600000  \n",
      "3      12  1.000000  \n",
      "4      12  1.000000  \n",
      "5      12  0.833333  \n",
      "6      24  0.958333  \n",
      "7       2  1.000000  \n",
      "8       4  1.000000  \n",
      "9      12  0.750000  \n",
      "10     12  1.000000  \n",
      "11     12  1.000000  \n",
      "12      9  1.000000  \n",
      "13     12  1.000000  \n",
      "14     18  1.000000  \n",
      "15      9  1.000000  \n",
      "16     12  1.000000  \n",
      "17     12  1.000000  \n",
      "18     12  1.000000  \n",
      "19     12  1.000000  \n",
      "20     12  1.000000  \n",
      "21     18  1.000000  \n",
      "Distance: 0, X shape: (13, 5), y shape: (13,)\n",
      "Distance 0: 13 correct, 0 incorrect\n",
      "Distance: 1, X shape: (64, 5), y shape: (64,)\n",
      "Distance 1: 63 correct, 1 incorrect\n",
      "Distance: 2, X shape: (92, 5), y shape: (92,)\n",
      "Distance 2: 85 correct, 7 incorrect\n",
      "Distance: 3, X shape: (85, 5), y shape: (85,)\n",
      "Distance 3: 80 correct, 5 incorrect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/davidhark/Desktop/Master These/master/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but GradientBoostingClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Per-Distance Results:\n",
      "   distance  correct  incorrect  total  accuracy\n",
      "0         0       13          0     13  1.000000\n",
      "1         1       63          1     64  0.984375\n",
      "2         2       85          7     92  0.923913\n",
      "3         3       80          5     85  0.941176\n",
      "🏃 View run amiable_feijoa_w7yw1ggx at: https://northeurope.api.azureml.ms/mlflow/v2.0/subscriptions/716d3e14-e009-4f92-89c9-01fa8347272a/resourceGroups/adda23ac-rg/providers/Microsoft.MachineLearningServices/workspaces/fall/#/experiments/d26258ba-da47-4538-9d29-dc17a3480b45/runs/5e66db4c-5c30-419e-9aac-ebc0dbf48eb2\n",
      "🧪 View experiment at: https://northeurope.api.azureml.ms/mlflow/v2.0/subscriptions/716d3e14-e009-4f92-89c9-01fa8347272a/resourceGroups/adda23ac-rg/providers/Microsoft.MachineLearningServices/workspaces/fall/#/experiments/d26258ba-da47-4538-9d29-dc17a3480b45\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trained_model = train_pipe(\n",
    "    model=model,\n",
    "    dataset_name=\"MPU_falls.csv\",\n",
    "    save_name=\"MF_VIF_RawAugAll_GBM_KFoldOptuna_Final\",\n",
    "    feature_columns=None,\n",
    "    target_column=\"fall_binary\",\n",
    "    experiment_name=\"Classic Models MPU\",\n",
    "    use_early_stopping=False,\n",
    "    if_optuna=True,\n",
    "    n_trials=200,\n",
    "    augment_data=\"all\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis model does not support feature importance.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Usage (after training a model)\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[43mplot_feature_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedian\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mp2p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimpulse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m, in \u001b[0;36mplot_feature_importance\u001b[0;34m(model, feature_columns)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_importances_\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Ensure model supports feature importance\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     importances \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfeature_importances_\n\u001b[0;32m---> 14\u001b[0m     feature_importance_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFeature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mImportance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimportances\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     feature_importance_df \u001b[38;5;241m=\u001b[39m feature_importance_df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImportance\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[0;32m~/Desktop/Master These/master/.venv/lib/python3.10/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/Desktop/Master These/master/.venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Master These/master/.venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/Desktop/Master These/master/.venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "def plot_feature_importance(model, feature_columns):\n",
    "    \"\"\"\n",
    "    Plots feature importance for tree-based models.\n",
    "\n",
    "    Args:\n",
    "        model: Trained XGBoost, RandomForest, or GradientBoosting model.\n",
    "        feature_columns: List of feature names.\n",
    "\n",
    "    Returns:\n",
    "        None (Displays feature importance plot)\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"feature_importances_\"):  # Ensure model supports feature importance\n",
    "        importances = model.feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({\"Feature\": feature_columns, \"Importance\": importances})\n",
    "        feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.barplot(x=\"Importance\", y=\"Feature\", data=feature_importance_df[:15])  # Top 15 features\n",
    "        plt.title(\"Feature Importance\")\n",
    "        plt.show()\n",
    "\n",
    "        return feature_importance_df\n",
    "    else:\n",
    "        raise ValueError(\"This model does not support feature importance.\")\n",
    "\n",
    "# Usage (after training a model)\n",
    "plot_feature_importance(trained_model, [\"median\", \"max\", \"mean\", \"p2p\", \"impulse\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def ensemble_predict(models_with_features, dataset_name, target_column):\n",
    "    \"\"\"\n",
    "    Loads a dataset, splits it, loads an ensemble of models (with different feature sets),\n",
    "    and predicts on the test set.\n",
    "\n",
    "    Args:\n",
    "        models_with_features: A list of tuples (model_path, feature_columns) specifying each model's file path\n",
    "                              and the corresponding feature columns used for training.\n",
    "        dataset_name: The name of the dataset CSV file.\n",
    "        target_column: The name of the target column.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    # ✅ Load dataset\n",
    "    df = load_data(dataset_name)\n",
    "\n",
    "    # ✅ Ensure target column exists\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in dataset.\")\n",
    "\n",
    "    # ✅ Train-test split\n",
    "    data_train, data_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    y_test = data_test[target_column].values  # Target remains the same for all models\n",
    "\n",
    "    # ✅ Load models and their feature columns\n",
    "    models = []\n",
    "    y_preds = []\n",
    "\n",
    "    for model_path, feature_columns in models_with_features:\n",
    "        model = joblib.load(model_path)  # Load model\n",
    "        models.append(model)\n",
    "\n",
    "        # ✅ Extract the correct feature set for this model\n",
    "        X_test = data_test[feature_columns].values  # Select only the features it was trained on\n",
    "\n",
    "        # ✅ Predict (check if the model has `predict_proba` for probability averaging)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_pred_proba = model.predict_proba(X_test)[:, 1]  # Get probability of the positive class\n",
    "        else:\n",
    "            y_pred_proba = model.predict(X_test)  # Some models might not have `predict_proba`\n",
    "\n",
    "        y_preds.append(y_pred_proba)\n",
    "\n",
    "    # ✅ Ensemble predictions (Average probabilities and threshold at 0.5)\n",
    "    y_pred = np.mean(y_preds, axis=0) > 0.5\n",
    "\n",
    "    # ✅ Compute metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    # ✅ Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # ✅ Print metrics\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "    # ✅ Plot confusion matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=[\"No Fall\", \"Fall\"], yticklabels=[\"No Fall\", \"Fall\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9764\n",
      "Precision: 0.9615\n",
      "Recall: 0.9615\n",
      "F1 Score: 0.9615\n",
      "ROC AUC: 0.9722\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ensemble_predict(\n",
    "    models_with_features=[\n",
    "        (\"models/MF_XGB_Optuna.pkl\", [\"max\", \"min\", \"mean\", \"std\", \"median\", \"peak\", \"p2p\", \"energy\", \"rms\", \"crest\", \"shape\", \"impulse\", \"margin\"]),  \n",
    "        (\"models/MF_XGB_RD40_Optuna.pkl\", [\"median\", \"max\", \"peak\", \"mean\", \"p2p\"]),\n",
    "        (\"models/MF_XGB_RD41_Optuna.pkl\", [\"median\", \"max\", \"peak\", \"mean\", \"p2p\"])    \n",
    "    ],\n",
    "    dataset_name=\"MPU_features_pca.csv\",\n",
    "    target_column=\"fall_binary\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
